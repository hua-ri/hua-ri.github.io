[{"title":"生产部署文档(v7-1)","url":"/2025/08/devops/blueking/sheng-chan-bu-shu-wen-dang-v7-1/","content":"# 1. 概述\n需要先准备一台中控机，在中控机安装 kubectl、helm、helmfile 等工具，以及蓝鲸安装脚本。然后部署基础套餐，最后再部署持续集成套餐（蓝盾）。\n\n简单来说就是三个步骤：\n- `1.准备环境` \n- `2.部署基础服务` \n- `3.部署蓝盾`\n\n# 2. 准备中控机\n按照[官方文档](https://bk.tencent.com/docs/markdown/ZH/DeploymentGuides/7.1/prepare-bkctrl.md)安装和配置即可。\n\n# 3. 部署基础服务\n需要按照[官方文档](https://bk.tencent.com/docs/markdown/ZH/DeploymentGuides/7.1/custom-values.md)一步步部署。\n\n## 3.1 下载安装文件\n请在 中控机 使用下载脚本下载蓝鲸 helmfile 包及公共证书。（ helmfile相关value文件在git上维护）\n```shell\nbkdl-7.1-stable.sh -ur latest base demo\n```\n\n这些文件默认放在了 ~/bkce7.1-install/ 目录。\n\n## 3.2 配置 Helm Chart 仓库\n```shell\nhelm repo add blueking https://hub.bktencent.com/chartrepo/blueking\nhelm repo update\nhelm repo list\n```\n\n## 3.3 配置全局 custom-values\n相关文件已经修改，在git上维护，配置访问域名。\n```shell\nBK_DOMAIN=bk.blazehu.com  # 请修改为你分配给蓝鲸平台的主域名 cd ~/bkce7.1-install/blueking/  # 进入工作目录# 可使用如下命令添加域名。如果文件已存在，请手动编辑。custom=environments/default/custom.yaml\ncat >> \"$custom\" <<EOF\nimageRegistry: ${REGISTRY:-hub.bktencent.com}\ndomain:\n  bkDomain: $BK_DOMAIN\n  bkMainSiteDomain: $BK_DOMAIN\nEOF\n```\n\n## 3.4 生成 values 文件\n还有一些 values 文件随着部署环境的不同而变化，所以我们提供了脚本快速生成。\n\n`生成蓝鲸 app code 对应的 secret`\n```shell\n./scripts/generate_app_secret.sh ./environments/default/app_secret.yaml\n```\n\n`生成 apigw 所需的 keypair`\n```shell\n./scripts/generate_rsa_keypair.sh ./environments/default/bkapigateway_builtin_keypair.yaml\n```\n\n`生成 paas 所需的 clusterAdmin`\n```shell\n./scripts/create_k8s_cluster_admin_for_paas3.sh\n```\n\n## 3.5 安装入口网关\n### 3.5.1 安装 ingress controller\n先检查你的环境是否已经部署了 ingress controller:\n```shell\nkubectl get pods -A -l app.kubernetes.io/name=ingress-nginx\n```\n\n如果没有，则使用如下命令创建：\n```shell\nhelmfile -f 00-ingress-nginx.yaml.gotmpl sync\nkubectl get pods -A -l app.kubernetes.io/name=ingress-nginx  查看创建的pod\n```\n\npops集群相关标签如下：\n```shell\nkubectl get pods -A -l app=ingress-nginx  # 查看创建的pod\nIP1=$(kubectl get svc -A -l app=nginx-ingress-lb -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}')\n# IP1=$(kubectl get svc -A -l app.kubernetes.io/name=ingress-nginx -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}')\n```\n\n### 3.5.2 配置 coredns\n在部署过程中，会在容器内访问这些域名，所以需要提前配置 coredns，将蓝鲸域名解析到 service IP。\n> 注意: 当 service 被删除，重建后 clusterIP 会变动，此时需刷新 hosts 文件。\n\n因此需要注入 hosts 配置项到 `kube-system` namespace 下的 `coredns` 系列 pod\n```shell\ncd ~/bkce7.1-install/blueking/  # 进入工作目录\nBK_DOMAIN=$(yq e '.domain.bkDomain' environments/default/custom.yaml)  # 从自定义配置中提取, 也可自行赋值\n#IP1=$(kubectl get svc -A -l app.kubernetes.io/instance=ingress-nginx -o jsonpath='{.items[0].spec.clusterIP}')\nIP1=$(kubectl get svc -A -l app=nginx-ingress-lb -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}')\n./scripts/control_coredns.sh update \"$IP1\" $BK_DOMAIN bkrepo.$BK_DOMAIN docker.$BK_DOMAIN bkapi.$BK_DOMAIN bkpaas.$BK_DOMAIN bkiam-api.$BK_DOMAIN bkiam.$BK_DOMAIN apps.$BK_DOMAIN bknodeman.$BK_DOMAIN job.$BK_DOMAIN jobapi.$BK_DOMAIN\n./scripts/control_coredns.sh update \"$IP1\" devops.$BK_DOMAIN\n./scripts/control_coredns.sh list  # 检查添加的记录。\n```\n\n确认注入结果，执行如下命令：\n```shell\ncd ~/bkce7.1-install/blueking/  进入工作目录\n./scripts/control_coredns.sh list\n```\n\n参考输出如下：\n```shell\n        172.27.124.109 bkce.diezhi.net\n        172.27.124.109 bkrepo.bkce.diezhi.net\n        172.27.124.109 docker.bkce.diezhi.net\n        172.27.124.109 bkapi.bkce.diezhi.net\n        172.27.124.109 bkpaas.bkce.diezhi.net\n        172.27.124.109 bkiam-api.bkce.diezhi.net\n        172.27.124.109 bkiam.bkce.diezhi.net\n        172.27.124.109 apps.bkce.diezhi.net\n        172.27.124.109 bknodeman.bkce.diezhi.net\n        172.27.124.109 job.bkce.diezhi.net\n        172.27.124.109 jobapi.bkce.diezhi.net\n        172.27.124.109 devops.bkce.diezhi.net\n```\n\n## 3.6 部署或对接存储服务\n### 3.6.1 部署蓝鲸预置的存储服务\n参考[官方文档](https://bk.tencent.com/docs/markdown/ZH/DeploymentGuides/7.1/storage-services.md)安装，相关helm配置已经放在[gitlab仓库](https://opsgit.papegames.com/infra/devops)上维护，可以直接简单执行以下命令：\n```shell\nhelmfile -f base-storage.yaml.gotmpl sync\n```\n\n### 3.6.2 对接已有的存储服务\n禁用蓝鲸内置服务，配置使用已有服务。helmfile 定义及 values 文件已经放在gitlab仓库上维护。\n\n此处可直接跳过。\n\n## 3.7 部署基础套餐\n通过helmfile安装 base-blueking.yaml.gotmpl ，按照顺序依次安装。具体每层安装的内容可以查看文件内容。\n参考[官方文档](https://bk.tencent.com/docs/markdown/ZH/DeploymentGuides/7.1/storage-services.md)安装，相关helm配置已经放在gitlab仓库上维护，可以直接简单执行以下命令：\n\n```shell\nhelmfile -f base-blueking.yaml.gotmpl -l seq=first sync\nhelmfile -f base-blueking.yaml.gotmpl -l seq=second sync\nhelmfile -f base-blueking.yaml.gotmpl -l seq=third sync\n# helmfile -f base-blueking.yaml.gotmpl -l seq=fourth sync\n```\n\n## 3.8 访问蓝鲸桌面\n在负载均衡器配置后端为 ingress-nginx pod 所在机器的内网 IP，端口为 80。详细信息参考[文档](https://bk.tencent.com/docs/markdown/ZH/DeploymentGuides/7.1/manual-install-bkce.md)。\n\n### 3.8.1 查找ingress nginx svc\n找到ingress nginx的svc\n```shell\n# pops\nkubectl get svc -n kube-system|grep ingress\n\n# pops-dev\nkubectl get svc -n ingress-nginx|grep ingress\n```\n\n结果：\n```shell\n# pops\ningress-nginx-controller-admission        ClusterIP      172.26.10.30    <none>          443/TCP                        3y69d\nnginx-ingress-lb                          LoadBalancer   172.26.1.155    10.212.14.158   80:30725/TCP,443:31357/TCP     3y69d\n\n# pops-dev\ningress-nginx-controller             NodePort    172.27.124.109   <none>        80:32080/TCP,443:32443/TCP   279d\ningress-nginx-controller-admission   ClusterIP   172.27.123.38    <none>        443/TCP                      279d\ningress-nginx-controller-metrics     ClusterIP   172.27.116.175   <none>        10254/TCP                    279d\n```\n\n这里`nginx-ingress-lb`是目前`pops集群`的`ingress nginx svc`，而`ingress-nginx-controller`是目前`pops-dev集群`的`ingress nginx svc`。\n\n### 3.8.2 设置svc type为LoadBalancer \n查看svc type：\n```shell\n# 查看pops集群ingress nginx svc命令\nkubectl get svc/nginx-ingress-lb -n kube-system -oyaml|grep type\n\n# 查看pops-dev集群ingress nginx svc命令\nkubectl get svc/ingress-nginx-controller -n ingress-nginx -oyaml|grep type\n```\n\n若无`type: LoadBalancer`结果，则手动进行修改：\n```shell\n# 修改pops集群ingress nginx svc命令\nkubectl edit svc/nginx-ingress-lb -n kube-system\n\n# 修改pops-dev集群ingress nginx svc命令\nkubectl edit svc/ingress-nginx-controller -n ingress-nginx\n```\n\n### 3.8.3 负载均衡clb实例\n查看svc yaml是否包含两个重要annotation：\n```shell\n# 查看pops集群ingress nginx svc命令\nkubectl get svc/nginx-ingress-lb -n kube-system -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-force-override-listeners\nkubectl get svc/nginx-ingress-lb -n kube-system -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id\n\n# 查看pops-dev集群ingress nginx svc命令\nkubectl get svc/ingress-nginx-controller -n ingress-nginx -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-force-override-listeners\nkubectl get svc/ingress-nginx-controller -n ingress-nginx -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id\n```\n\n若这两个重要annotation缺失，则需进行设置。\n\n负载均衡控制台地址：https://slb.console.aliyun.com/overview\n\n找到`pops-dev`集群的clb：`pops-k8s-dev-ingress`\n![img_12.png](img_12.png)\n\n找到`pops`集群的clb：`k8s-pops-ingress-slb`\n![img_14.png](img_14.png)\n\n```shell\n# 修改pops集群ingress nginx svc命令\n# pops集群clb名称：k8s-pops-ingress-slb\n# pops集群clb id：lb-bp1tt6vxctuzi38mqfkw0\nkubectl edit svc/nginx-ingress-lb -n kube-system\n\n# 修改pops-dev集群ingress nginx svc命令\n# pops-dev集群clb名称：pops-k8s-dev-ingress\n# pops-dev集群clb id：lb-bp1r1dsywnqktp1hxih38\nkubectl edit svc/ingress-nginx-controller -n ingress-nginx\n```\n\n### 3.8.4 访问地址\n查看浏览器访问地址：\n```shell\ncd ~/bkce7.1-install/blueking/  # 进入工作目录\n\n# 从自定义配置中提取\nBK_DOMAIN=$(yq e '.domain.bkDomain' environments/default/custom.yaml)  \n\n# 普通登录地址(接入统一登录，且登录成功后会跳转蓝盾)\necho \"http://$BK_DOMAIN\"\n\n# admin登录地址(接入统一登录后，可通过此方法适用admin登录)\necho \"http://${BK_DOMAIN}/login/origin/\"\n# 测试环境一般结果为：http://bkce.diezhi.net/login/origin/\n# 生产环境一般结果为：https://bk.diezhi.net/login/origin/\n\n# 查看用户密码(预设原始密码)：\nkubectl get cm -n blueking bk-user-api-general-envs -o go-template='user={{.data.INITIAL_ADMIN_USERNAME}}{{\"\\n\"}}password={{ .data.INITIAL_ADMIN_PASSWORD }}{{\"\\n\"}}'\n```\n\n## 3.9 对接Ldap服务\n在用户中心里配置Ldap相关配置，然后更新 bk-user-api-web 服务的镜像。\n![img_22.png](img_22.png)\n![img_23.png](img_23.png)\n\n\n# 4. 部署蓝盾\n参考[官方文档](https://bk.tencent.com/docs/markdown/ZH/DeploymentGuides/7.1/install-ci-suite.md)部署，配置 custom values 的内容提前修改完成，执行类似部署基础服务的以下命令：\n\n```shell\ncd ~/bkce7.1-install/blueking/  # 进入工作目录\nhelmfile -f 03-bkci.yaml.gotmpl sync  # 部署\nhelmfile -f 03-bkci.yaml.gotmpl apply # 更新\n```\n剩下的步骤参考官方文档执行即可，主要步骤有以下三个，其他的步骤可以不做。\n\n## 4.1 [可选]注册默认构建镜像\n我们提供了 bkci/ci 镜像用于提供构建环境。为了加速镜像下载过程，可以修改镜像地址为 hub.bktencent.com/bkci/ci，或者为你自己托管的内网 registry。\n```shell\nkubectl exec -it -n blueking bk-ci-mysql-0 -- /bin/bash -c 'MYSQL_PWD=\"$MYSQL_ROOT_PASSWORD\" mysql -u root -e \"USE devops_ci_store; SELECT IMAGE_NAME,IMAGE_CODE,IMAGE_REPO_NAME FROM T_IMAGE WHERE IMAGE_CODE = \\\"bkci\\\" ;\"'\n```\n\n请根据结果进行操作：\n- 如果有显示镜像数据，可以修改镜像地址为蓝鲸国内仓库，也可改为你已经缓存在内网的镜像：\n```shell\nkubectl exec -it -n blueking bk-ci-mysql-0 -- /bin/bash -c 'MYSQL_PWD=\"$MYSQL_ROOT_PASSWORD\" mysql -u root -e \"USE devops_ci_store; UPDATE  T_IMAGE SET IMAGE_REPO_NAME=\\\"hub.bktencent.com/bkci/ci\\\" WHERE IMAGE_CODE = \\\"bkci\\\" ;\"'\n```\n\n- 然后重新查询数据库，可以看到 IMAGE_REPO_NAME 列已经更新。\n- 如果没有镜像，可以新增：\n```shell\nkubectl exec -n blueking deploy/bk-ci-bk-ci-store -- \\curl -vs http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/market/image/init -X POST \\-H 'X-DEVOPS-UID: admin' -H 'Content-type: application/json' -d '{\"imageCode\":\"bkci\",\"imageName\":\"bkci\",\"imageRepo\":\"hub.bktencent.com/bkci/ci\",\"projectCode\":\"demo\",\"userId\":\"admin\"}' | jq .\n```\n\n> - 提示\n> - 当你单独卸载蓝盾重装后，可能出现查询镜像为空，但是新增镜像时报错 { status: 400, message: \"权限中心创建项目失败\" } 的情况。这是因为权限中心存在蓝盾 demo 项目的数据所致，我们后续会优化蓝盾单独卸载的文档。请先手动新建项目，并修改上述代码中 projectCode 字段的值。\n\n### 4.1.1 解决权限中心创建项目失败\n在 中控机 执行：\n```shell\ncd ~/bkce7.1-install/blueking/  # 进入工作目录\n./scripts/control_coredns.sh list  # 检查添加的记录。\n```\n\n会得到类似的结果：\n```shell\n        172.27.124.109 devops.bkce.diezhi.net\n        172.27.124.109 bkce.diezhi.net\n        172.27.124.109 bkrepo.bkce.diezhi.net\n        172.27.124.109 docker.bkce.diezhi.net\n        172.27.124.109 bkapi.bkce.diezhi.net\n        172.27.124.109 bkpaas.bkce.diezhi.net\n        172.27.124.109 bkiam-api.bkce.diezhi.net\n        172.27.124.109 bkiam.bkce.diezhi.net\n        172.27.124.109 apps.bkce.diezhi.net\n        172.27.124.109 bknodeman.bkce.diezhi.net\n        172.27.124.109 job.bkce.diezhi.net\n        172.27.124.109 jobapi.bkce.diezhi.net\n```\n\n使用admin账号通过浏览器访问devops.bkce.diezhi.net，并创建项目：\n![img_15.png](img_15.png)\n\n![img_16.png](img_16.png)\n\n![img_17.png](img_17.png)\n\n此时重新新增：\n```shell\nkubectl exec -n blueking deploy/bk-ci-bk-ci-store -- \\curl -vs http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/market/image/init -X POST \\-H 'X-DEVOPS-UID: admin' -H 'Content-type: application/json' -d '{\"imageCode\":\"bkci\",\"imageName\":\"bkci\",\"imageRepo\":\"hub.bktencent.com/bkci/ci\",\"projectCode\":\"huari-demo\",\"userId\":\"admin\"}' | jq .\n```\n\n如新建项目遇见可授权人员范围加载不出来(白屏转圈圈)\n\n需要确定跳转从蓝鲸跳转蓝盾是否是走了https，本文档是使用http部署，所以使用https会出现问题\n![img_18.png](img_18.png)\n\n## 4.2 [可跳过]对接制品库\n蓝盾依靠蓝鲸制品库来提供流水线仓库和自定义仓库，需要调整制品库的认证模式。\n\n当 `bk-ci` release 成功启动后，我们开始配置蓝鲸制品库，并注册到蓝盾中。\n\n### 4.2.1 修改 bk-repo custom values\n相关配置已经放在gitlab仓库上维护，可以直接跳过本步骤。\n\n[可跳过]请在 中控机 执行：\n```shell\ncd ~/bkce7.1-install/blueking/  # 进入工作目录\ncase $(yq e '.auth.config.realm' environments/default/bkrepo-custom-values.yaml.gotmpl 2>/dev/null) in\n  null|\"\")\n    tee -a environments/default/bkrepo-custom-values.yaml.gotmpl <<< $'auth:\\n  config:\\n    realm: devops'\n  ;;\n  devops)\n    echo \"environments/default/bkrepo-custom-values.yaml.gotmpl 中配置了 .auth.config.realm=devops, 无需修改.\"\n  ;;\n  *)\n    echo \"environments/default/bkrepo-custom-values.yaml.gotmpl 中配置了 .auth.config.realm 为其他值, 请手动修改值为 devops.\"\n  ;;\nesac\n```\n\n[可跳过]修改成功后，继续在工作目录执行如下命令使修改生效：\n```shell\nhelmfile -f base-blueking.yaml.gotmpl -l name=bk-repo apply\n```\n\n### 4.2.2 检查配置是否生效\n检查 release 生效的 values 和 configmap 是否重新渲染。\n\n[可跳过]请在 中控机 执行：\n```shell\nhelm get values -n blueking bk-repo | yq e '.auth.config.realm'\nkubectl get cm -n blueking bk-repo-bkrepo-auth -o json | jq -r '.data.\"application.yml\"' | yq e '.auth.realm' -\n```\n\n### 4.2.3 重启 bk-repo auth 微服务\n因为对接制品库的相关信息已经在gitlab仓库上维护了，所以此处不用进行重启。\n\n[可跳过]因为 deployment 没有变动，所以不会自动重启，此处需要单独重启：\n```shell\nkubectl rollout restart deployment -n blueking bk-repo-bkrepo-auth\n```\n\n### 4.2.4 在蓝盾中注册制品库\n[可跳过]请在 中控机 执行：\n```shell\ncd ~/bkce7.1-install/blueking/  # 进入工作目录\nBK_DOMAIN=$(yq e '.domain.bkDomain' environments/default/custom.yaml)  # 从自定义配置中提取, 也可自行赋值# 向project微服务注册制品库\nkubectl exec -i -n blueking deploy/bk-ci-bk-ci-project -- curl -sS -X PUT -H 'Content-Type: application/json' -H 'Accept: application/json' -H 'X-DEVOPS-UID: admin' -d \"{\\\"showProjectList\\\":true,\\\"showNav\\\":true,\\\"status\\\":\\\"ok\\\",\\\"deleted\\\":false,\\\"iframeUrl\\\":\\\"//bkrepo.$BK_DOMAIN/ui/\\\"}\" \"http://bk-ci-bk-ci-project.blueking.svc.cluster.local/api/op/services/update/Repo\"\n```\n\n## 4.3 [可选]下载和上传插件\n### 4.3.1 下载插件\n请在 中控机 执行：\n```shell\nbkdl-7.1-stable.sh -ur latest ci-plugins\n```\n\n### 4.3.2 上传插件\n此操作只能新建插件，每个插件只能上传一次。\n```shell\ncd ~/bkce7.1-install/blueking/  # 进入工作目录\nfor f in ../ci-plugins/*.zip; do\n    atom=\"${f##*/}\"\n    atom=${atom%.zip}\n    echo >&2 \"upload $atom from $f\"\n    kubectl exec -i -n blueking deploy/bk-ci-bk-ci-store -- \\\n      curl -s \\\n      http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/pipeline/atom/deploy/\"?publisher=admin\" \\\n      -H 'X-DEVOPS-UID: admin' -F atomCode=$atom -F file=@- < \"$f\" | jq .\n      # 设置为默认插件，全部项目可见。\n    kubectl exec -n blueking deploy/bk-ci-bk-ci-store -- \\\n    curl -s http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/pipeline/atom/default/atomCodes/$atom \\-H 'X-DEVOPS-UID: admin' -X POST | jq .\ndone\n```\n\n### 4.3.3 问题及解决方案\n`这个问题已经在蓝盾的helm charts里面进行了优化`\n\n上传插件时会碰到这个问题：\n![img_19.png](img_19.png)\n\n蓝盾官方文档说法是：\n![img_20.png](img_20.png)\n\n实际原因是：HTTP请求报了 413 Request Entity Too Large\n\n具体解决方式详见：[Ingress 域名方式导致413 Request Entity Too Large-阿里云开发者社区](https://developer.aliyun.com/article/1001630)\n\n造成这个问题的主要原因是nginx-ingress的默认配置中proxy-body-size的数值太小\n![img_21.png](img_21.png)\n\n# 5. 支持https\n如果开始就准备好了相关证书，那么可以将该步骤提前，在部署基础服务和蓝盾之前就先修改好相关的yaml，将需要创建的Secret和要更新的Ingress配置都提前修改好，然后直接部署即可。\n\n## 5.1 购买相关证书\n涉及的域名：bk.blazehu.com、*.bk.blazehu.com（如devops.bk.blazehu.com）。需购买泛域名证书。\n\n## 5.2 创建相关Secret（用于存储TLS证书和私钥）\n```shell\n# 创建Secret\nBK_DOMAIN=$(yq e '.domain.bkDomain' environments/default/custom.yaml)\ncd $HOME/$BK_DOMAIN\nkubectl create secret tls $BK_DOMAIN -n blueking --cert=$HOME/$BK_DOMAIN/$BK_DOMAIN.pem --key=$HOME/$BK_DOMAIN/$BK_DOMAIN.key\n————————————————\n本文链接：https://blazehu.com/2024/05/24/devops/landun_install/\n版权声明： 本博客所有文章除特别声明外，均采用 CC BY 4.0 CN协议 许可协议。转载请注明出处！\n```\n\n## 5.3 更新 Ingress TLS\n在证书及证书secret准备好之后，需要变更蓝鲸系列ingress开启tls的支持，执行对应的脚本\n```shell\n#!/bin/bash\n\n# 配置变量\nNAMESPACE=\"blueking\"\nDOMAIN_FILE=\"environments/default/custom.yaml\"\nBK_DOMAIN=$(yq e '.domain.bkDomain' \"$DOMAIN_FILE\")  # 从配置文件中读取域名\nTLS_HOST=\"*.$BK_DOMAIN\"  # 泛域名\nTLS_SECRET=\"$BK_DOMAIN\"  # Secret 名称与域名一致\n\n# 检查域名和 Secret 是否正确\nif [[ -z \"$BK_DOMAIN\" ]]; then\n  echo \"Error: BK_DOMAIN is not set in $DOMAIN_FILE.\"\n  exit 1\nfi\n\n# 获取命名空间中的所有 Ingress 资源\ningresses=$(kubectl get ingress -n \"$NAMESPACE\" -o jsonpath='{.items[*].metadata.name}')\n\n# 遍历所有 Ingress 资源并更新 TLS 配置\nfor ingress in $ingresses; do\n  echo \"Updating Ingress: $ingress in namespace: $NAMESPACE\"\n  \n  # 检查 Ingress 是否已存在 TLS 配置\n  if kubectl get ingress \"$ingress\" -n \"$NAMESPACE\" -o jsonpath='{.spec.tls}' | grep -q \"$TLS_HOST\"; then\n    echo \"TLS configuration for $TLS_HOST already exists in Ingress $ingress. Skipping.\"\n    continue\n  fi\n\n  # 更新 Ingress 的 TLS 配置\n  kubectl patch ingress \"$ingress\" -n \"$NAMESPACE\" --type=json -p='[\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/tls\",\n      \"value\": [\n        {\n          \"hosts\": [\"'\"$TLS_HOST\"'\"],\n          \"secretName\": \"'\"$TLS_SECRET\"'\"\n        }\n      ]\n    }\n  ]' || { echo \"Failed to update Ingress $ingress\"; exit 1; }\n\n  echo \"Updated Ingress $ingress with TLS configuration for $TLS_HOST.\"\ndone\n\necho \"All Ingress resources in namespace $NAMESPACE have been updated with TLS configuration for $TLS_HOST.\"\n```\n\n## 5.4 配置蓝鲸启用HTTPS\n在git仓库维护，主要有两个变更：\n\nenvironments/default/custom.yaml: .bkDomainScheme 值设置为 https\nenvironments/default/bkci/bkci-custom-values.yaml.gotmpl: .config.bkHttpSchema 值设置为 https\n```shell\nyq -i '.bkDomainScheme = \"https\"' environments/default/custom.yaml\n\n# 将bkHttpSchema: https替换为bkHttpSchema: http\nsed -i 's|bkHttpSchema: http|bkHttpSchema: https|' environments/default/bkci/bkci-custom-values.yaml.gotmpl\n```\n\n## 5.5 构建机Agent配置变更及重启\n```shell\n# 停止agent服务\n./stop.sh\n\nBK_DOMAIN=\"deveops.bk.blazehu.com\"\n\n# 修改.agent.properties文件，开启https\nsed -i '' 's|http://$BK_DOMAIN|https://$BK_DOMAIN|g' .agent.properties\n# 修改telegraf.conf文件，开启https\nsed -i '' 's|http://$BK_DOMAIN|https://$BK_DOMAIN|g' telegraf.conf\n\n# 启动agent\n./start.sh\n# 这里需要注意，仔细查看.agent.properties里devops.agent.user， 这里是哪个用户就用哪个用户启动agent\n```"},{"title":"Kustomize与Helm对比","url":"/2025/07/kubernetes/operator-kai-fa/kustomize-yu-helm-dui-bi/","content":"# 0、前言\nK8s 是一个开源容器编排平台，可自动执行容器化应用程序的部署、扩展和管理。近年来，K8s 已成为采用云原生架构和容器化技术的组织的标准。\n\n但是由于K8s的复杂性，所以很多公司以及开源组织都在开发相关的工具来简化k8s的使用门槛，这其中就包括两个很优秀的开源工具，Kustomize\n（K8s 的配置管理器）和Helm （K8s 的包管理器）。\n\n本文将针对二者来进行对比。\n\n|   |  Kustomize | Helm  |\n|---|---|---|\n| 操作方法  |  overlays | templating  |\n|  使用成本 |  简单 |  复杂 |\n| 是否支持封装  | 简单  |  是 |\n| kubectl集成  | 是  |  否 |\n| kubectl集成  | 声明式  | 命令式  |\n\n# 1、Kustomize\nKustomize 是 k8s集群的配置定制工具。它允许管理员使用非模板文件进行声明性更改，而不影响原始清单文件。\n\n来看一下kubebuilder生成项目的Kustomize配置，以/config/crd目录为例：\n```shell\n├── crd\n│   ├── bases\n│   │   └── apps.kubenode.alibaba-inc.com_myapps.yaml\n│   ├── kustomization.yaml\n│   ├── kustomizeconfig.yaml\n│   └── patches\n│       ├── cainjection_in_myapps.yaml\n│       └── webhook_in_myapps.yaml\n```\n\n其中config/crd目录是执行`kubebuilder create api`后生成的，最原始的目录结构是：\n```shell\n├── crd\n│   ├── kustomization.yaml\n│   └── kustomizeconfig.yaml\n```\n\n所以bases子目录和patches子目录都是执行`make manifests`后生成的。\n\n执行`kubebuilder create api`可以参考：https://blog.csdn.net/qq_41004932/article/details/142702284\n\n执行`make manifests`可以参考：https://blog.csdn.net/qq_41004932/article/details/142703870\n\n在 Kubebuilder 生成的项目中，config/crd 目录下的文件主要用于管理和配置自定义资源定义（Custom Resource Definitions, CRDs）。这些文件通过 Kustomize 工具进行管理，以便于在不同的环境中部署和管理 CRDs。下面是每个文件的作用解释：\n\n1. bases 目录\n   bases/apps.kubenode.alibaba-inc.com_myapps.yaml:\n   这个文件包含了自定义资源定义（CRD）的 YAML 描述。它定义了你的自定义资源（Custom Resource, CR）的结构和元数据。\n   例如，如果我们创建了一个名为 MyApp 的自定义资源，这个文件将描述 MyApp 的 API 版本、资源名称、字段等。\n2. kustomization.yaml:\n   这个文件是 Kustomize 的配置文件，用于定义如何组合和修改 Kubernetes 资源文件。\n   它指定了哪些资源文件（如 CRD 文件）需要被应用，并且可以包含补丁文件和其他配置选项。\n   例如，它可以指定 bases 目录中的 CRD 文件，以及 patches 目录中的补丁文件。\n3. kustomizeconfig.yaml:\n   这个文件通常用于配置 Kustomize 的一些高级选项，但在这个上下文中可能不是必需的。它可能会包含一些特定的配置项，用于定制 Kustomize 的行为。\n   在大多数情况下，这个文件可能不需要手动编辑。\n4. patches/cainjection_in_myapps.yaml:\n   这个文件是一个补丁文件，用于在 CRD 上应用 Webhook 注入（例如，证书注入）。\n   例如，如果你的 CRD 需要与 mutating 或 validating webhooks 一起工作，这个补丁文件会确保 Webhook 配置正确地注入到 CRD 中。\n5. patches/webhook_in_myapps.yaml:\n   这个文件也是一个补丁文件，用于在 CRD 上应用 Webhook 配置。\n   例如，它可能会添加或修改 Webhook 的配置，以便在创建或更新自定义资源时触发特定的行为。\n\n总结\n\nbases 目录：包含 CRD 的基本定义文件。\n\nkustomization.yaml：Kustomize 的配置文件，用于定义如何组合和修改资源文件。\n\nkustomizeconfig.yaml：Kustomize 的高级配置文件（可选）。\n\npatches 目录：包含用于修改 CRD 的补丁文件，通常用于注入 Webhook 配置。\n\n如果我们像让这个crd根据不同的环境做针对性部署的，例如下面的目录结构:\n```shell\n├── crd\n│   ├── bases\n│   │   └── apps.kubenode.alibaba-inc.com_myapps.yaml\n│   ├── development\n│   │   ├── kustomization.yaml\n│   │   └── patches\n│   │       └── webhook_in_myapps.yaml\n│   ├── testing\n│   │   ├── kustomization.yaml\n│   │   └── patches\n│   │       └── webhook_in_myapps.yaml\n│   ├── production\n│   │   ├── kustomization.yaml\n│   │   └── patches\n│   │       └── webhook_in_myapps.yaml\n│   ├── kustomization.yaml\n│   ├── kustomizeconfig.yaml\n│   └── patches\n│       ├── cainjection_in_myapps.yaml\n│       └── webhook_in_myapps.yaml\n```\n\n所有自定义规范都包含在 kustomization.yaml 文件中，该文件将规范叠加在现有清单之上以生成资源的自定义版本。\n\n所以我们可以根据这一特性，针对不同的环境，对crd进行定制。\n\n# 2、Helm\nHelm 是一个能够在 K8s 上打包、部署和管理应用程序的工具，即使是最复杂的 K8s 应用程序它都可以帮助定义，安装和升级，同时Helm 也是 [CNCF](https://cncf.io/) 的毕业项目。\n这里涉及到了以及关于helm的重要概念：\n- Helm Charts：预先配置yaml的模板，在这里叫Chart，用于描述 K8s 应用程序的yaml和配置\n- Helm Client：用于与 Helm 交互并管理这些Chart版本的命令行界面\n- Chart 仓库：管理Chart的仓库，跟Maven的Nexus一个意思，比如在公司环境构建上传，在客户的机房连接到这Chart 仓库下载Chart，并部署到k8s中。\n\n# 2.1 helm示例\nhelm的示例需要用到kubectl、helm以及k8s集群，相应的安装参考：\n- mac环境：https://blog.csdn.net/qq_41004932/article/details/142684319\n- ubuntu环境：https://blog.csdn.net/qq_41004932/article/details/142691490\n- kind集群：https://blog.csdn.net/qq_41004932/article/details/142691490\n\nHelm Charts 是预先配置的 K8s 资源包。Helm Chart 包含部署特定应用程序或服务所需的所有信息，包括 K8s 清单、环境变量和其他配置\n\n目录名称是Chart的名称，如[Helm 文档](https://helm.sh/docs/topics/charts/)所示，我们通过helm create demo命令创建一个Chart，执行完以后，默认会生成一个 nginx 的Chart。\n\n```shell\nhelm create demo\n```\n\n结果：\n```shell\n$ helm create demo                                                                                                                      [10:33:35]\nCreating demo\n```\n\n查看目录结构：\n```shell\n.\n├── Chart.yaml\n├── charts\n├── templates\n│   ├── NOTES.txt\n│   ├── _helpers.tpl\n│   ├── deployment.yaml\n│   ├── hpa.yaml\n│   ├── ingress.yaml\n│   ├── service.yaml\n│   ├── serviceaccount.yaml\n│   └── tests\n│       └── test-connection.yaml\n└── values.yaml\n\n3 directories, 10 files\n```\n\n## 2.2 Chart.yaml\n定义了当前 chart版本，以及描述当前chart用途，其中 name 参数表示 chart 名称，后期上传下载都会用此名称\n```shell\napiVersion: v2\nname: demo\ndescription: A Helm chart for Kubernetes\n\n# A chart can be either an 'application' or a 'library' chart.\n#\n# Application charts are a collection of templates that can be packaged into versioned archives\n# to be deployed.\n#\n# Library charts provide useful utilities or functions for the chart developer. They're included as\n# a dependency of application charts to inject those utilities and functions into the rendering\n# pipeline. Library charts do not define any templates and therefore cannot be deployed.\ntype: application\n\n# This is the chart version. This version number should be incremented each time you make changes\n# to the chart and its templates, including the app version.\n# Versions are expected to follow Semantic Versioning (https://semver.org/)\nversion: 0.1.0\n\n# This is the version number of the application being deployed. This version number should be\n# incremented each time you make changes to the application. Versions are not expected to\n# follow Semantic Versioning. They should reflect the version the application is using.\n# It is recommended to use it with quotes.\nappVersion: \"1.16.0\"\n```\n\n## 2.3 values.yaml\n可变参数，都是在此文件中定义，在yaml模板中引用，比如：image.repository，而引用则通过.Values+变量的名进行引用\n```shell\n# Default values for demo.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nreplicaCount: 1\n\nimage:\n  repository: nginx\n  pullPolicy: IfNotPresent\n  # Overrides the image tag whose default is the chart appVersion.\n  tag: \"\"\n\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\npodAnnotations: {}\n\npodSecurityContext: {}\n  # fsGroup: 2000\n\nsecurityContext: {}\n  # capabilities:\n  #   drop:\n  #   - ALL\n  # readOnlyRootFilesystem: true\n  # runAsNonRoot: true\n  # runAsUser: 1000\n\nservice:\n  type: ClusterIP\n  port: 80\n\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nresources: {}\n  # We usually recommend not to specify default resources and to leave this as a conscious\n  # choice for the user. This also increases chances charts run on environments with little\n  # resources, such as Minikube. If you do want to specify resources, uncomment the following\n  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  # limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n```\n\n## 2.4 _helpers.tpl\n定义通用代码块，然后yaml 文件会通过 include 引用\n\n定义:\n```shell\n{{- define \"demo.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n```\n\n引用:\n```shell\n{{ include \"demo.fullname\" . }}\n```\n## 2.5 templates\n此目录主要存放的是要部署的 yaml文件模板，同时也包含_helpers.tpl文件，模板会引用values.yaml、Chart.yaml定义的参数，以及_helpers.tpl定义的通用代码块\n```shell\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"demo.fullname\" . }}\n  labels:\n    {{- include \"demo.labels\" . | nindent 4 }}\nspec:\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"demo.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      {{- with .Values.podAnnotations }}\n      annotations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"demo.selectorLabels\" . | nindent 8 }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"demo.serviceAccountName\" . }}\n      securityContext:\n        {{- toYaml .Values.podSecurityContext | nindent 8 }}\n      containers:\n        - name: {{ .Chart.Name }}\n          securityContext:\n            {{- toYaml .Values.securityContext | nindent 12 }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n            - name: http\n              containerPort: {{ .Values.service.port }}\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {{- toYaml .Values.resources | nindent 12 }}\n      {{- with .Values.nodeSelector }}\n      nodeSelector:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.affinity }}\n      affinity:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.tolerations }}\n      tolerations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n```\n\n```shell\nimage: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n```\n\n## 2.5 部署\n如果想部署的话，可以使用下面命令进行\n```shell\nhelm package helm-demo\n```\n\n可以发现，这里helm是将要部署的资源当作一个整体来进行操作的，也就是讲一个资源的所有yaml配置作为一个整体的形式来进行操作。\n\n# 3、主要差异\n## 3.1 操作方法\nKustomize 依赖特定于目录的kustomization.yaml文件来构建各个资源并对其进行更改。这些文件将补丁和覆盖应用到共享基文件夹中声明的资源，以提供自动化的多环境配置。\n\nHelm 通过引用value.yaml文件作为变量源，使用模板生成有效的 K8s 配置。模板目录托管 Helm Chart在部署期间用于创建资源的文件。\n\n## 3.2 便捷性\n从K8s 版本 1.14 开始，Kustomize 与 kubectl CLI 捆绑在一起，因此不需要掌握任何其他工具。Kustomize 支持声明式部署，并对每个文件使用纯 YAML，从而更容易使用。\n\nHelm 为K8s包管理任务添加了额外的抽象层，从而加快了希望简化集群配置和发布自动化的团队的学习曲线。Helm Chart 相对Kustomize复杂，不过功能更加强大。\n\n## 3.3 打包\nKustomize 缺乏的打包功能，并且每个资源都必须在基本文件夹中声明，并在覆盖kustomization.yaml文件中单独声明变体。\n\n而Helm将所有必需的K8s资源都打包到一个文件夹中，该文件夹可以根据需要重复使用。Helm 还允许设置应用程序默认值，并且使用values.yaml文件修改参数，从而注入引用的 yaml 文件中。\n\n## 3.4 原生 kubectl 集成\n从 K8s 1.14 版开始，Kustomize 就预装了 kubectl，Helm 并未与 K8s 预先集成，因此必须手动安装 Helm。\n\n## 3.5 Kustomize 与 Helm - 何时使用\n### 3.5.1 何时使用 Kustomize\nKustomize允许在不改变原始文件的情况下进行精确更改。 因此可以有以下场景\n- 应用配置的变体管理：当你需要管理多个环境（例如开发、测试、生产）中应用的变体时，Kustomize 是一个很好的选择。它允许你为不同的环境创建不同的配置，并使用一套基础配置来定义通用部分。\n- 持续集成和持续部署（CI/CD）流水线：Kustomize 可以与 CI/CD 工具集成，帮助你实现自动化部署。通过在流水线中使用 Kustomize，你可以根据需要生成特定环境的配置，并将其应用到集群中。\n\n### 3.5.2 何时使用 Helm\nHelm 将所有 K8s 对象封装到一个包中，减少了与各个yaml 文件的交互。除此之外，大多数第三方供应商还提供预构建的 Helm 图表，以简化将其产品部署到 K8s 中的过程。因此，Helm 通常是安装现成解决方案（例如监控、数据库和消息中间件等）的首选\n\n"},{"title":"operator部署验证","url":"/2025/07/kubernetes/operator-kai-fa/operator-bu-shu-yan-zheng/","content":"# 1、部署命令\n这个是很多博客教程都在使用的部署命令：\n```shell\nmake manifests\nmake install\nexport ENABLE_WEBHOOKS=false\nmake run\n```\n我们使用之前的demo来进行部署验证：[Kubernetes-Operator篇-02-脚手架熟悉](https://hua-ri.cn/2025/07/kubernetes/Operator%E5%BC%80%E5%8F%91/operator%E5%BC%80%E5%8F%91%E8%84%9A%E6%89%8B%E6%9E%B6/)\n\n这里面涉及到的makefile的配置可以参考：[Kubernetes-Operator篇-03-kubebuilder的Makefile文件熟悉](https://hua-ri.cn/2025/07/kubernetes/Operator%E5%BC%80%E5%8F%91/kubebuilder%E7%9A%84makefile%E6%96%87%E4%BB%B6/)\n\n下面让我来看看这些命令的含义，并且都是在干什么\n\n## 1.1 make manifests\n`make manifests`：\n```shell\n.PHONY: manifests\nmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.\n        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n```\n这里主要是为了生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象。\n\n在生成了WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象后，我们可以进行查看，但是不要手动去修改，因为后面的`make install`和`make run`等命令，都依赖该目标，并且因为都是使用的`.PHONY`关键字来申明的伪目标，所以每次执行`make install`和`make run`都会执行manifests依赖。\n\n所以单独执行make manifests的意义是什么呢，个人观点，在一个完整部署动作中，完成了代码变更后，是需要通过`make manifests`来进行配置的生成查看的。`但是如果在本部署动作中，不关心生成配置的具体情况，只想走一遍流程，或者直接测试，那其实忽略不执行也ok，毕竟后面每一步都会重新生成且覆盖。`\n\n## 1.2 make install\n`make install`：\n```shell\n.PHONY: install\ninstall: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.\n        $(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -\n```\n\n先运行manifests和kustomize目标，然后使用kustomize 构建config/crd目录中的资源，并使用kubectl将其应用到集群。\n\n这里manifests目标会生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象，并保存在config/crd/bases目录下。\n\n而kustomize目标是进行kustomize工具的下载，如果二进制文件不存在则下载，然后生成二进制文件的软链，二进制文件只在首次下载，后续都是仅更新软链。\n\n下面的命令是install实际执行的动作：\n```shell\n$(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -\n```\n\n## 1.3 make run\n`make run`：\n```shell\n.PHONY: run\nrun: manifests generate fmt vet ## Run a controller from your host.\n        go run ./cmd/main.go\n```\n先运行manifests、generate、fmt和vet目标，然后使用go run命令运行cmd/main.go文件\n\n这里manifests目标会生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象，并保存在config/crd/bases目录下。\n\n而generate 目标是使用controller-gen工具生成包含DeepCopy、DeepCopyInto, 和DeepCopyObject方法实现的代码。\n\n至于fmt和vet目标就是执行go的命令：`fmt ./...`和`go vet ./...`\n\n下面的命令是install实际执行的动作：\n```shell\ngo run ./cmd/main.go\n```\n\n### 1.4 环境变量ENABLE_WEBHOOKS\n环境变量ENABLE_WEBHOOKS控制着webhook的启用与否。\n\n这段代码是kubectl生成的operator项目的main函数中启用webhook逻辑的部分：\n```shell\nif os.Getenv(\"ENABLE_WEBHOOKS\") != \"false\" {\n                if err = (&appsv1.Myapp{}).SetupWebhookWithManager(mgr); err != nil {\n                        setupLog.Error(err, \"unable to create webhook\", \"webhook\", \"Myapp\")\n                        os.Exit(1)\n                }\n        }\n```\n\n这里很简单粗暴的根据环境变量ENABLE_WEBHOOKS来决定是否启用webhook，os.Getenv的定义:\n```shell\nfunc Getenv(key string) string {\n        testlog.Getenv(key)\n        v, _ := syscall.Getenv(key)\n        return v\n}\n\nfunc Getenv(key string) (value string, found bool) {\n        envOnce.Do(copyenv)\n        if len(key) == 0 {\n                return \"\", false\n        }\n\n        envLock.RLock()\n        defer envLock.RUnlock()\n\n        i, ok := env[key]\n        if !ok {\n                return \"\", false\n        }\n        s := envs[i]\n        for i := 0; i < len(s); i++ {\n                if s[i] == '=' {\n                        return s[i+1:], true\n                }\n        }\n        return \"\", false\n}\n```\n这里返回的值，可能是空字符串，也可能是bool类型值的字符串，但是只要值不是false，就会开启webhook。\n\n至于这里为什么要临时关闭webhook，因为启用webhook是需要证书的，也就是我们需要在本地安装cert-manager，并且还需要配置，在本实验项目就不搞太复杂了。\n\n# 2、CRD 调试\n## 2.1 make manifests\n```shell\nmake manifests\n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n\nls -lh config/crd/bases/\n总计 1.1M\n-rw-rw-r-- 1 king king 1.1M 10月  5 16:17 apps.kubenode.kingtest.com_myapps.yaml\n```\n\n## 2.2 make install\n命令：\n```shell\nmake install\n```\n\n### 2.2.1 make install错误：dial tcp 127.0.0.1:8080: connect: connection refused\n```shell\nmake install\n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n/home/king/workspace/king-devops/operator/myapp-operator/bin/kustomize build config/crd | kubectl apply -f -\nerror: error validating \"STDIN\": error validating data: failed to download openapi: Get \"http://localhost:8080/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nmake: *** [Makefile:131：install] 错误 1\n```\n\n这个错误很诡异： dial tcp 127.0.0.1:8080: connect: connection refused\n\n首先集群是存在的：\n```shell\nsudo kind get clusters\nmyk8s-test\n```\n\n其次已经使用下面的命令将集群信息，设置进去kubectl的上下文里：\n```shell\nsudo kubectl cluster-info --context kind-myk8s-test\n```\n\nsudo kubectl cluster-info --context kind-myk8s-test\n```shell\nsudo kubectl get nodes\nNAME                       STATUS   ROLES           AGE   VERSION\nmyk8s-test-control-plane   Ready    control-plane   43h   v1.31.0\nmyk8s-test-worker          Ready    <none>          43h   v1.31.0\nmyk8s-test-worker2         Ready    <none>          43h   v1.31.0\n```\n\n进一步测试，分别执行`kubectl version`和`kubectl cluster-info dump`都遇见了类似的错误：\n\n`kubectl version`：\n```shell\nkubectl version\nClient Version: v1.31.1\nKustomize Version: v5.4.2\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n```\n\n`kubectl cluster-info dump`：\n```shell\nkubectl cluster-info dump\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n```\n\n但是很悲催的是，忘记将kind集群的kubeconfig导出到本地，我的理解是，虽然kubectl上下文是有集群kubeconfig的，但是本地并没有，简单来说，就是这个信息是kubectl自身保存的，不是直接使用的系统上存储的kubeconfig\n\n使用下面的命令，将kubeconfig信息保存在`$HOME/.kube/config内`：\n\n```shell\nsudo kind export kubeconfig --name=myk8s-test --kubeconfig=$HOME/.kube/config\n```\n\n再执行`make install`命令，本问题已经解决，但是新的问题出现了：\n```shell\nmake install\n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n/home/king/workspace/king-devops/operator/myapp-operator/bin/kustomize build config/crd | kubectl apply -f -\nThe CustomResourceDefinition \"myapps.apps.kubenode.kingtest.com\" is invalid: metadata.annotations: Too long: must have at most 262144 bytes\nmake: *** [Makefile:131：install] 错误 1\n```\n\n### 2.2.2 make install错误：metadata.annotations: Too long: must have at most 262144 bytes\n\nkubebuilder的github issues中，修复过这个问题: https://github.com/kubernetes-sigs/kubebuilder/pull/2862/commits/2c5b9edf614444acd43ae5f65af1702a5ed63ed6\n\n\n修复方法：打开Makefile，在 manifests 命令处，修改 crd 为 crd:maxDescLen=0\n\n原始的：\n```shell\n.PHONY: manifests\nmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.\n        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n```\n\n修复后的：\n```shell\n.PHONY: manifests\nmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.\n        $(CONTROLLER_GEN) rbac:roleName=manager-role crd:maxDescLen=0 webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n```\n\n问题解决：\n```shell\nmake install\n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd:maxDescLen=0 webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n/home/king/workspace/king-devops/operator/myapp-operator/bin/kustomize build config/crd | kubectl apply -f -\ncustomresourcedefinition.apiextensions.k8s.io/my\n```\n\n### 2.2.3 make install验证\n查看安装结果：\n```shell\nkubectl get crd\nNAME                                CREATED AT\nmyapps.apps.kubenode.kingtest.com   2024-10-05T11:56:22Z\n```\n已经可以看到我们定义的crd了。\n\n## 2.3 make run\n命令：\n```shell\nexport ENABLE_WEBHOOKS=false\nmake run\n```\n\n执行结果：\n```shell\nmake run\n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd:maxDescLen=0 webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"\ngo fmt ./...\ngo vet ./...\ngo run ./cmd/main.go\n2024-10-05T20:29:02+08:00       INFO    setup   starting manager\n2024-10-05T20:29:02+08:00       INFO    starting server {\"name\": \"health probe\", \"addr\": \"[::]:8081\"}\n2024-10-05T20:29:02+08:00       INFO    Starting EventSource    {\"controller\": \"myapp\", \"controllerGroup\": \"apps.kubenode.kingtest.com\", \"controllerKind\": \"Myapp\", \"source\": \"kind source: *v1.Myapp\"}\n2024-10-05T20:29:02+08:00       INFO    Starting Controller     {\"controller\": \"myapp\", \"controllerGroup\": \"apps.kubenode.kingtest.com\", \"controllerKind\": \"Myapp\"}\n2024-10-05T20:29:02+08:00       INFO    Starting workers        {\"controller\": \"myapp\", \"controllerGroup\": \"apps.kubenode.kingtest.com\", \"controllerKind\": \"Myapp\", \"worker count\": 1}\n```\n\n已经成功启动！！！\n## 2.4 执行debug\n### 2.4.1 启动debug\n在上面执行了以下步骤：\n```shell\nmake manifests\nmake install\nexport ENABLE_WEBHOOKS=false\nmake run\n```\n我们先将make run运行的controller停止，然后打开operator项目，在项目的调谐函数内打上断点，然后直接以debug模式启动。\n\n打断点\n![img_5.png](img_5.png)\n\n别忘了设置环境变量，关闭webhook\n![img_6.png](img_6.png)\n\n以debug方式启动：\n![img_7.png](img_7.png)\n\n执行结果\n![img_8.png](img_8.png)\n\n接下来，我们来apply一个资源，测试一下。\n\n### 2.4.2 crd测试\n编写一个测试的yaml：\n```shell\napiVersion: apps.kubenode.kingtest.com/v1\nkind: Myapp\nmetadata:\n  name: myapp-sample\nspec:\n  foo: \"test value\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp-container\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n```\n\n将测试yaml通过kubectl apply进去集群:\n```shell\nsudo kubectl apply -f myapptest.yaml\nmyapp.apps.kubenode.kingtest.com/myapp-sample created\n```\n\n然后controller的调谐函数成功断住\n![img_9.png](img_9.png)\n\n再向下执行一步\n![img_10.png](img_10.png)\n\n在终端里已经成功打印出来代码里的日志：\n![img_11.png](img_11.png)\n\n通过kubectl查看crd状态：\n```shell\nsudo kubectl get Myapp -A -owide\nNAMESPACE   NAME           AGE\ndefault     myapp-sample   14m\n```\n到此都是符合预期的。\n\n# 4 构建CRD镜像并部署进k8s\n上面我们是在本地运行的controller，那么在实际中该怎么办呢。\n\n答案是将controller打包成docker镜像，然后部署进集群。\n\n构建镜像并推送至你的镜像仓库\n```shell\nmake docker-build docker-push IMG=<some-registry>/<project-name>:tag\n```\n\n看下makefile的定义：\n```shell\n.PHONY: docker-build\ndocker-build: ## Build docker image with the manager.\n        $(CONTAINER_TOOL) build -t ${IMG} .\n\n.PHONY: docker-push\ndocker-push: ## Push docker image with the manager.\n        $(CONTAINER_TOOL) push ${IMG}\n```\n\n这里就是使用CONTAINER_TOOL(默认为docker)构建并推送docker镜像，并使用IMG变量指定镜像名称和标签\n指定镜像将controller部署进你的集群\n```shell\nmake deploy IMG=<some-registry>/<project-name>:tag\n```\n\n查看makefile定义：\n```shell\n.PHONY: deploy\ndeploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.\n        cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}\n        $(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -\n```\n\n- 先运行manifests和kustomize目标\n- 使用kustomize设置controller镜像为IMG\n- 使用kustomize构建config/default目录中的资源，并使用kubectl将其应用到集群\n\n# 5 删除controller部署和卸载CRD\n删除controller部署：\n```shell\nmake undeploy\n```\n\n查看makefile定义：\n```shell\n.PHONY: undeploy\nundeploy: kustomize ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n        $(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n```\n\n使用kustomize 构建config/default目录中的资源，并使用kubectl将其从集群中删除。可以通过ignore-not-found=true忽略资源未找到的错误\n\n从集群卸载CRD：\n```shell\nmake uninstall\n```\n\n查看makefile定义：\n```shell\n.PHONY: uninstall\nuninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n        $(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n```\n- 先运行manifests和kustomize目标\n- 使用kustomize 构建config/crd目录中的资源，并使用kubectl将其从集群中删除，可以通过ignore-not-found=true忽略资源未找到的错误\n\n# 6 其他\n这是在我们本地运行的controller，我在最初一直想不通是怎么连接的集群的，知道想起了最初的client-go的使用。\n\n在看本文的同学，肯定是听说过，甚至使用过client-go的，我们可以使用client-go去实现一些小工具，比如说某个namespace下pod的查询，甚至可以获取node的列表，pod的全部信息等等，只要是kubectl可以做的，都可以通过client-go来实现，并且可以做更复杂的场景。\n\n我们是可以在工具里指定kubeconfig的路径的，让client-go去读取，并渲染对应的client，然后和对应集群的apiserver做交互。\n\n知道了这点，那么问题来了，operator是怎么做的呢，operator其实是使用的controller-runtime库，他和client-go有什么区别呢，他是client-go以及其他库的更上一层封装。\n\n所以虽然没具体看，但是controller-runtime肯定是默认读取的$HOME/.kube/config文件的kubeconfig，至于可不可以指定其他的kubeconfig，理论上是可以的，因为就是指定下kubeconfig的路径，然后去读取渲染client，这里和自己直接使用client-go去实现是一样的逻辑。\n\n但是，controller-runtime究竟支不支持，还需要自己去找一下源码或者资料，因为本地运行controller这本身肯定是不建议的，所以不支持也是说得过去的。\n\n"},{"title":"kubebuilder的makefile文件","url":"/2025/07/kubernetes/operator-kai-fa/kubebuilder-de-makefile-wen-jian/","content":"# 1、Makefile简介\nMakefile 是一种用于自动化构建软件项目的文件。它通常用于管理和执行编译、链接、测试等一系列任务，以提高开发效率。\n\n## 1.1 目标与依赖\n### 1.1.1目标（Targets）\n- 代表一个任务或一个文件的生成结果。例如，编译生成的可执行文件、库文件等都可以是目标。\n- 常见的目标有 “all”（表示构建整个项目）、“clean”（用于清理生成的文件）等。\n\n### 1.1.2 依赖（Dependencies）\n- 目标所依赖的其他文件或目标。只有当依赖发生变化时，才会重新执行生成目标的命令。\n- 例如，一个可执行文件可能依赖于多个源文件和库文件。\n\n## 1.2 规则与命令\n### 1.2.1 规则（Rules）\n- 描述了如何从依赖生成目标。通常由目标、依赖和命令三部分组成。\n- 格式一般为：target: dependencies，接着是命令部分，每行命令前面必须以制表符（Tab）开头。\n\n### 1.2.2 命令（Commands）\n- 用于执行生成目标的具体操作。可以是编译器命令、链接器命令、文件复制命令等。\n- 例如，编译 C 语言源文件的命令可能是gcc -o target source.c。\n\n## 1.3 变量与函数\n### 1.3.1 变量（Variables）\n- 可以定义和使用变量来存储常用的值，如编译器名称、编译选项、源文件列表等。\n- 变量定义方式为VARIABLE = value，使用时用$(VARIABLE)。\n\n### 1.3.2 函数（Functions）\n- Makefile 提供了一些内置函数，可以进行字符串处理、文件操作等。\n- 例如，$(wildcard *.c)可以获取当前目录下所有的 C 语言源文件。\n\n## 1.4 优势\n### 1.4.1 自动化构建\n- 可以自动执行一系列构建步骤，减少手动操作和错误。\n- 当源文件发生变化时，只重新构建受影响的部分，提高构建效率。\n\n### 1.4.2 可重复性\n确保在不同的环境中都能以相同的方式构建项目。\n\n### 1.4.3 易于维护\n项目的构建过程集中在一个文件中，便于修改和管理。\n\n# 2、kubebuilder makefile\n## 2.1 kubebuilder 项目makefile\n一个kubebuilder创建的operator项目的makefile文件内容：\n```shell\n# Image URL to use all building/pushing image targets\nIMG ?= controller:latest\n# ENVTEST_K8S_VERSION refers to the version of kubebuilder assets to be downloaded by envtest binary.\nENVTEST_K8S_VERSION = 1.31.0\n\n# Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)\nifeq (,$(shell go env GOBIN))\nGOBIN=$(shell go env GOPATH)/bin\nelse\nGOBIN=$(shell go env GOBIN)\nendif\n\n# CONTAINER_TOOL defines the container tool to be used for building images.\n# Be aware that the target commands are only tested with Docker which is\n# scaffolded by default. However, you might want to replace it to use other\n# tools. (i.e. podman)\nCONTAINER_TOOL ?= docker\n\n# Setting SHELL to bash allows bash commands to be executed by recipes.\n# Options are set to exit when a recipe line exits non-zero or a piped command fails.\nSHELL = /usr/bin/env bash -o pipefail\n.SHELLFLAGS = -ec\n\n.PHONY: all\nall: build\n\n##@ General\n\n# The help target prints out all targets with their descriptions organized\n# beneath their categories. The categories are represented by '##@' and the\n# target descriptions by '##'. The awk command is responsible for reading the\n# entire set of makefiles included in this invocation, looking for lines of the\n# file as xyz: ## something, and then pretty-format the target and help. Then,\n# if there's a line with ##@ something, that gets pretty-printed as a category.\n# More info on the usage of ANSI control characters for terminal formatting:\n# https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_parameters\n# More info on the awk command:\n# http://linuxcommand.org/lc3_adv_awk.php\n\n.PHONY: help\nhelp: ## Display this help.\n        @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n  make \\033[36m<target>\\033[0m\\n\"} /^[a-zA-Z_0-9-]+:.*?##/ { printf \"  \\033[36m%-15s\\033[0m %s\\n\", $$1, $$2 } /^##@/ { printf \"\\n\\033[1m%s\\033[0m\\n\", substr($$0, 5) } ' $(MAKEFILE_LIST)\n\n##@ Development\n\n.PHONY: manifests\nmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.\n        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n\n.PHONY: generate\ngenerate: controller-gen ## Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.\n        $(CONTROLLER_GEN) object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"\n\n.PHONY: fmt\nfmt: ## Run go fmt against code.\n        go fmt ./...\n\n.PHONY: vet\nvet: ## Run go vet against code.\n        go vet ./...\n\n.PHONY: test\ntest: manifests generate fmt vet envtest ## Run tests.\n        KUBEBUILDER_ASSETS=\"$(shell $(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path)\" go test $$(go list ./... | grep -v /e2e) -coverprofile cover.out\n\n# Utilize Kind or modify the e2e tests to load the image locally, enabling compatibility with other vendors.\n.PHONY: test-e2e  # Run the e2e tests against a Kind k8s instance that is spun up.\ntest-e2e:\n        go test ./test/e2e/ -v -ginkgo.v\n\n.PHONY: lint\nlint: golangci-lint ## Run golangci-lint linter\n        $(GOLANGCI_LINT) run\n\n.PHONY: lint-fix\nlint-fix: golangci-lint ## Run golangci-lint linter and perform fixes\n        $(GOLANGCI_LINT) run --fix\n\n##@ Build\n\n.PHONY: build\nbuild: manifests generate fmt vet ## Build manager binary.\n        go build -o bin/manager cmd/main.go\n\n.PHONY: run\nrun: manifests generate fmt vet ## Run a controller from your host.\n        go run ./cmd/main.go\n\n# If you wish to build the manager image targeting other platforms you can use the --platform flag.\n# (i.e. docker build --platform linux/arm64). However, you must enable docker buildKit for it.\n# More info: https://docs.docker.com/develop/develop-images/build_enhancements/\n.PHONY: docker-build\ndocker-build: ## Build docker image with the manager.\n        $(CONTAINER_TOOL) build -t ${IMG} .\n\n.PHONY: docker-push\ndocker-push: ## Push docker image with the manager.\n        $(CONTAINER_TOOL) push ${IMG}\n\n# PLATFORMS defines the target platforms for the manager image be built to provide support to multiple\n# architectures. (i.e. make docker-buildx IMG=myregistry/mypoperator:0.0.1). To use this option you need to:\n# - be able to use docker buildx. More info: https://docs.docker.com/build/buildx/\n# - have enabled BuildKit. More info: https://docs.docker.com/develop/develop-images/build_enhancements/\n# - be able to push the image to your registry (i.e. if you do not set a valid value via IMG=<myregistry/image:<tag>> then the export will fail)\n# To adequately provide solutions that are compatible with multiple platforms, you should consider using this option.\nPLATFORMS ?= linux/arm64,linux/amd64,linux/s390x,linux/ppc64le\n.PHONY: docker-buildx\ndocker-buildx: ## Build and push docker image for the manager for cross-platform support\n        # copy existing Dockerfile and insert --platform=${BUILDPLATFORM} into Dockerfile.cross, and preserve the original Dockerfile\n        sed -e '1 s/\\(^FROM\\)/FROM --platform=\\$$\\{BUILDPLATFORM\\}/; t' -e ' 1,// s//FROM --platform=\\$$\\{BUILDPLATFORM\\}/' Dockerfile > Dockerfile.cross\n        - $(CONTAINER_TOOL) buildx create --name myapp-operator-builder\n        $(CONTAINER_TOOL) buildx use myapp-operator-builder\n        - $(CONTAINER_TOOL) buildx build --push --platform=$(PLATFORMS) --tag ${IMG} -f Dockerfile.cross .\n        - $(CONTAINER_TOOL) buildx rm myapp-operator-builder\n        rm Dockerfile.cross\n\n.PHONY: build-installer\nbuild-installer: manifests generate kustomize ## Generate a consolidated YAML with CRDs and deployment.\n        mkdir -p dist\n        cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}\n        $(KUSTOMIZE) build config/default > dist/install.yaml\n\n##@ Deployment\n\nifndef ignore-not-found\n  ignore-not-found = false\nendif\n\n.PHONY: install\ninstall: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.\n        $(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -\n\n.PHONY: uninstall\nuninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n        $(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n\n.PHONY: deploy\ndeploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.\n        cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}\n        $(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -\n\n.PHONY: undeploy\nundeploy: kustomize ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n        $(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n\n##@ Dependencies\n\n## Location to install dependencies to\nLOCALBIN ?= $(shell pwd)/bin\n$(LOCALBIN):\n        mkdir -p $(LOCALBIN)\n\n## Tool Binaries\nKUBECTL ?= kubectl\nKUSTOMIZE ?= $(LOCALBIN)/kustomize\nCONTROLLER_GEN ?= $(LOCALBIN)/controller-gen\nENVTEST ?= $(LOCALBIN)/setup-envtest\nGOLANGCI_LINT = $(LOCALBIN)/golangci-lint\n\n## Tool Versions\nKUSTOMIZE_VERSION ?= v5.4.3\nCONTROLLER_TOOLS_VERSION ?= v0.16.1\nENVTEST_VERSION ?= release-0.19\nGOLANGCI_LINT_VERSION ?= v1.59.1\n\n.PHONY: kustomize\nkustomize: $(KUSTOMIZE) ## Download kustomize locally if necessary.\n$(KUSTOMIZE): $(LOCALBIN)\n        $(call go-install-tool,$(KUSTOMIZE),sigs.k8s.io/kustomize/kustomize/v5,$(KUSTOMIZE_VERSION))\n\n.PHONY: controller-gen\ncontroller-gen: $(CONTROLLER_GEN) ## Download controller-gen locally if necessary.\n$(CONTROLLER_GEN): $(LOCALBIN)\n        $(call go-install-tool,$(CONTROLLER_GEN),sigs.k8s.io/controller-tools/cmd/controller-gen,$(CONTROLLER_TOOLS_VERSION))\n\n.PHONY: envtest\nenvtest: $(ENVTEST) ## Download setup-envtest locally if necessary.\n$(ENVTEST): $(LOCALBIN)\n        $(call go-install-tool,$(ENVTEST),sigs.k8s.io/controller-runtime/tools/setup-envtest,$(ENVTEST_VERSION))\n\n.PHONY: golangci-lint\ngolangci-lint: $(GOLANGCI_LINT) ## Download golangci-lint locally if necessary.\n$(GOLANGCI_LINT): $(LOCALBIN)\n        $(call go-install-tool,$(GOLANGCI_LINT),github.com/golangci/golangci-lint/cmd/golangci-lint,$(GOLANGCI_LINT_VERSION))\n\n# go-install-tool will 'go install' any package with custom target and name of binary, if it doesn't exist\n# $1 - target path with name of binary\n# $2 - package url which can be installed\n# $3 - specific version of package\ndefine go-install-tool\n@[ -f \"$(1)-$(3)\" ] || { \\\nset -e; \\\npackage=$(2)@$(3) ;\\\necho \"Downloading $${package}\" ;\\\nrm -f $(1) || true ;\\\nGOBIN=$(LOCALBIN) go install $${package} ;\\\nmv $(1) $(1)-$(3) ;\\\n} ;\\\nln -sf $(1)-$(3) $(1)\nendef\n```\n\n## 2.2 变量定义部分\n### 2.2.1 IMG（定义构建和推送Docker镜像的目标镜像URL）\n```shell\n# Image URL to use all building/pushing image targets\nIMG ?= controller:latest\n```\n作用：定义构建和推送Docker镜像的目标镜像URL\n解释：如果没有在命令中指定IMG，则默认值为controller:latest\n\n### 2.2.2 ENVTEST_K8S_VERSION（定义envtest工具下载的Kubernetes资产版本）\n```shell\n# ENVTEST_K8S_VERSION refers to the version of kubebuilder assets to be downloaded by envtest binary.\nENVTEST_K8S_VERSION = 1.31.0\n```\n作用：定义envtest工具下载的Kubernetes资产版本\n解释：envtest是一个用于测试的工具，这个变量制定了要下载的Kubernetes版本。\n\n### 2.2.3 GOBIN（当前使用的Go的安装路径）\n```shell\n# Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)\nifeq (,$(shell go env GOBIN))\nGOBIN=$(shell go env GOPATH)/bin\nelse\nGOBIN=$(shell go env GOBIN)\nendif\n```\n\n作用：获取当前使用的Go的安装路径\n解释：如果GONBIN环境变量未设置，则使用GOPATCH/bin作为默认路径；否则使用GOBIN的值。\n\n### 2.2.4 CONTAINER_TOOL（用于构建镜像的容器工具）\n```shell\n# CONTAINER_TOOL defines the container tool to be used for building images.\n# Be aware that the target commands are only tested with Docker which is\n# scaffolded by default. However, you might want to replace it to use other\n# tools. (i.e. podman)\nCONTAINER_TOOL ?= docker\n```\n\n作用：定义用于构建镜像的容器工具\n解释：默认值为docker，但是可以替换为其他工具，例如podman\n\n### 2.2.5 SHELL 和 .SHELLFLAGS（设置Makefile使用的shell和shell选项）\n```shell\n# Setting SHELL to bash allows bash commands to be executed by recipes.\n# Options are set to exit when a recipe line exits non-zero or a piped command fails.\nSHELL = /usr/bin/env bash -o pipefail\n.SHELLFLAGS = -ec\n```\n\n作用：设置Makefile使用的shell和shell选项\n解释：\n- SHELL设置为/usr/bin/env bash -o pipefail\n- .SHELLFLAGS 设置为-ec，表示shell应该立即退出并在任何命令失败时报告错误\n\n## 2.3 目标定义部分\n### 2.3.1 通用目标\n#### 2.3.1.1 all（定义默认目标，调用build目标）\n```shell\n.PHONY: all\nall: build\n```\n作用：定义默认目标，调用build目标\n解释：当运行make时，如果没有指定目标。默认会执行build目标。\n\n#### 2.3.1.2 help（显示帮助信息）\n```shell\n.PHONY: help\nhelp: ## Display this help.\n        @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n  make \\033[36m<target>\\033[0m\\n\"} /^[a-zA-Z_0-9-]+:.*?##/ { printf \"  \\033[36m%-15s\\033[0m %s\\n\", $$1, $$2 } /^##@/ { printf \"\\n\\033[1m%s\\033[0m\\n\", substr($$0, 5) } ' $(MAKEFILE_LIST)\n```\n作用：显示帮助信息\n解释：使用awk命令解析Makefile,提取每个目标的描述，并按类别组织显示。##@表示类别，##表示目标描述\n\n### 2.3.2 开发目标\n#### 2.3.2.1 manifests（生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象）\n```shell\n.PHONY: manifests\nmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.\n        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=\"./...\" output:crd:artifacts:config=config/crd/bases\n```\n作用：生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象\n解释：使用controller-gen工具生成必要的Kubernetes资源文件，并保存到config/crd/bases目录\n\n#### 2.3.2.2 generate（生成包含DeepCopy、DeepCopyInto, 和DeepCopyObject方法实现的代码。）\n```shell\n.PHONY: generate\ngenerate: controller-gen ## Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.\n        $(CONTROLLER_GEN) object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"\n```\n作用：生成包含DeepCopy、DeepCopyInto, 和DeepCopyObject方法实现的代码。\n解释：使用controller-gen工具生成GO代码，这些代码自动生成对象的深拷贝方法。\n\n#### 2.3.2.3 fmt（格式化代码）\n```shell\n.PHONY: fmt\nfmt: ## Run go fmt against code.\n        go fmt ./...\n```\n作用：格式化代码\n解释：使用go fmt命令格式化项目中的所有Go代码\n\n#### 2.3.2.4 vet（检查代码中的潜在问题）\n```shell\n.PHONY: vet\nvet: ## Run go vet against code.\n        go vet ./...\n```\n作用：检查代码中的潜在问题\n解释：使用go vet命令检查项目中的所有Go代码，查找可能的问题。\n\n#### 2.3.2.5 test（运行测试）\n```shell\n.PHONY: test\ntest: manifests generate fmt vet envtest ## Run tests.\n        KUBEBUILDER_ASSETS=\"$(shell $(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path)\" go test $$(go list ./... | grep -v /e2e) -coverprofile cover.out\n```\n作用：运行测试\n解释：\n- 先运行manifests、generate、fmt和vet目标\n- 使用envtest工具设置环境变量KUBEBUILDER_ASSETS，然后运行项目中的所有测试(排除e2e测试)\n\n#### 2.3.2.6 test-e2e（运行端到端测试）\n```shell\n# Utilize Kind or modify the e2e tests to load the image locally, enabling compatibility with other vendors.\n.PHONY: test-e2e  # Run the e2e tests against a Kind k8s instance that is spun up.\ntest-e2e:\n        go test ./test/e2e/ -v -ginkgo.v\n```\n作用：运行端到端测试\n解释：使用go test命令运行test/e2e目录中的端到端测试，并启动详细输出。\n\n#### 2.3.2.7 lint（运行代码风格检查）\n```shell\n.PHONY: lint\nlint: golangci-lint ## Run golangci-lint linter\n        $(GOLANGCI_LINT) run\n```\n作用：运行代码风格检查\n解释：使用golangci-lint工具运行代码风格检查\n\n#### 2.3.2.8 lint-fix （运行代码风格检查并自动修复问题）\n```shell\n.PHONY: lint-fix\nlint-fix: golangci-lint ## Run golangci-lint linter and perform fixes\n        $(GOLANGCI_LINT) run --fix\n\n```\n作用：运行代码风格检查并自动修复问题\n解释：使用golangci-lint工具运行代码风格检查，并使用–fix选项自动修复发现的问题。\n\n### 2.3.3 构建目标\n#### 2.3.3.1 build（构建controller的二进制文件）\n```shell\n.PHONY: build\nbuild: manifests generate fmt vet ## Build manager binary.\n        go build -o bin/manager cmd/main.go\n```\n作用：构建controller的二进制文件\n解释：\n- 先运行manifests、generate、fmt和vet目标\n- 使用go build命令编译cmd/main.go文件，并将生成的二进制文件保存到bin/manager。\n\n#### 2.3.3.2 run（在主机上运行controller）\n```shell\n.PHONY: run\nrun: manifests generate fmt vet ## Run a controller from your host.\n        go run ./cmd/main.go\n```\n作用：在主机上运行controller\n解释：\n- 先运行manifests、generate、fmt和vet目标\n- 使用go run命令运行cmd/main.go文件\n\n#### 2.3.3.3 docker-build（构建Docker镜像）\n```shell\n# If you wish to build the manager image targeting other platforms you can use the --platform flag.\n# (i.e. docker build --platform linux/arm64). However, you must enable docker buildKit for it.\n# More info: https://docs.docker.com/develop/develop-images/build_enhancements/\n.PHONY: docker-build\ndocker-build: ## Build docker image with the manager.\n        $(CONTAINER_TOOL) build -t ${IMG} .\n```\n作用：构建Docker镜像\n解释：使用CONTAINER_TOOL(默认为docker)构建docker镜像，并使用IMG变量指定镜像名称和标签\n\n#### 2.3.3.4 docker-push（推送Docker镜像）\n```shell\n.PHONY: docker-push\ndocker-push: ## Push docker image with the manager.\n        $(CONTAINER_TOOL) push ${IMG}\n```\n作用：推送Docker镜像\n解释：使用CONTAINER_TOOL(默认为docker)推送构建好的Docker镜像到指定的仓库\n\n#### 2.3.3.5 docker-buildx（构建并推送多平台支持的Docker镜像）\n```shell\n# PLATFORMS defines the target platforms for the manager image be built to provide support to multiple\n# architectures. (i.e. make docker-buildx IMG=myregistry/mypoperator:0.0.1). To use this option you need to:\n# - be able to use docker buildx. More info: https://docs.docker.com/build/buildx/\n# - have enabled BuildKit. More info: https://docs.docker.com/develop/develop-images/build_enhancements/\n# - be able to push the image to your registry (i.e. if you do not set a valid value via IMG=<myregistry/image:<tag>> then the export will fail)\n# To adequately provide solutions that are compatible with multiple platforms, you should consider using this option.\nPLATFORMS ?= linux/arm64,linux/amd64,linux/s390x,linux/ppc64le\n.PHONY: docker-buildx\ndocker-buildx: ## Build and push docker image for the manager for cross-platform support\n        # copy existing Dockerfile and insert --platform=${BUILDPLATFORM} into Dockerfile.cross, and preserve the original Dockerfile\n        sed -e '1 s/\\(^FROM\\)/FROM --platform=\\$$\\{BUILDPLATFORM\\}/; t' -e ' 1,// s//FROM --platform=\\$$\\{BUILDPLATFORM\\}/' Dockerfile > Dockerfile.cross\n        - $(CONTAINER_TOOL) buildx create --name myapp-operator-builder\n        $(CONTAINER_TOOL) buildx use myapp-operator-builder\n        - $(CONTAINER_TOOL) buildx build --push --platform=$(PLATFORMS) --tag ${IMG} -f Dockerfile.cross .\n        - $(CONTAINER_TOOL) buildx rm myapp-operator-builder\n        rm Dockerfile.cross\n```\n\n作用：构建并推送多平台支持的Docker镜像\n解释：\n- 使用sed命令复制现有的Dockerfile并插入--platform=${BUILDPLATFORM}选项,生成Dockerfile.cross\n- 创建一个名为myapp-builder的构建器\n- 使用buildx构建并推送多平台镜像\n- 删除构建器和临时生成的Dockerfile.cross\n\n#### 2.3.3.6 docker-installer（生成包含CRDs和部署的合并YAML文件）\n```shell\n.PHONY: build-installer\nbuild-installer: manifests generate kustomize ## Generate a consolidated YAML with CRDs and deployment.\n        mkdir -p dist\n        cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}\n        $(KUSTOMIZE) build config/default > dist/install.yaml\n```\n作用：生成包含CRDs和部署的合并YAML文件\n解释：\n- 创建dist目录\n- 使用kustomize工具设置controller镜像为IMG\n- 使用kustomize构建config/default目录中的资源，并保存到dist/install.yaml\n\n### 2.3.4 部署目标\n#### 2.3.4.1 install（安装CRDs到Kubernetes集群）\n```shell\n.PHONY: install\ninstall: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.\n        $(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -\n```\n作用：安装CRDs到Kubernetes集群\n解释：\n- 先运行manifests和kustomize目标\n- 使用kustomize 构建config/crd目录中的资源，并使用kubectl将其应用到集群。\n\n#### 2.3.4.2 uninstall（从Kubernetes集群卸载CRDs）\n```shell\n.PHONY: uninstall\nuninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n        $(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n```\n\n作用：从Kubernetes集群卸载CRDs\n解释：\n- 先运行manifests和kustomize目标\n- 使用kustomize 构建config/crd目录中的资源，并使用kubectl将其从集群中删除，可以通过ignore-not-found=true忽略资源未找到的错误\n\n#### 2.3.4.3 deploy（部署controller到Kubernetes集群）\n```shell\n.PHONY: deploy\ndeploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.\n        cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}\n        $(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -\n```\n作用：部署controller到Kubernetes集群\n解释：\n- 先运行manifests和kustomize目标\n- 使用kustomize设置controller镜像为IMG\n- 使用kustomize构建config/default目录中的资源，并使用kubectl将其应用到集群\n\n#### 2.3.4.4 undeploy（从Kubernetes集群中卸载Controller）\n```shell\n.PHONY: undeploy\nundeploy: kustomize ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n        $(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n```\n作用：从Kubernetes集群中卸载Controller\n解释：使用kustomize 构建config/default目录中的资源，并使用kubectl将其从集群中删除。可以通过ignore-not-found=true忽略资源未找到的错误\n\n### 2.3.5 依赖目标\n#### 2.3.5.1 ignore-not-found（定义ignore-not-found变量，默认值为false）\n```shell\nifndef ignore-not-found\n  ignore-not-found = false\nendif\n```\n作用：定义ignore-not-found变量，默认值为false\n解释：如果命令行中未指定ignore-not-found，则默认值为false\n\n#### 2.3.5.2 LOCALBIN（定义本地二进制文件的安装目录）\n```shell\n## Location to install dependencies to\nLOCALBIN ?= $(shell pwd)/bin\n$(LOCALBIN):\n        mkdir -p $(LOCALBIN)\n```\n作用：定义本地二进制文件的安装目录，默认为当前目录下的bin目录\n解释：如果LOCALBIN未设置，则默认为$(shell pwd)/bin。如果LOCALBIN目录不存在，则创建它。\n\n#### 2.3.5.3 工具二进制文件（定义各种工具的路径和默认值）\n```shell\n## Tool Binaries\nKUBECTL ?= kubectl\nKUSTOMIZE ?= $(LOCALBIN)/kustomize\nCONTROLLER_GEN ?= $(LOCALBIN)/controller-gen\nENVTEST ?= $(LOCALBIN)/setup-envtest\nGOLANGCI_LINT = $(LOCALBIN)/golangci-lint\n```\n作用：定义各种工具的路径和默认值\n解释：\n- KUBECTL：默认为kubectl\n- KUSTOMIZE：默认为$(LOCALBIN)/kustomize\n- CONTROLLER_GEN：默认为$(LOCALBIN)/controller-gen\n- ENVTEST：默认为$(LOCALBIN)/setup-envtest\n- GOLANGCI_LINT： 默认为$(LOCALBIN)/golangci-lint\n\n#### 2.3.5.4 工具版本（定义各个工具的版本）\n```shell\n## Tool Versions\nKUSTOMIZE_VERSION ?= v5.4.3\nCONTROLLER_TOOLS_VERSION ?= v0.16.1\nENVTEST_VERSION ?= release-0.19\nGOLANGCI_LINT_VERSION ?= v1.59.1\n```\n作用：定义各个工具的版本\n解释：\n- KUSTOMIZE_VERSION：默认为v5.4.3\n- CONTROLLER_TOOLS_VERSION：默认为v0.16.1\n- ENVTEST_VERSION：默认为release-0.19\n- GOLANGCI_LINT_VERSION：默认为v1.59.1\n\n#### 2.3.5.5 kustomize（下载kustomize工具）\n```shell\n.PHONY: kustomize\nkustomize: $(KUSTOMIZE) ## Download kustomize locally if necessary.\n$(KUSTOMIZE): $(LOCALBIN)\n        $(call go-install-tool,$(KUSTOMIZE),sigs.k8s.io/kustomize/kustomize/v5,$(KUSTOMIZE_VERSION))\n```\n作用：下载kustomize工具(如有必要)\n解释：如果kustomize文件不存在，则调用 go-install-tool函数下载kustomize工具\n\n#### 2.3.5.6 controller-gen（下载controller-gen工具）\n```shell\n.PHONY: controller-gen\ncontroller-gen: $(CONTROLLER_GEN) ## Download controller-gen locally if necessary.\n$(CONTROLLER_GEN): $(LOCALBIN)\n        $(call go-install-tool,$(CONTROLLER_GEN),sigs.k8s.io/controller-tools/cmd/controller-gen,$(CONTROLLER_TOOLS_VERSION))\n```\n作用：下载controller-gen工具(如有必要)\n解释：如果$(CONTROLLER_GEN)文件不存在，则调用 go-install-tool函数下载controller-gen工具\n\n#### 2.3.5.7 envtest（下载setup-envtest工具）\n```shell\n.PHONY: envtest\nenvtest: $(ENVTEST) ## Download setup-envtest locally if necessary.\n$(ENVTEST): $(LOCALBIN)\n        $(call go-install-tool,$(ENVTEST),sigs.k8s.io/controller-runtime/tools/setup-envtest,$(ENVTEST_VERSION))\n```\n作用：下载setup-envtest工具(如有必要)\n解释：如果$(ENVTEST)文件不存在，则调用go-install-tool函数下载setup-envtes工具\n\n#### 2.3.5.8 golangci-lint（下载golangci-lint工具）\n```shell\n.PHONY: golangci-lint\ngolangci-lint: $(GOLANGCI_LINT) ## Download golangci-lint locally if necessary.\n$(GOLANGCI_LINT): $(LOCALBIN)\n        $(call go-install-tool,$(GOLANGCI_LINT),github.com/golangci/golangci-lint/cmd/golangci-lint,$(GOLANGCI_LINT_VERSION))\n```\n作用：下载golangci-lint工具(如有必要)\n解释：如果$(GOLANGCI_LINT)文件不存在，则调用go-install-tool函数下载golangci-lint工具。\n\n## 2.4 自定义函数\n### 2.4.1 go-install-tool（下载并安装指定的Go包）\n```shell\n# go-install-tool will 'go install' any package with custom target and name of binary, if it doesn't exist\n# $1 - target path with name of binary\n# $2 - package url which can be installed\n# $3 - specific version of package\ndefine go-install-tool\n@[ -f \"$(1)-$(3)\" ] || { \\\nset -e; \\\npackage=$(2)@$(3) ;\\\necho \"Downloading $${package}\" ;\\\nrm -f $(1) || true ;\\\nGOBIN=$(LOCALBIN) go install $${package} ;\\\nmv $(1) $(1)-$(3) ;\\\n} ;\\\nln -sf $(1)-$(3) $(1)\nendef\n```\n作用：下载并安装指定的Go包\n解释：\n- $1：目标路径和二进制文件名\n- $2：包的URL\n- $3:包的具体版本\n- 如果目标文件$(1)~$(3)不存在，则执行以下步骤\n- 设置set -e，确保任何命令执行失败立即退出\n- 设置package变量为包的URL和版本\n- 输出下载信息\n- 删除旧的二进制文件(如果存在)\n- 使用go install命令安装包，并将二进制文件保存在LOCALBIN目录\n- 将生成的二进制文件重命令为$(1)-$(3)\n- 最后创建一个符号链接，指向$(1)-$(3)\n\n\n\n\n\n\n\n\n"},{"title":"operator开发脚手架","url":"/2025/07/kubernetes/operator-kai-fa/operator-kai-fa-jiao-shou-jia/","content":"# 1、脚手架工具\n\nOperator的实现方式主要包括OperatorSDK和KubeBuilder，目前KubeBuilder在阿里使用的比较多。\n\n[KubeBuilder](https://github.com/kubernetes-sigs/kubebuilder?spm=ata.21736010.0.0.5e4b6ec8eEasr9)\n\n[OperatorSDK](https://github.com/operator-framework/operator-sdk?spm=ata.21736010.0.0.5e4b6ec8eEasr9)\n\n我们这里主要是用KubeBuilder来进行，其中OperatorSDK其实也是应用了KubeBuilder。\n\n# 2、创建Operator工程\n创建脚手架工程：\n```shell\nmkdir myapp-operator\ncd myapp-operator/\ngo env -w GO111MODULE=on\ngo env -w GOPROXY=https://goproxy.cn,direct\nkubebuilder init --domain kubenode.kingtest.com\n```\n\n创建脚手架结果：\n```shell\nkubebuilder init --domain kubenode.kingtest.com\nINFO Writing kustomize manifests for you to edit... \nINFO Writing scaffold for you to edit...          \nINFO Get controller runtime:\n$ go get sigs.k8s.io/controller-runtime@v0.19.0 \ngo: downloading sigs.k8s.io/controller-runtime v0.19.0\ngo: downloading k8s.io/apimachinery v0.31.0\ngo: downloading k8s.io/api v0.31.0\ngo: downloading k8s.io/client-go v0.31.0\ngo: downloading k8s.io/utils v0.0.0-20240711033017-18e509b52bc8\ngo: downloading github.com/go-logr/logr v1.4.2\ngo: downloading k8s.io/klog/v2 v2.130.1\ngo: downloading golang.org/x/exp v0.0.0-20230515195305-f3d0a9c9a5cc\ngo: downloading github.com/evanphx/json-patch/v5 v5.9.0\ngo: downloading github.com/prometheus/client_golang v1.19.1\ngo: downloading gomodules.xyz/jsonpatch/v2 v2.4.0\ngo: downloading github.com/gogo/protobuf v1.3.2\ngo: downloading github.com/google/gofuzz v1.2.0\ngo: downloading sigs.k8s.io/structured-merge-diff/v4 v4.4.1\ngo: downloading k8s.io/apiextensions-apiserver v0.31.0\ngo: downloading sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd\ngo: downloading k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340\ngo: downloading github.com/google/uuid v1.6.0\ngo: downloading github.com/prometheus/client_model v0.6.1\ngo: downloading github.com/prometheus/common v0.55.0\ngo: downloading github.com/fsnotify/fsnotify v1.7.0\ngo: downloading golang.org/x/net v0.26.0\ngo: downloading gopkg.in/inf.v0 v0.9.1\ngo: downloading github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da\ngo: downloading github.com/imdario/mergo v0.3.6\ngo: downloading github.com/spf13/pflag v1.0.5\ngo: downloading golang.org/x/term v0.21.0\ngo: downloading github.com/golang/protobuf v1.5.4\ngo: downloading github.com/google/gnostic-models v0.6.8\ngo: downloading golang.org/x/time v0.3.0\ngo: downloading sigs.k8s.io/yaml v1.4.0\ngo: downloading github.com/beorn7/perks v1.0.1\ngo: downloading github.com/cespare/xxhash/v2 v2.3.0\ngo: downloading github.com/prometheus/procfs v0.15.1\ngo: downloading golang.org/x/sys v0.21.0\ngo: downloading google.golang.org/protobuf v1.34.2\ngo: downloading golang.org/x/oauth2 v0.21.0\ngo: downloading github.com/json-iterator/go v1.1.12\ngo: downloading gopkg.in/yaml.v2 v2.4.0\ngo: downloading github.com/fxamacker/cbor/v2 v2.7.0\ngo: downloading github.com/google/go-cmp v0.6.0\ngo: downloading gopkg.in/yaml.v3 v3.0.1\ngo: downloading github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc\ngo: downloading github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822\ngo: downloading github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd\ngo: downloading github.com/modern-go/reflect2 v1.0.2\ngo: downloading github.com/go-openapi/jsonreference v0.20.2\ngo: downloading github.com/go-openapi/swag v0.22.4\ngo: downloading golang.org/x/text v0.16.0\ngo: downloading github.com/go-openapi/jsonpointer v0.19.6\ngo: downloading github.com/emicklei/go-restful/v3 v3.11.0\ngo: downloading github.com/mailru/easyjson v0.7.7\ngo: downloading github.com/josharian/intern v1.0.0\ngo: downloading github.com/x448/float16 v0.8.4\ngo: downloading github.com/pkg/errors v0.9.1\nINFO Update dependencies:\n$ go mod tidy           \ngo: downloading github.com/onsi/ginkgo/v2 v2.19.0\ngo: downloading github.com/stretchr/testify v1.9.0\ngo: downloading github.com/onsi/gomega v1.33.1\ngo: downloading github.com/go-logr/zapr v1.3.0\ngo: downloading go.uber.org/zap v1.26.0\ngo: downloading k8s.io/apiserver v0.31.0\ngo: downloading go.uber.org/goleak v1.3.0\ngo: downloading github.com/evanphx/json-patch v0.5.2\ngo: downloading gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c\ngo: downloading gopkg.in/evanphx/json-patch.v4 v4.12.0\ngo: downloading github.com/kr/pretty v0.3.1\ngo: downloading go.uber.org/multierr v1.11.0\ngo: downloading github.com/go-task/slim-sprig/v3 v3.0.0\ngo: downloading golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d\ngo: downloading github.com/google/pprof v0.0.0-20240525223248-4bfdf5a9a2af\ngo: downloading github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2\ngo: downloading github.com/rogpeppe/go-internal v1.12.0\ngo: downloading github.com/kr/text v0.2.0\ngo: downloading k8s.io/component-base v0.31.0\ngo: downloading go.opentelemetry.io/otel v1.28.0\ngo: downloading go.opentelemetry.io/otel/trace v1.28.0\ngo: downloading google.golang.org/grpc v1.65.0\ngo: downloading sigs.k8s.io/apiserver-network-proxy/konnectivity-client v0.30.3\ngo: downloading golang.org/x/sync v0.7.0\ngo: downloading github.com/google/cel-go v0.20.1\ngo: downloading github.com/blang/semver/v4 v4.0.0\ngo: downloading go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.53.0\ngo: downloading go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.27.0\ngo: downloading go.opentelemetry.io/otel/sdk v1.28.0\ngo: downloading google.golang.org/genproto/googleapis/api v0.0.0-20240528184218-531527333157\ngo: downloading github.com/felixge/httpsnoop v1.0.4\ngo: downloading go.opentelemetry.io/otel/metric v1.28.0\ngo: downloading github.com/asaskevich/govalidator v0.0.0-20190424111038-f61b66f89f4a\ngo: downloading github.com/go-logr/stdr v1.2.2\ngo: downloading google.golang.org/genproto/googleapis/rpc v0.0.0-20240701130421-f6361c86f094\ngo: downloading github.com/antlr4-go/antlr/v4 v4.13.0\ngo: downloading google.golang.org/genproto v0.0.0-20230822172742-b8732ec3820d\ngo: downloading github.com/spf13/cobra v1.8.1\ngo: downloading github.com/stoewer/go-strcase v1.2.0\ngo: downloading github.com/inconshreveable/mousetrap v1.1.0\ngo: downloading go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.28.0\ngo: downloading github.com/cenkalti/backoff/v4 v4.3.0\ngo: downloading go.opentelemetry.io/proto/otlp v1.3.1\ngo: downloading github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0\ngo: downloading github.com/grpc-ecosystem/grpc-gateway v1.16.0\nNext: define a resource with:\n$ kubebuilder create api\n```\n\n本步骤创建了 Go module 工程的模板文件，引入了必要的依赖。\n这里先设置了go的代理，不然会无法下载依赖。\n第一次新建工程时，会下载一些依赖(go: downloading 。。。)\n查看此时的项目结构:\n```shell\n.\n├── cmd\n│   └── main.go\n├── config\n│   ├── default\n│   │   ├── kustomization.yaml\n│   │   ├── manager_metrics_patch.yaml\n│   │   └── metrics_service.yaml\n│   ├── manager\n│   │   ├── kustomization.yaml\n│   │   └── manager.yaml\n│   ├── network-policy\n│   │   ├── allow-metrics-traffic.yaml\n│   │   └── kustomization.yaml\n│   ├── prometheus\n│   │   ├── kustomization.yaml\n│   │   └── monitor.yaml\n│   └── rbac\n│       ├── kustomization.yaml\n│       ├── leader_election_role_binding.yaml\n│       ├── leader_election_role.yaml\n│       ├── metrics_auth_role_binding.yaml\n│       ├── metrics_auth_role.yaml\n│       ├── metrics_reader_role.yaml\n│       ├── role_binding.yaml\n│       ├── role.yaml\n│       └── service_account.yaml\n├── Dockerfile\n├── go.mod\n├── go.sum\n├── hack\n│   └── boilerplate.go.txt\n├── Makefile\n├── PROJECT\n├── README.md\n└── test\n    ├── e2e\n    │   ├── e2e_suite_test.go\n    │   └── e2e_test.go\n    └── utils\n        └── utils.go\n\n12 directories, 29 files\n```\n\n创建API，生成CRD和Controller:\n```shell\nkubebuilder create api --group apps --version v1 --kind Myapp\n```\n\n参数含义：\n- group参数表示组的概念\n- version定义版本\n- kind定义自定义资源类型\n- 以上参数组成 自定义yaml 的 apiVersion和kind\n- 执行kubebuilder create api时直接带上–namespaced=false可以将该对象设计为集群级别（类似node、pv）\n  \n创建API结果：\n```shell\nkubebuilder create api --group apps --version v1 --kind Myapp\nINFO Create Resource [y/n]                        \ny\nINFO Create Controller [y/n]                      \ny\nINFO Writing kustomize manifests for you to edit... \nINFO Writing scaffold for you to edit...          \nINFO api/v1/myapp_types.go                        \nINFO api/v1/groupversion_info.go                  \nINFO internal/controller/suite_test.go            \nINFO internal/controller/myapp_controller.go      \nINFO internal/controller/myapp_controller_test.go \nINFO Update dependencies:\n$ go mod tidy           \nINFO Running make:\n$ make generate                \nmkdir -p /home/king/workspace/king-devops/operator/myapp-operator/bin\nDownloading sigs.k8s.io/controller-tools/cmd/controller-gen@v0.16.1\ngo: downloading sigs.k8s.io/controller-tools v0.16.1\ngo: downloading github.com/fatih/color v1.17.0\ngo: downloading golang.org/x/tools v0.24.0\ngo: downloading github.com/gobuffalo/flect v1.0.2\ngo: downloading golang.org/x/net v0.28.0\ngo: downloading github.com/mattn/go-isatty v0.0.20\ngo: downloading github.com/mattn/go-colorable v0.1.13\ngo: downloading golang.org/x/sys v0.23.0\ngo: downloading golang.org/x/sync v0.8.0\ngo: downloading golang.org/x/mod v0.20.0\ngo: downloading golang.org/x/text v0.17.0\n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"\nNext: implement your new API and generate the manifests (e.g. CRDs,CRs) with:\n$ make manifests\n```\n\n此时的目录结构：\n```shell\n.\n├── api\n│   └── v1\n│       ├── groupversion_info.go\n│       ├── myapp_types.go\n│       └── zz_generated.deepcopy.go\n├── bin\n│   ├── controller-gen -> /home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen-v0.16.1\n│   └── controller-gen-v0.16.1\n├── cmd\n│   └── main.go\n├── config\n│   ├── crd\n│   │   ├── kustomization.yaml\n│   │   └── kustomizeconfig.yaml\n│   ├── default\n│   │   ├── kustomization.yaml\n│   │   ├── manager_metrics_patch.yaml\n│   │   └── metrics_service.yaml\n│   ├── manager\n│   │   ├── kustomization.yaml\n│   │   └── manager.yaml\n│   ├── network-policy\n│   │   ├── allow-metrics-traffic.yaml\n│   │   └── kustomization.yaml\n│   ├── prometheus\n│   │   ├── kustomization.yaml\n│   │   └── monitor.yaml\n│   ├── rbac\n│   │   ├── kustomization.yaml\n│   │   ├── leader_election_role_binding.yaml\n│   │   ├── leader_election_role.yaml\n│   │   ├── metrics_auth_role_binding.yaml\n│   │   ├── metrics_auth_role.yaml\n│   │   ├── metrics_reader_role.yaml\n│   │   ├── myapp_editor_role.yaml\n│   │   ├── myapp_viewer_role.yaml\n│   │   ├── role_binding.yaml\n│   │   ├── role.yaml\n│   │   └── service_account.yaml\n│   └── samples\n│       ├── apps_v1_myapp.yaml\n│       └── kustomization.yaml\n├── Dockerfile\n├── go.mod\n├── go.sum\n├── hack\n│   └── boilerplate.go.txt\n├── internal\n│   └── controller\n│       ├── myapp_controller.go\n│       ├── myapp_controller_test.go\n│       └── suite_test.go\n├── Makefile\n├── PROJECT\n├── README.md\n└── test\n    ├── e2e\n    │   ├── e2e_suite_test.go\n    │   └── e2e_test.go\n    └── utils\n        └── utils.go\n\n19 directories, 43 files\n```\n\n如果需要在Nginx CRUD 时进行合法性检查， 可以生成webhook:\n```shell\nkubebuilder create webhook --group apps --version v1 --kind Myapp --conversion --defaulting --programmatic-validation\n```\n\n参数含义：\n- group参数表示组的概念\n- version定义版本\n- kind定义自定义资源类型\n- 以上参数组成 自定义yaml 的 apiVersion和kind\n- defaulting：默认值设置（Defaulting） 的目的是在CRD对象创建或更新时，如果某些字段没有被显式设置值，自动为其填充一个合理的默认值。这对于确保资源实例有一套统一的、预先定义的配置非常有用，可以减少用户的配置负担，并保证资源的一致性。例如，如果你定义了一个监控应用的CRD，可能希望默认开启日志记录功能，除非用户明确关闭它。\n- programmatic-validation：程序化验证（Programmatic Validation） 则是在CRD对象创建或更新前，对提交的数据进行逻辑验证的机制。与CRD schema提供的静态验证相比，程序化验证可以实现更加复杂的业务逻辑验证。这意味着你可以在webhook服务中编写代码来检查CRD实例的数据是否满足特定的业务规则或约束条件。例如，你可以验证一个资源请求的内存配额是否超过了集群的最大允许值，或者确保引用的其他资源确实存在。\n- conversion：自动数据迁移：conversion webhook可以在后台自动将CRD的一个版本转换为另一个版本，无需人工干预，从而简化了版本升级过程。\n    - 兼容性保障：即使API发生变化，也能确保旧客户端和新客户端都能够正常工作，提高了系统的向后和向前兼容性。\n    - 复杂逻辑处理：对于简单的字段添加或删除，Kubernetes的自动转换器可能足够用。但涉及到复杂的数据结构调整或逻辑变换时，就需要自定义conversion webhook来精确控制转换过程。\n\n执行结果：\n```shell\nkubebuilder create webhook --group apps --version v1 --kind Myapp --conversion --defaulting --programmatic-validation\nINFO Writing kustomize manifests for you to edit... \nINFO Writing scaffold for you to edit...          \nINFO api/v1/myapp_webhook.go                      \nINFO api/v1/myapp_webhook_test.go                 \nINFO Webhook server has been set up for you.\nYou need to implement the conversion.Hub and conversion.Convertible interfaces for your CRD types. \nINFO api/v1/webhook_suite_test.go                 \nINFO Update dependencies:\n$ go mod tidy           \nINFO Running make:\n$ make generate                \n/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"\nNext: implement your new Webhook and generate the manifests with:\n$ make manifests\n```\n\n此时目录结构：\n```shell\n.\n├── api\n│   └── v1\n│       ├── groupversion_info.go\n│       ├── myapp_types.go\n│       ├── myapp_webhook.go\n│       ├── myapp_webhook_test.go\n│       ├── webhook_suite_test.go\n│       └── zz_generated.deepcopy.go\n├── bin\n│   ├── controller-gen -> /home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen-v0.16.1\n│   └── controller-gen-v0.16.1\n├── cmd\n│   └── main.go\n├── config\n│   ├── certmanager\n│   │   ├── certificate.yaml\n│   │   ├── kustomization.yaml\n│   │   └── kustomizeconfig.yaml\n│   ├── crd\n│   │   ├── kustomization.yaml\n│   │   ├── kustomizeconfig.yaml\n│   │   └── patches\n│   │       ├── cainjection_in_myapps.yaml\n│   │       └── webhook_in_myapps.yaml\n│   ├── default\n│   │   ├── kustomization.yaml\n│   │   ├── manager_metrics_patch.yaml\n│   │   ├── manager_webhook_patch.yaml\n│   │   ├── metrics_service.yaml\n│   │   └── webhookcainjection_patch.yaml\n│   ├── manager\n│   │   ├── kustomization.yaml\n│   │   └── manager.yaml\n│   ├── network-policy\n│   │   ├── allow-metrics-traffic.yaml\n│   │   ├── allow-webhook-traffic.yaml\n│   │   └── kustomization.yaml\n│   ├── prometheus\n│   │   ├── kustomization.yaml\n│   │   └── monitor.yaml\n│   ├── rbac\n│   │   ├── kustomization.yaml\n│   │   ├── leader_election_role_binding.yaml\n│   │   ├── leader_election_role.yaml\n│   │   ├── metrics_auth_role_binding.yaml\n│   │   ├── metrics_auth_role.yaml\n│   │   ├── metrics_reader_role.yaml\n│   │   ├── myapp_editor_role.yaml\n│   │   ├── myapp_viewer_role.yaml\n│   │   ├── role_binding.yaml\n│   │   ├── role.yaml\n│   │   └── service_account.yaml\n│   ├── samples\n│   │   ├── apps_v1_myapp.yaml\n│   │   └── kustomization.yaml\n│   └── webhook\n│       ├── kustomization.yaml\n│       ├── kustomizeconfig.yaml\n│       └── service.yaml\n├── Dockerfile\n├── go.mod\n├── go.sum\n├── hack\n│   └── boilerplate.go.txt\n├── internal\n│   └── controller\n│       ├── myapp_controller.go\n│       ├── myapp_controller_test.go\n│       └── suite_test.go\n├── Makefile\n├── PROJECT\n├── README.md\n└── test\n    ├── e2e\n    │   ├── e2e_suite_test.go\n    │   └── e2e_test.go\n    └── utils\n        └── utils.go\n\n22 directories, 57 files\n```\n\n最终的工程结构：\n```shell\n.\n├── api\n│   └── v1\n│       ├── groupversion_info.go\n│       ├── myapp_types.go\n│       ├── myapp_webhook.go\n│       ├── myapp_webhook_test.go\n│       ├── webhook_suite_test.go\n│       └── zz_generated.deepcopy.go\n├── bin\n│   ├── controller-gen -> /home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen-v0.16.1\n│   └── controller-gen-v0.16.1\n├── cmd\n│   └── main.go\n├── config\n│   ├── certmanager\n│   │   ├── certificate.yaml\n│   │   ├── kustomization.yaml\n│   │   └── kustomizeconfig.yaml\n│   ├── crd\n│   │   ├── kustomization.yaml\n│   │   ├── kustomizeconfig.yaml\n│   │   └── patches\n│   │       ├── cainjection_in_myapps.yaml\n│   │       └── webhook_in_myapps.yaml\n│   ├── default\n│   │   ├── kustomization.yaml\n│   │   ├── manager_metrics_patch.yaml\n│   │   ├── manager_webhook_patch.yaml\n│   │   ├── metrics_service.yaml\n│   │   └── webhookcainjection_patch.yaml\n│   ├── manager\n│   │   ├── kustomization.yaml\n│   │   └── manager.yaml\n│   ├── network-policy\n│   │   ├── allow-metrics-traffic.yaml\n│   │   ├── allow-webhook-traffic.yaml\n│   │   └── kustomization.yaml\n│   ├── prometheus\n│   │   ├── kustomization.yaml\n│   │   └── monitor.yaml\n│   ├── rbac\n│   │   ├── kustomization.yaml\n│   │   ├── leader_election_role_binding.yaml\n│   │   ├── leader_election_role.yaml\n│   │   ├── metrics_auth_role_binding.yaml\n│   │   ├── metrics_auth_role.yaml\n│   │   ├── metrics_reader_role.yaml\n│   │   ├── myapp_editor_role.yaml\n│   │   ├── myapp_viewer_role.yaml\n│   │   ├── role_binding.yaml\n│   │   ├── role.yaml\n│   │   └── service_account.yaml\n│   ├── samples\n│   │   ├── apps_v1_myapp.yaml\n│   │   └── kustomization.yaml\n│   └── webhook\n│       ├── kustomization.yaml\n│       ├── kustomizeconfig.yaml\n│       └── service.yaml\n├── Dockerfile\n├── go.mod\n├── go.sum\n├── hack\n│   └── boilerplate.go.txt\n├── internal\n│   └── controller\n│       ├── myapp_controller.go\n│       ├── myapp_controller_test.go\n│       └── suite_test.go\n├── Makefile\n├── PROJECT\n├── README.md\n└── test\n    ├── e2e\n    │   ├── e2e_suite_test.go\n    │   └── e2e_test.go\n    └── utils\n        └── utils.go\n\n22 directories, 57 files\n```\n\n关键目录及其内容的概述：\n- Dockerfile: 定义了用于构建Operator容器镜像的Docker配置文件。\n- Makefile: 包含了一系列预定义的Make任务，用于构建、测试、运行和部署Operator。\n- PROJECT: 存储项目元数据的文件，Kubebuilder使用它来跟踪项目的状态和配置。\n- README.md: 项目的主要说明文档，通常包含项目简介、安装指南、开发流程等信息。\n- api/v1: 定义了自定义资源定义(CRD)的目录，其中包含了资源的Go类型定义及DeepCopy生成文件。\n    - myapp_types.go: 资源类型的定义(myapp_types.go)\n    - myapp_webhook.go: webhook逻辑\n    - zz_generated.deepcopy.go: 深拷贝生成代码\n    - groupversion_info.go: 定义了API组和版本信息。\n- bin: 存放项目依赖的可执行文件，如controller-gen，用于CRD代码生成。\n- cmd/main.go: Operator的入口点，负责初始化和运行控制器。\n- config: 配置目录，包含用于Kubernetes资源部署的各种Kustomize配置。\n    - crd/: 用于CRD资源的Kustomize配置。\n    - default/: 应用默认的RBAC、服务等配置。\n    - manager/: Operator Manager的部署配置。\n    - network-policy/, prometheus/, rbac/, samples/: 分别包含网络策略、Prometheus监控配置、RBAC规则和示例资源的配置。\n- go.mod, go.sum: Go模块管理文件，记录项目依赖。\n- hack/boilerplate.go.txt: 用于代码生成的模板文件，保持版权头部一致性。\n- internal/controller: Operator控制器的核心逻辑所在，包含监控资源(Myapp)的控制器实现。\n- test: 测试相关目录。\n    - e2e/: 端到端测试代码，验证整个Operator在Kubernetes集群上的行为。\n    - utils/: 测试辅助工具和函数。\n\n# 3、编写代码\n\n下面主要以Deployment为例，核心逻辑是把自定义CR（Myapp）当做终态，把Deployment当做运行态，通过比对属性的不一致，编写相关的Reconcile逻辑。\n\n一张图解释各种资源和 Controller 的关系：\n![img_4.png](img_4.png)\n\n\n## 3.1 定义 CRD\n在api/v1/myapp_type.go中定义 Spec 和 Status\n```shell\n// MyappSpec defines the desired state of Myapp\ntype MyappSpec struct {\n        // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster\n        // Important: Run \"make\" to regenerate code after modifying this file\n\n        // Foo is an example field of Myapp. Edit myapp_types.go to remove/update\n        Foo                   string `json:\"foo,omitempty\"`\n        appsv1.DeploymentSpec `json:\",inline\"`\n}\n\n// MyappStatus defines the observed state of Myapp\ntype MyappStatus struct {\n        // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster\n        // Important: Run \"make\" to regenerate code after modifying this file\n        appsv1.DeploymentSpec `json:\",inline\"`\n}\n\n// +kubebuilder:object:root=true\n// +kubebuilder:subresource:status\n\n// Myapp is the Schema for the myapps API\ntype Myapp struct {\n        metav1.TypeMeta   `json:\",inline\"`\n        metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n        Spec   MyappSpec   `json:\"spec,omitempty\"`\n        Status MyappStatus `json:\"status,omitempty\"`\n}\n```\n\n注意`+kubebuilder`并非普通注释，不能随意删除。\n\n## 3.2 编写Reconcile逻辑\n在internal/controller/myapp_controller.go中实现 Reconcile 逻辑\n```shell\nfunc (r *MyappReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n        logger := log.FromContext(ctx)\n\n        // TODO(user): your logic here\n        logger.Info(\"start Reconcile\" + req.Name)\n\n        return ctrl.Result{}, nil\n}\n```\n## 3.3 修改Webhook\n在api/v1/myapp_webhook.go中根据需要进行修改\n```shell\nfunc (r *Myapp) ValidateCreate() error \nfunc (r *Myapp) ValidateUpdate(old runtime.Object) error\nfunc (r *Myapp) Default()\n```\n\n## 3.4 修改main入口\n以前的老版本kubebuilder生成的项目，需要在cmd/main.go添加监听的namespace\n但是新版本没有这个信息，以下是示例：\n```shell\nmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{\n                Scheme:                 scheme,\n                Metrics:                metricsServerOptions,\n                WebhookServer:          webhookServer,\n                HealthProbeBindAddress: probeAddr,\n                LeaderElection:         enableLeaderElection,\n                LeaderElectionID:       \"6a511ed4.kubenode.kingtest.com\",\n                // LeaderElectionReleaseOnCancel defines if the leader should step down voluntarily\n                // when the Manager ends. This requires the binary to immediately end when the\n                // Manager is stopped, otherwise, this setting is unsafe. Setting this significantly\n                // speeds up voluntary leader transitions as the new leader don't have to wait\n                // LeaseDuration time first.\n                //\n                // In the default scaffold provided, the program ends immediately after\n                // the manager stops, so would be fine to enable this option. However,\n                // if you are doing or is intended to do any operation such as perform cleanups\n                // after the manager stops then its usage might be unsafe.\n                // LeaderElectionReleaseOnCancel: true,\n```\n\n如果想监听固定的namespace信息，可以在Reconcile内实现。\n\n\n\n"},{"title":"operator简介","url":"/2025/07/kubernetes/operator-kai-fa/operator-jian-jie/","content":"# 1 kubernetes背景\n## 1.1 Controller模式\n\ncontroller通过根据被控制对象的属性和字段来实现编排，对于每一个build-in的资源类型，都有对应的controller。\n\n比如以下是一个简单的Deployment的yaml示例：\n\n其对应的Deployment Controller编排动作是确保app=test的Pod数量为2，Pod的属性和字段由spec.template定义\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test\nspec:\n  selector:\n    matchLabels:\n      app: test\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n```\n\n所有控制器的主要操作都是一个调谐循环（Reconcile loop）。主要有三步：\n- 观察期望的状态。\n- 观察所管理资源的当前状态。\n- 采取行动，使托管的资源处在期望的状态。\n```shell\nfor {\n    actualState := GetResourceActualState(rsvc)\n    expectState := GetResourceExpectState(rsvc)\n    if actualState == expectState {\n        // do nothing\n    } else {\n        Reconcile(rsvc)\n    }\n}\n```\n\n## 1.2 声明式API\n声明式API：告诉K8S你要什么，而不是告诉它你怎么做。\n\n声明式API的操作体现在kubectl apply命令上，在对象创建和后续修改更新都使用apply命令，告诉k8s对象的终态即可，底层的实现是一个对原有API对象的PATCH操作实现的，可以一次性处理多个写操作，相对于命令时一个个的命令大大提高了操作效率。\n\n比如上面的deployment.yaml文件，在提交后会通过Group/Version/Resource的分级来找到deployment在GO语言中的结构体定义，从而将YAML描述转换成一个序列化的deployment对象，并通过etcd的API将其保存起来。\n\n## 1.3 “原生的资源-Controller”的问题\n- 原生资源不够用\n    - K8S内部的基础资源，如Pod、Statefulset、Service，能够覆盖大部分应用的工作模式。\n    - 但对以下情况不友好：\n        - 有些应用组件无法找到合适的基础资源作为模板，比如GPU资源、训练数据集、训练任务等；\n        - 有些应用组件需要多个基础资源共同输出一项能力，会变得很复杂；\n- 细粒度操作繁琐\n    - 需要对云原生K8S的细粒度资源的种类比较熟悉；\n    - 应用在提供服务时，存在通过Label等标签信息对应的逻辑。\n- 缺乏定制化能力，如：\n    - 一些简单的中间件集成\n    - 对不同资源的操作时序的流水线管理。\n\n## 1.4 拓展自定义资源与控制器\n主要做两件事：\n- 编写自定义资源，并将其部署到K8S集群中: 通过编写符合K8S资源和结构属性的文件，使得K8S能够校验该资源并进行持久化。\n- 编写其对应的控制器，并将其部署到K8S集群中: 通过实现调谐逻辑，来完成资源编排的实际需求。\n\n# 2、Operator 介绍\n## 2.1 Operator\n借助 Kubernetes 的控制器模式，编写自定义的编排规则，完成对自定义资源的操作，比如增删改查等。\n\n一般来说，Operator=CRD(自定义资源)+Controller(自定义控制器)+Webhook(Admission根据实际情况选择是否添加)\n\n## 2.2 自定义资源 CRD\nCRD（Custom Resource Definition）是用户的自定义资源类型，可以基于Kubernetes的API Server进行管理。其实例为CR（Custom Resource）。\n```shell\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: test ## CRD名称\nspec:\n  conversion:\n    strategy: None\n  group: tq   # REST API: /apis/<group>/<version>\n  names:\n    kind: App\n    listKind: AppList\n    plural: apps\n    singular: app\n  scope: Namespaced  # 资源是否区分namespace\n  versions: [] # 资源的版本信息，例如：v1beta1、v1等\n```\n\n## 2.3 自定义控制器 Controller\nKubernetes控制器会监视资源的创建/更新/删除事件，并触发 Reconcile 函数作为响应。Reconcile 是一个使用 object（Resource 的实例）的命名空间和实例名来调用的函数，使object的实际状态与object的Spec中定义的状态保持一致。 调用完成后，Reconcile 会将object的状态更新为当前实际状态。\n\n在实际环境中，Controller会被打包成镜像，以Deployment拉起的Pod的形式应用在集群中，并用service中的ClusterIP方式进行服务暴露发现。\n![img.png](img.png)\n\n## 2.4 名词解释\n- CRD（Custom Resource Definition）: CRD允许用户在Kubernetes API中定义新的资源类型。这意味着你可以创建符合自己应用或服务特性的资源，这些资源能够被Kubernetes以类似于内置资源（如Pods、Services）的方式来管理和操作。CRD实质上是扩展了Kubernetes API的功能，允许你定义资源的名称、字段结构、验证规则等元数据。通过YAML或JSON文件定义CRD后，Kubernetes会将其注册到API服务器，使得用户可以通过Kubernetes API来创建、读取、更新和删除这些自定义资源。\n- CR（Custom Resource）: CR是基于CRD定义的具体实例。一旦定义了CRD，用户就可以创建该类型的资源实例，这些实例就是CR。每个CR代表了特定的配置或状态，比如一个特定数据库实例的配置、一个复杂应用的部署配置等。CR的工作原理类似于Kubernetes中的其他对象（如Deployment、Service），但它们是用户自定义的，能够更好地适应特定应用的需求。开发者或运维人员通过创建CR来声明他们希望在集群中部署或管理的资源状态，而与之关联的Operator会根据这些声明来创建、维护和更新实际的Kubernetes对象。\n- GVK（Group, Version, Kind）: GVK代表了资源的组（Group）、版本（Version）和种类（Kind）。它是从资源的API层面描述资源的一种方式。\n    - Group：指资源所属的API组，用于区分不同功能领域的API。例如，apps/v1中的apps就是组名，它通常对应着Kubernetes的一个特定功能领域，如应用程序管理。\n    - Version：资源的API版本，表明资源定义遵循的特定API版本规范。随着Kubernetes的发展，资源的API可能会有变化，版本号帮助区分这些不同的规范。\n    - Kind：资源的类型，如Pod、Service、Deployment等，它定义了一类资源的基本结构和用途。\n- GVR (Group, Version, Resource): GVR与GVK类似，但用Resource替换了Kind，它从资源的访问路径角度描述资源。\n    - Group：指资源所属的API组，用于区分不同功能领域的API。例如，apps/v1中的apps就是组名，它通常对应着Kubernetes的一个特定功能领域，如应用程序管理。\n    - Version：资源的API版本，表明资源定义遵循的特定API版本规范。随着Kubernetes的发展，资源的API可能会有变化，版本号帮助区分这些不同的规范。\n    - Resource: 指的是资源在API路径中的名称，它直接关联到API端点，是用于与API服务器交互时的URL路径组成部分。Resource通常与Kind相似或相同，但在某些情况下可能有细微差别，特别是在使用自定义资源定义（CRDs）时。\n- Scheme: 负责管理和注册API类型（如各种资源对象）以及它们之间的转换关系。简单来说，Scheme是理解和操作Kubernetes对象的基础框架，它确保了对象的序列化、反序列化以及类型转换能够正确进行\n    - 类型注册：Scheme允许你注册自定义或内置的API类型。这意味着你可以告诉Scheme如何识别和处理特定的Go结构体类型，比如Pod、Service、或者自定义资源定义（CRDs）等。\n    - 对象转换：它提供了类型转换功能，允许在不同的API版本之间转换对象。这对于处理API的版本兼容性和升级非常重要。\n    - 默认化：Scheme支持为对象设置默认值。当创建或更新资源时，如果没有明确指定某些字段，Scheme可以根据预设规则自动填充默认值\n    - 序列化与反序列化：Scheme知道如何将Go结构体转换为JSON或YAML格式的数据（序列化），以及如何将这些数据反序列化回Go结构体。这对于与Kubernetes API服务器通信至关重要。\n    - 元数据处理：它还负责处理对象的元数据，如API版本信息和Kind信息，确保这些信息在序列化和反序列化过程中得到妥善处理。\n- Manager：Manager整合了一系列功能，使得开发者能够更容易地创建和管理自定义资源（CRDs）及其对应的控制器逻辑。以下是Manager的一些关键职责和功能\n    - 自定义资源管理：Manager负责监听和管理自定义资源定义（CRDs）的变化，当CR实例被创建、更新或删除时，它能确保相应的控制器逻辑得到执行。\n    - 控制器注册：它提供了一个中心点来注册和管理不同的控制器（Controllers）。控制器是执行具体业务逻辑的组件，负责维护期望状态与实际集群状态的一致性。\n    - 依赖注入：Manager框架通常支持依赖注入，使得开发者可以轻松地复用和共享服务，比如客户端集（ClientSets）用于与Kubernetes API交互、日志记录器、事件记录器等。\n    - 领导者选举：在分布式环境中，Manager可以实现领导者选举逻辑，确保在一个集群中只有一个活跃的实例在执行操作，避免冲突和重复处理。\n    - Webhook注册：它还可能支持注册和管理自定义的Admission Webhooks，允许在资源创建或修改时插入自定义验证或修改逻辑。\n    - 生命周期管理：Manager负责启动、停止控制器，以及处理它们的生命周期事件，确保资源的有效管理和清理。\n- Controller：Kubebuilder为我们生成的脚手架文件，我们只需要实现Reconcile方法即可。\n- Informers：在Kubernetes中，Informers是客户端库（client-go）提供的一个核心组件，用于高效地监听和同步Kubernetes API资源的变化。它是实现控制器模式的关键技术之一，尤其是对于那些需要根据资源状态变化做出反应的组件，如自定义控制器、Operator等\n    - 资源监听：Informers持续监听Kubernetes API服务器中指定资源类型（如Pods、Deployments等）的增删改事件，通过watch API机制实现近乎实时的资源变化感知。\n    - 缓存同步：它在本地维护一个资源对象的缓存副本，这个缓存会随着API服务器的事件更新而自动保持最新状态。这样，控制器或其他组件可以直接查询本地缓存获取资源列表或详情，而不需要频繁地直接查询API服务器，大大提高了效率。\n    - 事件处理：Informers提供回调机制，允许用户注册处理函数（如EventHandler或Informer的事件处理器），当资源发生变化时自动触发这些函数，执行相应的业务逻辑。\n    - 列表和详细信息的统一管理：通过Lister接口，Informers不仅能够提供资源列表，还能高效地获取单个资源的详细信息，进一步简化了资源的管理和查询过程。\n- Index：由于Controller经常要对Cache进行查询，Kubebuilder提供Index utility给Cache加索引提升查询效率。\n- Finalizer：在Kubernetes中，Finalizer 是一种高级特性，用于确保资源在其被删除之前能完成必要的清理工作。它作为一种保障机制，可以让控制器或操作者有机会在对象被永久删除前执行一些清理、备份或其他必要的操作，确保资源的优雅删除和资源使用的完整性。\n    - 阻止删除：删除操作会被暂停，直到所有列出的finalizer都被处理完毕。\n    - 通知处理者：Kubernetes会通知或等待负责该finalizer的控制器（或外部系统）完成相应的清理操作。\n    - 清除Finalizer：一旦处理者完成了其任务，它会通过API更新该对象，从metadata.finalizers字段中移除相应的finalizer项。\n    - 继续删除流程：当所有finalizer都被移除后，Kubernetes才会最终从API服务器中删除该对象。\n    - 配置Finalizer：Finalizer通常是通过自定义控制器（如Operator）动态添加和管理的。控制器可以在创建或更新资源时向对象的metadata.finalizers字段添加自定义的finalizer名称，同样，在完成清理工作后，需要通过API调用来移除这个finalizer，以允许资源被彻底删除。\n- OwnerReference工作原理：在Kubernetes中，OwnerReference 是一种机制，用于建立资源对象之间的所有权关系。这种关系定义了资源的生命周期依赖性，即“拥有者”资源（owner）控制着“被拥有”资源（owned resource）的生命周期。当一个资源作为另一个资源的Owner时，它会对被拥有资源的创建、更新和删除产生影响，确保资源之间的一致性和自动化管理。\n    - 自动清理（Garbage Collection）：当一个资源对象被删除，并且它拥有其他资源时，Kubernetes的垃圾收集机制会自动删除这些被拥有的资源，确保资源依赖关系被正确清理，避免孤儿资源的产生。\n    - 生命周期耦合：被拥有资源的生命周期与拥有者资源紧密相连。如果拥有者资源被修改或删除，Kubernetes会相应地更新或清理被拥有资源。\n    - 防止循环依赖：Kubernetes禁止创建会导致循环OwnerReference关系的资源，以防止资源管理混乱和潜在的死锁情况。\n- OwnerReference结构定义：OwnerReference是以API对象的一个字段形式存在的，通常位于被拥有资源的metadata.ownerReferences字段中。它包含几个关键属性：\n    - APIVersion：拥有者的API版本。\n    - Kind：拥有者的资源类型。\n    - Name：拥有者的名称。\n    - UID：拥有者的唯一标识符（UID），这是区分不同资源实例的关键。\n    - Controller（可选）：布尔值，表示是否由一个控制器（如Deployment或StatefulSet）管理这个关系。如果是控制器，Kubernetes会在适当的时候自动管理被拥有资源。\n\n\n## 2.5 CRD命名规范\nCRD的全名必须是符合如下的命名规范：${Kind}.${Group}.${Organization}.[kubenode.alibaba-inc.com](http://kubenode.alibaba-inc.com/).\n- ${Organization}：一般为仓库的git group，即团队英文简称\n- ${Group}：必须是一种功能类别，如ops、apps、auth等，尽量用精简的单个英文单词的方式传达你的CRD属于的”类别”。组成的字母必须小写\n- ${Kind}：即为CRD真正的短名字，用精简的单个或多个英文单词的拼接来明明真正的CRD短名字。如AdvanceDeployment，NetBook等。使用大驼峰命名法(首字母也是答谢，即UpperCamelCase)。\n- [alipay.com](http://alipay.com/)：根据自己的公司名称进行确定，即Company Name Domain\n- 目前对于CRD的版本转换不太友好，一般统一使用v1.\n\n## 2.6 Spec，Status 规范\n- 用命令在apis包下生成CRD Types后，请不要随意修改apis里的结构体、命名规则、以及注释。\n- 只能，也只修改${Kind}_types.go文件里的Spec和StatusSpec结构体里的内容。\n- Spec和StatusSpec里的字段都必须是Public的，也就是字段名首字母是大写。\n- 每个字段，都应该写上JSON Tag，JSON Tag必须使用小驼峰命名法，即LowerCamelCase。\n- 如果字段允许为空，JSON Tag记得带上omitempty。StatusSpec的字段一般都是允许为空的。例子：\n\n```shell\ntype DemoSpecs struct {\n        // FiledA 允许为空\n        FieldA string `json\"fieldA,omitempty\"`\n        \n        // FieldB 不允许为空\n        FieldB string `json\"fieldB\"`\n}\n```\n\n# 3 Operator 工作流程\n![img_1.png](img_1.png)\n\n## 3.1 核心组件\n- Informer：一个依赖 Kubernetes List/Watch API 、可监听事件并触发回调函数的二级缓存工具包。\n    - 在启动时Reflector调用List API获得crd的全部Object实例，并缓存在Local Store中。\n    - 然后Reflector调用Watch API来获取和监听对象实例的变化，维护缓存的变化。\n    - 每当收到add/delete等实例请求时，Reflector会将变更的事件+对象推送到deltaFIFO的队列中。\n    - 根据delttaFIFO中的内容，先到Local Store中更新对象的信息与状态，之后经过eventHandler进入WorkQueue，进而触发controller的reconcilet调谐逻辑。\n- WorkQueue：需要处理的事件队列，供controller来进行真正的业务处理。\n- Control Loop：实际的控制器角色，通过实现调谐逻辑，确保期望和实际运行状态是一致的。通过Lister向Local Store中读Object实例，通过Client向API Server写具体的操作逻辑。\n\n## 3.2 实现细节\n- 消息可靠性：List短链接查询当前全量的资源及状态；Watch长链接接受增量的资源变更事件并做相应处理。两者互相帮助，达到消息的可靠性和数据的一致性。\n- 共享Informer：对同一类型的资源只建立一个链接，为多个controller提供共享cache的能力。\n- 若请求为添加操作，Indexer把API对象保存到本地缓存中，并为它创建索引；若为删除操作，则在本地缓存中删除该对象。\n- LocalStore 会周期性地把所有资源的信息重新放到 DeltaFIFO 中，确保二级缓存间的同步，让失败的事件得到重新处理。若在入队前发现DeltaFIFO中已经有新版本的Object实例，则不入队。\n- Reconcilers是核心处理逻辑，但其只获取资源的名称和命名空间，并不知道资源的操作(增删改)是什么，也不知道资源的其他信息。目的就是在收到资源变更时，根据object的期望状态直接调整资源的状态。\n\n\n## 3.3 以Pod为对象的流程示例\n![img_2.png](img_2.png)\n\n- Informer 在初始化时，Reflector 会先通过 List API 向 API Server 获得所有的 Pod 实例。\n- Reflector 拿到全部 Pod 实例后，会将全部 Pod 实例放到 Local Store 中\n- 如果后面 Controller 调用 Lister 的 List/Get 方法获取 Pod 实例时， 那么 Lister 会直接从 Local Store 中拿数据。\n- Informer 初始化完成之后，Reflector开始通过Watch API监听Pod相关的所有事件；如果此时 pod_1 被删除，那么 Reflector 会监听到这个事件。\n- Reflector 将 pod_1 被删除的这个事件发送到 DeltaFIFO\n- DeltaFIFO 首先会将这个事件（一般为key : value的形式）存储在自己的WorkQueue，然后会直接操作 Local Store 中的数据，删除 Local Store 中的 pod_1。\n- WorkQueue 会 Pop 这个事件到 Controller 中。\n- Controller 收到这个事件，通过 Lister 从本地 Local Store 中获取真正的对象实例，执行真正的业务逻辑。\n    - 若有关联资源（ownerReference为该对象的资源），则删除关联资源。\n    - 若无关联资源，则等待读取下一个事件。\n\n# 4 原生实现\n- CRD实现： 首先利用CRD的定义文件types.go，通过官方的code-generator生成CRD的相关代码，包括标准client，deepcopy，informer和lister。\n- controller实现： 然后基于官方的sample-controller的实践示例，进行自定义的编排逻辑编写，包括对于不同add / update /delete 等操作的响应等细节操作。\n- YAML文件编写： 手动编写CRD定义和Deployment定义YAML文件，通过kubectl apply完成k8s资源的扩展和控制器的部署。\n- 其他开发工作： 比如对Controller进行二进制编译、镜像打包上传、以Deployment中的Pod的形式部署到K8s中等。\n\n等后面来看下，这些工作中，脚手架能帮忙实现多少。\n![img_3.png](img_3.png)\n\n\n"},{"title":"集群接入dashboard","url":"/2025/07/kubernetes/ji-qun-jie-ru-dashboard/","content":"# 1 Kubernets Dashboard安装\n下载kubernetes-dashboard的yaml:\n```shell\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml -O dashboard.yaml\n```\n\n在官方yaml里，我们需要将镜像修改一下，不然会遇见ImagePullBackOff的错误：\n- kubernetesui/dashboard:v2.7.0\n- kubernetesui/metrics-scraper:v1.0.8\n- 备注：版本可能不同，认准镜像名\n\n将以上镜像修改为可用的：\n- m.daocloud.io/docker.io/kubernetesui/dashboard:v2.7.0\n- m.daocloud.io/docker.io/kubernetesui/metrics-scraper:v1.0.8\n  \n安装dashboard：\n```shell\nkubectl apply -f ./dashboard.yaml\n```\n\n验证操作界面已经部署并且正在运行:\n```shell\nsudo kubectl get pod -n kubernetes-dashboard\nNAME                                         READY   STATUS    RESTARTS   AGE\ndashboard-metrics-scraper-864c58f57b-fjlfs   1/1     Running   0          98s\nkubernetes-dashboard-58db7bd7d4-pdh76        1/1     Running   0          98s\n```\n\n创建 ServiceAccount 和 ClusterRoleBinding 以提供对新创建的集群的管理权限访问:\n```shell\nkubectl create serviceaccount -n kubernetes-dashboard admin-user\nkubectl create clusterrolebinding -n kubernetes-dashboard admin-user --clusterrole cluster-admin --serviceaccount=kubernetes-dashboard:admin-user\n```\n\n需要用 Bearer Token 来登录到操作界面。使用以下命令将 token 保存到变量:\n```shell\nsudo kubectl -n kubernetes-dashboard create token admin-user\neyJhbGciOiJSUzI1NiIsImtpZCI6InF3b1ZJN0ZVSWUyRkF4blgxVG42d2hVMm0wTGtoSTg3VkVoai1yRTdMN3MifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzI3OTc0NzYyLCJpYXQiOjE3Mjc5NzExNjIsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiZTI2YjE1NjctYjk5YS00ZGRlLTlmMWUtYTIwYmUzZDAwZGJiIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiNWMxYjMyMmUtNDFmNS00ODcxLTkxNjQtZTYzOTk2NzkxZDM4In19LCJuYmYiOjE3Mjc5NzExNjIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.d-b12OQrq_9BnmWz5g-_2nvRS-ktEhg813N8zb-kWBh5GScUHhuiAej2v1p1kt54Xom1H6DaeyvlmL3G8ub7aKgZwJjOyJBFDnt0B04Ysz-KSj788jR_Yg2d1FhTbgk8-pBdV9qSweBVT6GRyQ53NIsTIc5ArDsvfOg66nEiW9rp5-3XLitKpoSLtp_Dpib1VpOR_1XAV8wRNVc9psxOp3vtALs1_jI0Izo_4qOX17OZ9FnxgkeeKglRFynlgGiQ0g2KG74oYQn0b_sUROvb52cdDJ2RDhk4yao2vjMyg19f_x1gK-xM8O7kgfYkA8gXEzguRMl0OEbWP_UgH0RQqA\n```\n\n使用 kubectl 命令行工具运行以下命令以访问操作界面：\n```shell\nkubectl proxy\n```\n\n进入dashboard：\n```shell\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n```\n\n![img.png](img.png)\n![img_1.png](img_1.png)\n\n这里是用kubectl proxy起了一个代理，实现在集群外访问集群内的dashboard"},{"title":"加速代理","url":"/2025/07/kubernetes/jia-su-dai-li/","content":"# 1 友情链接\n- 镜像加速：https://github.com/DaoCloud/public-image-mirror\n- 二进制文件加速：https://github.com/DaoCloud/public-binary-files-mirror\n- Helm 加速：https://github.com/DaoCloud/public-helm-charts-mirror\n\n# 2 二进制文件加速\n## 2.1 使用方法\n在原始 URL 上面加入 files.m.daocloud.io 的 前缀 就可以使用。比如：\n```shell\n# Helm 下载原始URL\nwget https://get.helm.sh/helm-v3.9.1-linux-amd64.tar.gz\n\n# 加速后的 URL\nwget https://files.m.daocloud.io/get.helm.sh/helm-v3.9.1-linux-amd64.tar.gz\n```\n\n## 2.2 安装 Helm\n```shell\ncd /tmp\nexport HELM_VERSION=\"v3.9.3\"\n\nwget \"https://files.m.daocloud.io/get.helm.sh/helm-${HELM_VERSION}-linux-amd64.tar.gz\"\ntar -zxvf helm-${HELM_VERSION}-linux-amd64.tar.gz\nmv linux-amd64/helm /usr/local/bin/helm\nhelm version\n```\n\n## 2.3 安装 KIND\n```shell\ncd /tmp\nexport KIND_VERSION=\"v0.22.0\"\n\ncurl -Lo ./kind https://files.m.daocloud.io/github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-linux-amd64\nchmod +x ./kind\nmv ./kind /usr/bin/kind\nkind version\n```\n\n## 2.4 安装 istio\n```shell\ncd /tmp\nexport ISTIO_VERSION=\"1.14.3\"\n\nwget \"https://files.m.daocloud.io/github.com/istio/istio/releases/download/${ISTIO_VERSION}/istio-${ISTIO_VERSION}-linux-amd64.tar.gz\"\ntar -zxvf istio-${ISTIO_VERSION}-linux-amd64.tar.gz\n# Do follow the istio docs to install istio\n```\n\n## 2.5 安装 nerdctl （代替 docker 工具）\n```shell\nexport NERDCTL_VERSION=\"1.7.6\"\nmkdir -p nerdctl ;cd nerdctl\nwget https://files.m.daocloud.io/github.com/containerd/nerdctl/releases/download/v${NERDCTL_VERSION}/nerdctl-full-${NERDCTL_VERSION}-linux-amd64.tar.gz\ntar -zvxf nerdctl-full-${NERDCTL_VERSION}-linux-amd64.tar.gz\nmkdir -p /opt/cni/bin ;cp -f libexec/cni/* /opt/cni/bin/ ;cp bin/* /usr/local/bin/ ;cp lib/systemd/system/*.service /usr/lib/systemd/system/\nsystemctl enable containerd ;systemctl start containerd --now\nsystemctl enable buildkit;systemctl start buildkit --now\n```\n\n# 3 docker镜像加速\n## 3.1 使用方法\n增加前缀 (推荐方式)。比如：\n```shell\n              docker.io/library/busybox\n                 |\n                 V\nm.daocloud.io/docker.io/library/busybox\n```\n\n或者 支持的镜像仓库 的 前缀替换 就可以使用。比如：\n```shell\n           docker.io/library/busybox\n             |\n             V\ndocker.m.daocloud.io/library/busybox\n```\n\n## 3.2 通过 加速 安装 kubeadm\n```shell\nkubeadm config images pull --image-repository k8s-gcr.m.daocloud.io\n```\n\n## 3.3 通过 加速 安装 kind\n```shell\nkind create cluster --name kind --image m.daocloud.io/docker.io/kindest/node:v1.22.1\n```\n\n## 3.4 通过 加速 部署 应用(这里以 Ingress 为例)\n```shell\nwget -O image-filter.sh https://github.com/DaoCloud/public-image-mirror/raw/main/hack/image-filter.sh && chmod +x image-filter.sh\n\nwget -O deploy.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/baremetal/deploy.yaml\n\ncat ./deploy.yaml | ./image-filter.sh | kubectl apply -f -\n```\n\n## 3.5 Docker 加速 \n添加到 /etc/docker/daemon.json\n```shell\n{\n  \"registry-mirrors\": [\n    \"https://docker.m.daocloud.io\"\n  ]\n}\n```\n\n## 4 helm 加速\n```shell\nhelm repo add community https://release.daocloud.io/chartrepo/community \n```\n\n\n"},{"title":"kubectl常用命令","url":"/2025/07/kubernetes/kubectl-chang-yong-ming-ling/","content":"# 登录命令\n根据机器ip使用kubectl登录机器(field-selector):\n```shell\n#!/bin/bash\nexport targetIp=\"6.0.90.240\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\npodinfo=`kubectl get pod --all-namespaces --field-selector=status.podIP=\"$targetIp\" -o wide | grep -v NAME | head -n 1 `\nns=`echo ${podinfo} | awk '{print $1}'`\npod=`echo ${podinfo} | awk '{print $2}'`\n\necho \"$kubectl exec -it -n ${ns} ${pod} -- su - root\"\nkubectl exec -it -n ${ns} ${pod} -- su - root\n```\n\n根据机器ip使用kubectl登录机器(label):\n```shell\n#!/bin/bash\nexport targetIp=\"6.3.144.241\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\npodinfo=`kubectl get pod --all-namespaces -l sigma.ali/ip=\"$targetIp\" -o wide | grep -v NAMESPACE`\nns=`echo ${podinfo} | awk '{print $1}'`\npod=`echo ${podinfo} | awk '{print $2}'`\n\necho \"$kubectl exec -it -n ${ns} ${pod} -- su - root\"\nkubectl exec -it -n ${ns} ${pod} -- su - root\n```\n\n更智能版本的kubectl登录命令：\n- 查看KUBECONFIG_DIR目录下有哪些kubeconfig可以用\n- 校验目标登录的ip格式\n- 查询并解析pod信息\n- 查询该pod有哪些容器并展示\n- 查询选定的容器有哪些用户(与user_array做交集)，支持自定义输入用户\n- 根据以上信息登录目标ip对应pod的选定容器\n```shell\n#!/bin/bash\n\n# 添加特定用户\nuser_array=(\"root\" \"admin\" \"log\")\n\n# 从指定目录获取所有 kubeconfig 文件\nKUBECONFIG_DIR=\"/Users/king/.kube\"\n\n# 初始化 kubectl 命令前缀\nKUBECTL_CMD=\"kubectl\"\n\n# 确保将信息打印到终端里，即使在函数之间$(...)调用的场景\nfunction print_to_console() {\n    printf \"%s\\n\" \"$1\" >&2\n}\n\n# 检查IP地址是否符合正确的格式\nfunction is_valid_ip() {\n    local ip=$1\n    local valid_regex='^([0-9]{1,3}\\.){3}[0-9]{1,3}$'\n\n    if [[ $ip =~ $valid_regex ]]; then\n        # 确保每个数字部分小于等于255\n        IFS='.' read -r -a octets <<< \"$ip\"\n        for octet in \"${octets[@]}\"; do\n            if ((octet > 255)); then\n                return 1\n            fi\n        done\n        return 0\n    fi\n    return 1\n}\n\nKUBECONFIGS=($(find $KUBECONFIG_DIR -maxdepth 1 -name \"*.config\" -print))\n\n# 检查是否找到 kubeconfig 文件\nif [ ${#KUBECONFIGS[@]} -eq 0 ]; then\n    print_to_console \"没有找到任何 kubeconfig 文件在目录: $KUBECONFIG_DIR\"\nelse\n    # 提供给用户选择的菜单，动态生成选项范围提示\n    cat << EOF\n    ----------------------------------------------\n    |*******Please Enter Your Choice:[1-${#KUBECONFIGS[@]}]*******|\n    ----------------------------------------------\nEOF\n\n    # 输出可供选择的配置文件选项\n    for i in \"${!KUBECONFIGS[@]}\"; do\n        print_to_console \"*     $(($i + 1)) ${KUBECONFIGS[$i]}\"\n    done\n\n    # 捕获用户输入并确保在合法范围内\n    while true; do\n        read -p \"please input your choice [1-${#KUBECONFIGS[@]}] (or press Enter to skip): \" num\n        if [[ -z \"$num\" ]]; then\n            break\n        elif [[ \"$num\" =~ ^[0-9]+$ ]] && [ \"$num\" -ge 1 ] && [ \"$num\" -le ${#KUBECONFIGS[@]} ]; then\n            selected_config=\"${KUBECONFIGS[$((num - 1))]}\"\n            KUBECTL_CMD=\"kubectl --kubeconfig=$selected_config\"\n            print_to_console \"Using configuration file: $selected_config\"\n            break\n        else\n            print_to_console \"Invalid choice. Please try again.\"\n        fi\n    done\nfi\n\n# 输出用户选择的配置文件\nselected_config=\"${KUBECONFIGS[$((num - 1))]}\"\nprint_to_console \"You selected: $selected_config\"\n\n# 捕获用户输入的targetIP\nwhile true; do\n    read -p \"please input your target ip: \" targetIP\n\n    # 检查输入是否为空和格式有效性\n    if [[ -z \"$targetIP\" ]]; then\n        print_to_console \"IP 地址不能为空，请重新输入。\"\n    elif ! is_valid_ip \"$targetIP\"; then\n        print_to_console \"无效的IP格式，请输入有效的IP地址。\"\n    else\n        print_to_console \"您输入的IP地址是: $targetIP\"\n        break\n    fi\ndone\n\n# 获取 Pod 信息\npodinfo=$($KUBECTL_CMD get pod --all-namespaces --field-selector=status.podIP=\"$targetIP\" -o wide | grep -v NAME | head -n 1)\n\n# 检查 podinfo 是否为空\nif [[ -z \"$podinfo\" ]]; then\n    print_to_console \"未能获取到对应 IP 的 Pod 信息，退出脚本。\"\n    exit 1\nfi\n\n# 提取命名空间和 Pod 名称\nns=$(echo \"${podinfo}\" | awk '{print $1}')\npod=$(echo \"${podinfo}\" | awk '{print $2}')\n\n# 检查 ns 和 pod 是否为空\nif [[ -z \"$ns\" || -z \"$pod\" ]]; then\n    print_to_console \"未能提取到命名空间或 Pod 名称，退出脚本。\"\n    exit 1\nfi\n\nprint_to_console \"Namespace: $ns, Pod: $pod\"\n\n# 获取容器列表\ncontainers=($($KUBECTL_CMD get pod $pod -n $ns -o jsonpath='{.spec.containers[*].name}'))\nselected_container=\"\"\nif [ ${#containers[@]} -gt 0 ]; then\n    print_to_console \"请选择一个容器 (或直接按 Enter 跳过使用默认容器):\"\n    for i in \"${!containers[@]}\"; do\n        print_to_console \"*      $(($i + 1)) ${containers[$i]}\"\n    done\n\n    while true; do\n        read -p \"please input your choice [1-${#containers[@]}] (or press Enter to skip): \" container_num\n        if [[ -z \"$container_num\" ]]; then\n            break\n        elif [[ \"$container_num\" =~ ^[0-9]+$ ]] && [ \"$container_num\" -ge 1 ] && [ \"$container_num\" -le ${#containers[@]} ]; then\n            selected_container=\"${containers[$((container_num - 1))]}\"\n            print_to_console \"Selected container: $selected_container\"\n            break\n        else\n            print_to_console \"Invalid choice. Please try again.\"\n        fi\n    done\nfi\n\n# 解析容器中的用户并添加到用户数组中\nwhile IFS=: read -r username _ uid _; do\n    if [[ $uid -ge 1000 && $username != \"nobody\" ]]; then\n        user_array+=(\"$username\")\n    fi\ndone <<< \"$user_list\"\n\n# 显示可供选择的用户列表\nprint_to_console \"请选择一个用户 (或自定义输入):\"\nfor i in \"${!user_array[@]}\"; do\n    print_to_console \"*      $(($i + 1)) ${user_array[$i]}\"\ndone\nprint_to_console \"*      $(( ${#user_array[@]} + 1 )) 自定义输入 \"\n\n# 捕获用户选择的用户\nwhile true; do\n    read -p \"please input your choice [1-$(( ${#user_array[@]} + 1 ))]: \" user_num\n    if [[ \"$user_num\" =~ ^[0-9]+$ ]] && [ \"$user_num\" -ge 1 ] && [ \"$user_num\" -le $(( ${#user_array[@]} + 1 )) ]; then\n        if [ \"$user_num\" -eq $(( ${#user_array[@]} + 1 )) ]; then\n            read -p \"请输入自定义用户名: \" targetUser\n        else\n            targetUser=\"${user_array[$((user_num - 1))]}\"\n        fi\n        print_to_console \"Selected user: $targetUser\"\n        break\n    else\n        print_to_console \"Invalid choice. Please try again.\"\n    fi\ndone\n\n# 执行命令\nif [[ -n \"$selected_container\" ]]; then\n    print_to_console \"$KUBECTL_CMD exec -it -n ${ns} ${pod} -c ${selected_container} -- su - $targetUser\"\n    $KUBECTL_CMD exec -it -n ${ns} ${pod} -c ${selected_container} -- su - $targetUser\nelse\n    print_to_console \"$KUBECTL_CMD exec -it -n ${ns} ${pod} -- su - $targetUser\"\n    $KUBECTL_CMD exec -it -n ${ns} ${pod} -- su - $targetUser\nfi\n```\n\n# 查询命令\n根据机器ip(field-selector)查询pod:\n```shell\n#!/bin/bash\nexport fieldKEY=\"status.podIP\"\nexport fieldVALUE=\"6.0.90.240\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\nkubectl get pod --all-namespaces --field-selector=$fieldKEY=$fieldVALUE -o wide\n```\n\n根据label查询pod:\n```shell\n#!/bin/bash\nexport labelKEY=\"sigma.ali/ip\"\nexport labelVALUE=\"6.0.90.240\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\nkubectl get pod --all-namespaces -l $labelKEY=$labelVALUE -o wide\n```\n\n# 导出yaml\n根据机器ip使用kubectl导出机器yaml:\n```shell\n#!/bin/bash\nlocal podName=\"\"\nlocal namespace=\"\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\nkubectl get pod/$podName -n ${namespace} -oyaml\n```\n\n# describe\n根据namespace和podName进行describe\n```shell\nlocal namespace=\"longtermbase\"\nlocal podName=\"inplaceset-antcodebuild-tn1oimjfl-gz00b-0\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\nkubectl describe pod $podName -n $namespace\n```\n\n# 清理terminating的pod\n通过清理finalizers实现\n```shell\nlocal namespace=\"\"\nlocal podName=\"\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\nkubectl patch pod/$podName -n $namespace -p '{\"metadata\":{\"finalizers\":null}}'\n```\n\n强制删除\n```shell\nlocal namespace=\"\"\nlocal podName=\"\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\nk delete pod/$podName -n $namespace --force --grace-period=0\n```\n\n# 复制文件到pod容器\n```shell\nlocal namespace=\"\"\nlocal podName=\"\"\nlocal sourceDir=\"\"\nlocal sourceFile=\"\"\nlocal targetDir=\"\"\nlocal targetFile=\"\"\n\n#alias kubectl='kubectl'\nalias kubectl='kubectl --kubeconfig=/Users/king/.kube/sa128.config'\n\nkubectl cp -n linkw $sourceDir/$sourceFile $podName:/targetDir/targetFile\n```"},{"title":"Kind搭建测试集群","url":"/2025/07/kubernetes/kind-da-jian-ce-shi-ji-qun/","content":"# Kind工具介绍\n"},{"title":"Ldap通过helm部署","url":"/2025/07/devops/ldap/ldap-tong-guo-helm-bu-shu/","content":"\n# 简介\n> ⚡️: OpenLDAP是轻量级目录访问协议（LDAP）的开源实现，它提供了一种存储和访问关于用户、组、计算机和其他资源的信息的中心化目录服务。\n\n# 属性介绍\n- cn（common name）：通用名称，表示一个对象的名称。在用户条目中，通常与用户的姓名相对应；在组条目中，则与组名相对应。\n- ou（organizational unit）：组织单位，表示一个组织或部门。在LDAP目录服务中，可以使用ou来创建多级组织结构，并将用户和其他对象分配到相应的组织单元中，以便更好地管理它们。\n- dc（domain component）：域组件，表示域名的一部分。在LDAP中，域名通常是按照层次结构组织的，例如：example.com可以被拆分为dc=example,dc=com。这样做有利于有效地组织和管理大规模的目录服务。\n- sn（surname）：姓氏，表示一个人的姓氏。与cn属性不同，sn只表示姓氏，而且通常不唯一。\n\n# kubernetes集群部署\n参考[快速搭建kind测试集群](https://hua-ri.cn/2025/07/kubernetes/Kind搭建测试集群/)快速搭建一本地测使用的kind集群。\n\n# Helm部署\n## 极简部署\n```shell\nhelm repo add stable https://charts.helm.sh/stable\nhelm install openldap stable/openldap\n```\n\n## 常规部署\n部署十分简单，但是我们可能需要调整Helm chart配置，所以建议把包拉取到本地调整之后再进行部署，下面是详细流程\n\n## 增加helm repo\n增加helm repo：\n```shell\nhelm repo add stable https://charts.helm.sh/stable\n```\n\n```shell\n➜  ~ helm repo add stable https://charts.helm.sh/stable\n\"stable\" has been added to your repositories\n```\n\n检索验证repo：\n```shell\nhelm search repo openldap\n```\n```shell\n➜  ~ helm search repo openldap\nNAME                   CHART VERSION        APP VERSION        DESCRIPTION                                   \nstable/openldap        1.2.7                2.4.48             DEPRECATED - Community developed LDAP software\n\n```\n## 拉取chart\n为了能够进行配置修改操作，将chart拉到本地：\n```shell\nhelm pull stable/openldap\n```\n\n解压chart：\n```shell\ntar -zxvf openldap-1.2.7.tgz \n```\n```shell\n➜  github tar -zxvf openldap-1.2.7.tgz \nx openldap/Chart.yaml\nx openldap/values.yaml\nx openldap/templates/NOTES.txt\nx openldap/templates/_helpers.tpl\nx openldap/templates/configmap-customldif.yaml\nx openldap/templates/configmap-env.yaml\nx openldap/templates/deployment.yaml\nx openldap/templates/pvc.yaml\nx openldap/templates/secret.yaml\nx openldap/templates/service.yaml\nx openldap/templates/tests/openldap-test-runner.yaml\nx openldap/templates/tests/openldap-tests.yaml\nx openldap/.helmignore\nx openldap/README.md\n```\n\n## 配置调整\n拉取的Chart包，整体比较小，来看下大概的结构。\n```shell\n.\n├── Chart.yaml\n├── README.md\n├── templates\n│   ├── NOTES.txt\n│   ├── _helpers.tpl\n│   ├── configmap-customldif.yaml\n│   ├── configmap-env.yaml\n│   ├── deployment.yaml\n│   ├── pvc.yaml\n│   ├── secret.yaml\n│   ├── service.yaml\n│   └── tests\n│       ├── openldap-test-runner.yaml\n│       └── openldap-tests.yaml\n└── values.yaml\n\n```\n\n我们如果有参数方面的变更，可以在values.yaml文件内进行填充配置，或者修改配置。\n\ntemplates目录下就是整个chart的模板，包含了部署需要的configmap、deployment、pvc、secret、svc等资源yaml模板。\n\n刚刚我们拉取的Chart包 部署openldap，values.yaml文件内需要调整的地方不是很多:\n```shell\nenv:\n  LDAP_ORGANISATION: \"King Inc.\"\n  LDAP_DOMAIN: \"king-ldap.net\"\n  LDAP_BACKEND: \"hdb\"\n  LDAP_TLS: \"false\"\n  LDAP_TLS_ENFORCE: \"false\"\n  LDAP_REMOVE_CONFIG_AFTER_SETUP: \"false\"\n\n```\n\n## 部署\n在chart的根目录执行安装：\n```shell\n helm install --kubeconfig=$HOME/.kube/king_test_config ldap ./\n```\n\n```shell\nPassword:\nWARNING: This chart is deprecated\nNAME: ldap\nLAST DEPLOYED: Wed Jan 15 21:56:54 2025\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nOpenLDAP has been installed. You can access the server from within the k8s cluster using:\n\n  ldap-openldap.default.svc.cluster.local:389\n\n\nYou can access the LDAP adminPassword and configPassword using:\n\n  kubectl get secret --namespace default ldap-openldap -o jsonpath=\"{.data.LDAP_ADMIN_PASSWORD}\" | base64 --decode; echo\n  kubectl get secret --namespace default ldap-openldap -o jsonpath=\"{.data.LDAP_CONFIG_PASSWORD}\" | base64 --decode; echo\n\n\nYou can access the LDAP service, from within the cluster (or with kubectl port-forward) with a command like (replace password and domain):\n  ldapsearch -x -H ldap://ldap-openldap.default.svc.cluster.local:389 -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w $LDAP_ADMIN_PASSWORD\n\n\nTest server health using Helm test:\n  helm test ldap\n\n\nYou can also consider installing the helm chart for phpldapadmin to manage this instance of OpenLDAP, or install Apache Directory Studio, and connect using kubectl port-forward.\n\n```\n\n# Helm 测试\n```shell\nhelm list ldap\n```\n\n查看idap的pod：\n```shell\nkubectl --kubeconfig=$HOME/.kube/king_test_config get pod -A|grep ldap\n```\n\n```shell\nNAMESPACE            NAME                                               READY   STATUS    RESTARTS   AGE\ndefault              ldap-openldap-6d5cc55fc-vzq5v                      1/1     Running   0          17m\n```\n\n登录idap的pod：\n```shell\nkubectl --kubeconfig=$HOME/.kube/king_test_config exec -it pod/ldap-openldap-6d5cc55fc-vzq5v -- /bin/bash\n```\n\n可以使用ldapsearch命令进行查询：\n```shell\n$ ldapsearch -x -b <search_base> -H <ldap_host> -D <bind_dn> -W\n```\n- -x：表示简单的身份认证。\n- -b：指定搜索的 DC。\n- -H：指定搜索的主机 URL，如果你是在 LDAP 服务器上，则不需要带这个参数。比如我这里为 ldap://192.168.31.76:389\n- -D：绑定的 DN。\n- -W：绑定的 DN 的密码。\n\n例如一个实际的查询命令：\n```shell\nldapsearch -x -b \"dc=king-ldap,dc=net\" -D \"cn=admin,dc=king-ldap,dc=net\"  -W\n```\n\n会要求输入密码，可以如此获取：\n```shell\nenv|grep -i PASSWORD\n```\n```shell\nLDAP_CONFIG_PASSWORD=M2ulnYD2lZPvM1hKsYP5sCy8meDN8h61\nLDAP_ADMIN_PASSWORD=a4QL47j3QvQWaTkDolvgLHpbpX8YDTdn\n```\n\n查询结果：\n```shell\nroot@ldap-openldap-6d5cc55fc-vzq5v:~# ldapsearch -x -b \"dc=king-ldap,dc=net\" -D \"cn=admin,dc=king-ldap,dc=net\"  -W\nEnter LDAP Password: \n# extended LDIF\n#\n# LDAPv3\n# base <dc=king-ldap,dc=net> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# king-ldap.net\ndn: dc=king-ldap,dc=net\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: King Inc.\ndc: king-ldap\n\n# admin, king-ldap.net\ndn: cn=admin,dc=king-ldap,dc=net\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9eEl0d3JvcFN1cTgxelBLSFozVHYzQzJSdmtLcXNXenY=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 3\n# numEntries: 2\n\n```\n\n# Helm 数据持久化？\n看下pvc的yaml模板：\n```shell\n{{- if and .Values.persistence.enabled (not .Values.persistence.existingClaim) }}\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: {{ template \"openldap.fullname\" . }}\n  labels:\n    app: {{ template \"openldap.name\" . }}\n    chart: {{ template \"openldap.chart\" . }}\n    release: {{ .Release.Name }}\n    heritage: {{ .Release.Service }}\n{{- if .Values.extraLabels }}\n{{ toYaml .Values.extraLabels | indent 4 }}\n{{- end }}\nspec:\n  accessModes:\n    - {{ .Values.persistence.accessMode | quote }}\n  resources:\n    requests:\n      storage: {{ .Values.persistence.size | quote }}\n{{- if .Values.persistence.storageClass }}\n{{- if (eq \"-\" .Values.persistence.storageClass) }}\n  storageClassName: \"\"\n{{- else }}\n  storageClassName: \"{{ .Values.persistence.storageClass }}\"\n{{- end }}\n{{- end }}\n{{- end }}\n```\n\n根据配置，只要在values.yaml文件的这部分进行pvc配置的填入再进行安装即可：\n```shell\n\n## Persist data to a persistent volume\npersistence:\n  enabled: false\n  ## database data Persistent Volume Storage Class\n  ## If defined, storageClassName: <storageClass>\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n  ##   GKE, AWS & OpenStack)\n  ##\n  # storageClass: \"-\"\n  accessMode: ReadWriteOnce\n  size: 8Gi\n  # existingClaim: \"\"\n```\n"},{"title":"仓库fork后同步","url":"/2025/07/devops/environment/cang-ku-fork-hou-tong-bu/","content":"# fork仓库\n![img_4.png](img_4.png)\n![img_5.png](img_5.png)\n\n此时会将该仓库在复制一份，并存放在你的路径下：\n![img_6.png](img_6.png)\n\n# 设置上游代码库\n进入本地代码库：\n```shell\n# 从自己仓库进行clone(fork)\ngit clone https://github.com/qiqiuyang/bk-ci.git\n\n# 进入目录\ncd bk-ci/\n\n# 查看目录结构\n$ ls\nCHANGELOG              CONTRIBUTING.md  README_EN.md  support-files\nCODE_OF_CONDUCT.en.md  docker-images    README.md     THIRD-PARTY-NOTICES.txt\nCODE_OF_CONDUCT.md     docs             scripts\nCODEOWNERS             helm-charts      SECURITY.md\nCONTRIBUTING.en.md     LICENSE.txt      src\n```\n\n查看远程仓库的路径\n```shell\n$ git remote -v\norigin        https://github.com/qiqiuyang/bk-ci.git (fetch)\norigin        https://github.com/qiqiuyang/bk-ci.git (push)\n```\n\n这里可以发现从自己仓库clone下来后，fetch和push的路径都是自己的。\n\n\n设置上游代码库\n```shell\n$ git remote add upstream git@github.com:TencentBlueKing/bk-ci.git\n```\n\n再次查看远程仓库地址：\n```shell\n$ git remote -v\norigin        https://github.com/qiqiuyang/bk-ci.git (fetch)\norigin        https://github.com/qiqiuyang/bk-ci.git (push)\nupstream        git@github.com:TencentBlueKing/bk-ci.git (fetch)\nupstream        git@github.com:TencentBlueKing/bk-ci.git (push)\n```\n\n# 同步源仓库的更新\n使用下面的命令拉取源仓库的更新：\n```shell\n$ git fetch upstream\nremote: Enumerating objects: 17643, done.\nremote: Counting objects: 100% (13492/13492), done.\nremote: Compressing objects: 100% (3960/3960), done.\nremote: Total 10070 (delta 4278), reused 8600 (delta 3184), pack-reused 0 (from 0)\n接收对象中: 100% (10070/10070), 2.44 MiB | 2.05 MiB/s, 完成.\n处理 delta 中: 100% (4278/4278), 完成 657 个本地对象.\n来自 github.com:TencentBlueKing/bk-ci\n * [新分支]                master          -> upstream/master\n * [新分支]                release-1.11    -> upstream/release-1.11\n * [新分支]                release-1.14    -> upstream/release-1.14\n * [新分支]                release-1.2     -> upstream/release-1.2\n * [新分支]                release-1.3     -> upstream/release-1.3\n。。。\n。。。\n * [新标签]                v1.8.4          -> v1.8.4\n * [新标签]                v1.8.5          -> v1.8.5\n```\n\n到这里就比较清晰了，在更新时就是把源仓库的更新的分支放在upstream下，例如：upstream/master\n\n所以同步远端分支时，就是git fetch upstream，然后将自己的分支merge目标分支内容，例如：git merge upstream/master\n\n如此便可以实现自己的fork仓库同步源仓库的新变更了。\n\n# 向原仓库发起PR\n首先在自己的仓库点击Pull Request->New Pull Request，进入以下截图页面\n![img_7.png](img_7.png)\n\nbase repository为原仓库的某个分支，head repository为fork仓库发某个分支, head的某个分支代码合到base的某个分支\n![img_8.png](img_8.png)\n\n\n进入原仓库的Pull requests可看到刚才发起的PR， 这里就不演示了"},{"title":"git环境配置","url":"/2025/07/devops/environment/git-huan-jing-pei-zhi/","content":"# 环境配置\n## 设置用户签名\n配置用户名： git config --global user.name 你的用户名\n配置邮箱： git config --global user.email 注册的邮箱\n\n配置好之后，可以用git config --global --list命令查看配置是否OK\n```shell\n$ git config --global --list\nuser.name=xxx\nuser.email=xxx@163.com\n```\n\n## 设置sshKey\n生成新的rsa密钥：ssh-keygen -t rsa -C '注册GitHub的邮箱'\n```shell\nssh-keygen -t rsa -C xxx@163.com\n```\n\n一直回车就可以，即不设置密码。\n```shell\n$ ssh-keygen -t rsa -C xxx@163.com\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/king/.ssh/id_rsa): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/king/.ssh/id_rsa\nYour public key has been saved in /home/king/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:n+yyy xxx@163.com\nThe key's randomart image is:\n+---[RSA 3072]----+\n|    .*=.+.       |\n|    = +=o= o     |\n|     * ++ o .    |\n|  o . o..  .     |\n| o * .  S o      |\n|. X o  o = +     |\n| + + .. = *      |\n|..o .  o E       |\n|.o==    .        |\n+----[SHA256]-----+\n```\n\n此时已经生成了ssh ras的密钥：\n```shell\n$ ls $HOME/.ssh\nauthorized_keys  id_rsa  id_rsa.pub  known_hosts\n```\n\n## 设置github的SSH key\n![img.png](img.png)\n![img_1.png](img_1.png)\n![img_2.png](img_2.png)\n\n将上面生成的ssh公钥复制进去。\n![img_3.png](img_3.png)","tags":["devops","environment"],"categories":["environment"]},{"title":"tencentcloud_ckafka_instance","url":"/2025/07/devops/terraform/tencentcloud-ckafka-instance/","content":"# 前言\n约定格式：\n```shell\n├── aliyun                                             # 云厂商实例文件\n│   └── aliyun_ecs_mod_demo                           # 模型名\n│       └── aliyun_china_platform_7111                # 云账号名\n│           └── ecs_instance_name_20241224121212      # 实例名\n│               ├── backend.tf                        # 实例state文件保存说明：oss\n│               └── main.tf                           # 实例具体的参数\n├── modules                                            # 模型数据文件夹\n│   ├── aliyun                                        # 云厂商模型文件夹\n│   │   └── aliyun_ecs_mod_demo                       # 模型名\n│   │       ├── main.tf                               # 模型定义主文件\n│   │       ├── outputs.tf                            # 模型定义输出文件\n│   │       └── variables.tf                          # 模型定义参数文件\n│   └── tenmod                                         # 另一个云厂商模型\n└── tenent                                              # 另一个云厂商实例文件\n```\n\n# 项目结构\n```shell\n.\n├── modules\n│   └── tcloud\n│       └── tcloud_ckafka_mod_demo\n│           ├── main.tf\n│           ├── outputs.tf\n│           └── variables.tf\n└── tcloud\n    └── tcloud_ckafka_demo\n        └── tcloud_china_game_x6\n            └── ckafaka_demo_202501221738\n                ├── backend.tf\n                └── main.tf\n```\n\n## modules\n### /modules/tcloud/tcloud_ckafka_mod_demo/main.tf\n```shell\nterraform {\n  required_providers {\n    tencentcloud = {\n      source = \"tencentcloudstack/tencentcloud\"\n    }\n  }\n}\n\nprovider \"tencentcloud\" {\n  region = var.region\n}\n\ndata \"tencentcloud_availability_zones_by_product\" \"zone\" {\n  name    = var.availability_zone\n  product = \"ckafka\"\n}\n\nresource \"tencentcloud_ckafka_instance\" \"this\" {\n  instance_name      = var.instance_name\n  zone_id            = data.tencentcloud_availability_zones_by_product.zone.zones[0].id\n  vpc_id             = var.vpc_id\n  subnet_id          = var.vswitch_id\n  msg_retention_time = var.msg_retention_time\n  kafka_version      = var.kafka_version\n  disk_size          = var.disk_size\n  band_width         = var.band_width\n  disk_type          = var.disk_type\n  partition          = var.partition\n  charge_type        = var.charge_type\n\n\n\n  config {\n    auto_create_topic_enable   = var.auto_create_topic_enable\n    default_num_partitions     = var.num_partitions\n    default_replication_factor = var.replication_factor\n  }\n\n  dynamic_retention_config {\n    enable = var.dynamic_retention_config_enable\n  }\n\n}\n```\n\n### /modules/tcloud/tcloud_ckafka_mod_demo/outputs.tf\n```shell\noutput \"ckafka_instance_id\" {\n  value = tencentcloud_ckafka_instance.this.id\n}\n```\n\n### /modules/tcloud/tcloud_ckafka_mod_demo/variables.tf\n```shell\nvariable \"region\" {\n  description = \"地域\"\n  type        = string\n  default     = \"ap-shanghai\"\n}\n\nvariable \"instance_name\" {\n  description = \"kafka实例名\"\n  type        = string\n}\n\nvariable \"availability_zone\" {\n  # 如ap-shanghai-2\n  description = \"可用区\"\n  type = string\n}\n\nvariable \"charge_type\" {\n  # PREPAID(预付费), POSTPAID_BY_HOUR(按量付费)\n  description = \"kafka实例计费方式\"\n  type = string\n  validation {\n    condition     = contains([\"PREPAID\", \"POSTPAID_BY_HOUR\"], var.charge_type)\n    error_message = \"The charge_type must be one of PREPAID, POSTPAID_BY_HOUR\"\n  }\n}\n\nvariable \"kafka_version\" {\n  # 0.10.2/1.1.1/2.4.1/2.8.1\n  description = \"kafka实例版本\"\n  type = string\n  validation {\n    condition     = contains([\"0.10.2\", \"1.1.1\", \"2.4.1\", \"2.4.2\", \"2.8.1\"], var.kafka_version)\n    error_message = \"The kafka_version must be one of 0.10.2, 1.1.1, 2.4.1, 2.4.2, 2.8.1\"\n  }\n}\n\n\nvariable \"vswitch_id\" {\n  // 阿里云vswitch_id -> 腾讯云subnet_id\n  description = \"绑定子网id\"\n  type = string\n}\n\nvariable \"vpc_id\" {\n  description = \"绑定vpc_id\"\n  type = string\n}\n\nvariable \"disk_size\" {\n  # 需满足当前实例的计费规格，此处预设200和400，可以根据需要修改\n  description = \"kafka实例磁盘规格\"\n  type = number\n  validation {\n    condition     = contains([200, 400], var.disk_size)\n    error_message = \"The disk_size must be one of 200, 400\"\n  }\n}\n\nvariable \"disk_type\" {\n  # 专业版实例磁盘类型，标准版实例不需要填写，CLOUD_SSD(SSD云硬盘), CLOUD_BASIC(高性能云硬盘)\n  description = \"kafka专业版实例磁盘类型\"\n  type = string\n  default = \"\"\n  validation {\n    condition     = contains([\"\", \"CLOUD_SSD\", \"CLOUD_BASIC\"], var.disk_type)\n    error_message = \"The disk_type must be empty, or one of CLOUD_SSD, CLOUD_BASIC\"\n  }\n}\n\nvariable \"band_width\" {\n  # 单位为MBps.\n  description = \"kafka实例带宽\"\n  type = number\n}\n\nvariable \"auto_create_topic_enable\" {\n  description = \"是否自动创建topic\"\n  type = bool\n  default = true\n  validation {\n    condition     = contains([true, false], var.auto_create_topic_enable)\n    error_message = \"The auto_create_topic_enable must be one of true, false\"\n  }\n}\n\nvariable \"num_partitions\" {\n  description = \"kafka实例默认分区数\"\n  type = number\n  default = 3\n}\n\nvariable \"replication_factor\" {\n  description = \"kafka实例默认副本数\"\n  type = number\n  default = 2\n}\n\nvariable \"dynamic_retention_config_enable\" {\n  description = \"是否启用动态消息保留时间配置\"\n  type = number\n  default = 0\n  validation {\n    condition     = contains([0, 1], var.dynamic_retention_config_enable)\n    error_message = \"The dynamic_retention_config_enable must be one of 0, 1\"\n  }\n}\n\nvariable \"msg_retention_time\" {\n  # 以分钟为单位\n  description = \"kafka实例日志的最大保留时间\"\n  type = number\n  default = 10080\n}\n\nvariable \"partition\" {\n  description = \"kafka实例分区大小\"\n  type = number\n  default = 3\n}\n```\n\n## Demo\n### /tcloud/tcloud_ckafka_demo/tcloud_china_game_x6/ckafaka_demo_202501221738/backend.tf\n```shell\nterraform {\n  backend \"oss\" {\n    endpoint = \"oss-cn-hangzhou.aliyuncs.com\"\n    bucket   = \"dz-devops\" # 替换为你的 OSS Bucket 名称\n    prefix = \"terraform_state/tcloud/tcloud_ckafka_demo/tcloud_china_game_x6/ckafka_demo_202501221738\"\n    key      = \"terraform.tfstate\" # 存储状态文件的路径和名称\n    region   = \"cn-hangzhou\"       # OSS 的地域（根据你的实际情况调整）\n  }\n}\n```\n\n### /tcloud/tcloud_ckafka_demo/tcloud_china_game_x6/ckafaka_demo_202501221738/main.tf\n```shell\nmodule \"ckafka_instance\" {\n  source              = \"../../../../modules/tcloud/tcloud_ckafka_mod_demo\"\n  region              = \"ap-shanghai\"\n  instance_name       = \"ckafka-test\"\n  availability_zone   = \"ap-shanghai-2\"\n  charge_type         = \"POSTPAID_BY_HOUR\"\n  kafka_version       = \"2.4.2\"\n  vpc_id              = \"vpc-4tkroxts\"\n  vswitch_id          = \"subnet-oy1pqvzv\"\n  disk_size           = 200\n  # disk_type           = \"CLOUD_BASIC\"\n  band_width          = 20\n  auto_create_topic_enable = true\n  num_partitions     = 3\n  replication_factor = 3\n  dynamic_retention_config_enable = 1\n  msg_retention_time = 1300\n  partition = 400\n}\n```\n\n## 运行测试\n### 获取 AK/SK\n在首次使用 Terraform 之前，需要前往腾讯云的[云 API 密钥页面](https://console.cloud.tencent.com/cam/capi)申请安全凭证SecretId和SecretKey2。若已有可使用的安全凭证，则跳过该步骤2。具体步骤如下2：\n1. 登录腾讯云[访问管理控制台](https://console.cloud.tencent.com/cam)，在左侧导航栏，选择访问密钥>API 密钥管理。\n2. 在API 密钥管理页面，单击新建密钥，即可以创建一对SecretId/SecretKey。\n\n### 设置环境变量\n将获取到的SecretId和SecretKey设置为环境变量：\n```shell\nexport TENCENTCLOUD_SECRET_ID=your_secret_id\nexport TENCENTCLOUD_SECRET_KEY=your_secret_key\n```\n\n### 运行项目\n进入项目根目录，这里是ckafaka_demo_202501221738目录：\n```shell\ncd ./tcloud/tcloud_ckafka_demo/tcloud_china_game_x6/ckafaka_demo_202501221738/\n```\n\n初始化 Terraform 项目:\n```shell\n# 将xxx替换为实际backend的ak，将yyy替换为实际backend的sk\nterraform init -backend-config=\"access_key=xxx\" -backend-config=\"secret_key=yyy\"\n```\n\n该命令会下载所需的插件和依赖，并初始化后端配置。\n类似的输出（首次使用某一个provier时，会先下载）：\n```shell\nInitializing the backend...\n\nSuccessfully configured the backend \"oss\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nInitializing modules...\n- ckafka_instance in ../../../../modules/tcloud/tcloud_ckafka_mod_demo\nInitializing provider plugins...\n- Finding latest version of tencentcloudstack/tencentcloud...\n- Installing tencentcloudstack/tencentcloud v1.81.162...\n- Installed tencentcloudstack/tencentcloud v1.81.162 (signed by a HashiCorp partner, key ID 84F69E1C1BECF459)\nPartner and community providers are signed by their developers.\nIf you'd like to know more about provider signing, you can read about it here:\nhttps://www.terraform.io/docs/cli/plugins/signing.html\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n```\n\n### 预览计划变更：\n```shell\nterraform plan\n```\n```shell\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Reading...\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Read complete after 0s [id=2066006299]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # module.ckafka_instance.tencentcloud_ckafka_instance.this will be created\n  + resource \"tencentcloud_ckafka_instance\" \"this\" {\n      + band_width          = 20\n      + charge_type         = \"POSTPAID_BY_HOUR\"\n      + disk_size           = 200\n      + disk_type           = \"CLOUD_BASIC\"\n      + id                  = (known after apply)\n      + instance_name       = \"ckafka-test\"\n      + instance_type       = (known after apply)\n      + kafka_version       = \"kafka_version\"\n      + max_message_byte    = (known after apply)\n      + msg_retention_time  = 1300\n      + partition           = 400\n      + public_network      = (known after apply)\n      + renew_flag          = (known after apply)\n      + specifications_type = \"profession\"\n      + subnet_id           = \"subnet-oy1pqvzv\"\n      + tag_set             = (known after apply)\n      + upgrade_strategy    = 1\n      + vip                 = (known after apply)\n      + vpc_id              = \"vpc-4tkroxts\"\n      + vport               = (known after apply)\n      + zone_id             = 200002\n\n      + config {\n          + auto_create_topic_enable   = true\n          + default_num_partitions     = 3\n          + default_replication_factor = 3\n        }\n\n      + dynamic_retention_config {\n          + bottom_retention        = (known after apply)\n          + disk_quota_percentage   = (known after apply)\n          + enable                  = 1\n          + step_forward_percentage = (known after apply)\n        }\n\n      + tags (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\nNote: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now.\n```\n\n执行变更：\n```shell\nterraform apply\n```\n```shell\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Reading...\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Read complete after 1s [id=2066006299]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # module.ckafka_instance.tencentcloud_ckafka_instance.this will be created\n  + resource \"tencentcloud_ckafka_instance\" \"this\" {\n      + band_width          = 20\n      + charge_type         = \"POSTPAID_BY_HOUR\"\n      + disk_size           = 200\n      + disk_type           = \"CLOUD_BASIC\"\n      + id                  = (known after apply)\n      + instance_name       = \"ckafka-test\"\n      + instance_type       = (known after apply)\n      + kafka_version       = \"kafka_version\"\n      + max_message_byte    = (known after apply)\n      + msg_retention_time  = 1300\n      + partition           = 400\n      + public_network      = (known after apply)\n      + renew_flag          = (known after apply)\n      + specifications_type = \"profession\"\n      + subnet_id           = \"subnet-oy1pqvzv\"\n      + tag_set             = (known after apply)\n      + upgrade_strategy    = 1\n      + vip                 = (known after apply)\n      + vpc_id              = \"vpc-4tkroxts\"\n      + vport               = (known after apply)\n      + zone_id             = 200002\n\n      + config {\n          + auto_create_topic_enable   = true\n          + default_num_partitions     = 3\n          + default_replication_factor = 3\n        }\n\n      + dynamic_retention_config {\n          + bottom_retention        = (known after apply)\n          + disk_quota_percentage   = (known after apply)\n          + enable                  = 1\n          + step_forward_percentage = (known after apply)\n        }\n\n      + tags (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Creating...\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [10s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [20s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [30s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [40s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [50s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m0s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m10s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m20s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m30s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m40s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m50s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m0s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m10s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m20s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m30s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m40s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m50s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [3m0s elapsed]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Creation complete after 3m7s [id=ckafka-9jnda3jn]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n```\n\n### 验证创建\n在前端查看是否成功创建实例：\n![img_1.png](img_1.png)\n\n### 销毁资源\n执行下面的命令进行销毁：\n```shell\nterraform destroy\n```\n```shell\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Reading...\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Read complete after 1s [id=2066006299]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Refreshing state... [id=ckafka-9jnda3jn]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  - destroy\n\nTerraform will perform the following actions:\n\n  # module.ckafka_instance.tencentcloud_ckafka_instance.this will be destroyed\n  - resource \"tencentcloud_ckafka_instance\" \"this\" {\n      - band_width          = 20 -> null\n      - charge_type         = \"POSTPAID_BY_HOUR\" -> null\n      - disk_size           = 200 -> null\n      - disk_type           = \"CLOUD_BASIC\" -> null\n      - id                  = \"ckafka-9jnda3jn\" -> null\n      - instance_name       = \"ckafka-test\" -> null\n      - instance_type       = 1 -> null\n      - kafka_version       = \"0.10.2.1\" -> null\n      - msg_retention_time  = 1300 -> null\n      - partition           = 400 -> null\n      - public_network      = 3 -> null\n      - renew_flag          = 0 -> null\n      - specifications_type = \"profession\" -> null\n      - subnet_id           = \"subnet-oy1pqvzv\" -> null\n      - tag_set             = {} -> null\n      - upgrade_strategy    = 1 -> null\n      - vip                 = \"172.17.0.3\" -> null\n      - vpc_id              = \"vpc-4tkroxts\" -> null\n      - vport               = \"9092\" -> null\n      - zone_id             = 200002 -> null\n\n      - config {\n          - auto_create_topic_enable   = true -> null\n          - default_num_partitions     = 3 -> null\n          - default_replication_factor = 3 -> null\n        }\n\n      - dynamic_retention_config {\n          - bottom_retention        = 0 -> null\n          - disk_quota_percentage   = 0 -> null\n          - enable                  = 1 -> null\n          - step_forward_percentage = 0 -> null\n        }\n    }\n\nPlan: 0 to add, 0 to change, 1 to destroy.\n\nDo you really want to destroy all resources?\n  Terraform will destroy all your managed infrastructure, as shown above.\n  There is no undo. Only 'yes' will be accepted to confirm.\n\n  Enter a value: yes\n\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Destroying... [id=ckafka-9jnda3jn]\nmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Destruction complete after 5s\n\nDestroy complete! Resources: 1 destroyed.\n```\n\n### 验证销毁\n![img_2.png](img_2.png)"},{"title":"alicloud_alikafka_instance","url":"/2025/07/devops/terraform/alicloud-alikafka-instance/","content":"\n# 前言\n约定格式：\n```shell\n├── aliyun                                             # 云厂商实例文件\n│   └── aliyun_ecs_mod_demo                           # 模型名\n│       └── aliyun_china_platform_7111                # 云账号名\n│           └── ecs_instance_name_20241224121212      # 实例名\n│               ├── backend.tf                        # 实例state文件保存说明：oss\n│               └── main.tf                           # 实例具体的参数\n├── modules                                            # 模型数据文件夹\n│   ├── aliyun                                        # 云厂商模型文件夹\n│   │   └── aliyun_ecs_mod_demo                       # 模型名\n│   │       ├── main.tf                               # 模型定义主文件\n│   │       ├── outputs.tf                            # 模型定义输出文件\n│   │       └── variables.tf                          # 模型定义参数文件\n│   └── tenmod                                         # 另一个云厂商模型\n└── tenent                                              # 另一个云厂商实例文件\n```\n\n# 项目结构\n```shell\n.\n├── aliyun\n│   └── aliyun_alikafka_demo\n│       └── aliyun_china_pt_7111\n│           └── alikafka_demo_202502061910\n│               ├── backend.tf\n│               └── main.tf\n└── modules\n    └── aliyun\n        └── aliyun_alikafka_mod_demo\n            ├── main.tf\n            └── variables.tf\n```\n\n## modules\n### /modules/aliyun/aliyun_alikafka_mod_demo/main.tf\n```shell\nterraform {\n  required_providers {\n    alicloud = {\n      source  = \"aliyun/alicloud\"\n      version = \"1.225.0\"\n    }\n  }\n}\nprovider \"alicloud\" {\n  region = var.region\n}\n\nresource \"alicloud_alikafka_instance\" \"this\" {\n  name          = var.instance_name\n  deploy_type   = var.deploy_type\n  disk_size     = var.disk_size\n  disk_type     = var.disk_type\n  vswitch_id    = var.vswitch_id\n  partition_num = var.partition_num\n  io_max        = var.io_max\n}\n```\n\n### /modules/aliyun/aliyun_alikafka_mod_demo/variables.tf\n```shell\nvariable \"region\" {\n  description = \"地域\"\n  type        = string\n}\n\nvariable \"instance_name\" {\n  description = \"kafka实例名\"\n  type        = string\n}\n\nvariable \"deploy_type\" {\n  // - 4: eip/vpc instance；- 5: vpc instance.\n  description = \"部署类型\"\n  type        = number\n  default = 5\n  validation {\n    condition     = contains([4, 5], var.deploy_type)\n    error_message = \"The deploy_type must be one of 4, 5\"\n  }\n}\n\nvariable \"disk_size\" {\n  description = \"kafka实例磁盘规格\"\n  type = number\n  validation {\n    condition     = contains([500, 1000], var.disk_size)\n    error_message = \"The disk_size must be one of 200, 400\"\n  }\n}\n\nvariable \"disk_type\" {\n  # 0: efficient cloud disk , 1: SSD.\n  description = \"磁盘类型\"\n  type = number\n  default = 1\n  validation {\n    condition     = contains([0, 1], var.disk_type)\n    error_message = \"The disk_type must be empty, or one of 0, 1\"\n  }\n}\n\nvariable \"vswitch_id\" {\n  description = \"绑定虚拟交换机id\"\n  type = string\n}\n\nvariable \"partition_num\" {\n  description = \"分区数量\"\n  type = number\n}\n\n\nvariable \"io_max\" {\n  description = \"io的最大值\"\n  type = number\n}\n```\n\n## Demo\n### /aliyun/aliyun_alikafka_demo/aliyun_china_pt_7111/alikafka_demo_202502061910/backend.tf\n```shell\nterraform {\n  backend \"oss\" {\n    endpoint = \"oss-cn-hangzhou.aliyuncs.com\"\n    bucket   = \"dz-devops\" # 替换为你的 OSS Bucket 名称\n    prefix = \"terraform_state//aliyun/aliyun_alikafka_demo/aliyun_china_pt_7111/alikafka_demo_202502061910\"\n    key      = \"terraform.tfstate\" # 存储状态文件的路径和名称\n    region   = \"cn-hangzhou\"       # OSS 的地域（根据你的实际情况调整）\n  }\n}\n```\n\n### /aliyun/aliyun_alikafka_demo/aliyun_china_pt_7111/alikafka_demo_202502061910/main.tf\n```shell\n\nmodule \"alikafka_instance\" {\n  source        = \"../../../../modules/aliyun/aliyun_alikafka_mod_demo\"\n  region        = \"cn-hangzhou\"\n  disk_size     = 500\n  instance_name = \"alikafka-testV2\"\n  deploy_type   = 5\n  vswitch_id    = \"vsw-bp1co65f3q2s0bis9yfkg\"\n  partition_num = 50\n  io_max        = 20\n}\n```\n\n## 运行测试\n### 获取 AK/SK\n在首次使用 Terraform 之前，需要前往腾讯云的[云 API 密钥页面](https://console.cloud.tencent.com/cam/capi)申请安全凭证SecretId和SecretKey2。若已有可使用的安全凭证，则跳过该步骤2。具体步骤如下2：\n\n1. 登录腾讯云[访问管理控制台](https://console.cloud.tencent.com/cam)，在左侧导航栏，选择访问密钥>API 密钥管理。\n2. 在API 密钥管理页面，单击新建密钥，即可以创建一对SecretId/SecretKey。\n\n### 设置环境变量\n将获取到的SecretId和SecretKey设置为环境变量：\n```shell\nexport TENCENTCLOUD_SECRET_ID=your_secret_id\nexport TENCENTCLOUD_SECRET_KEY=your_secret_key\n```\n\n### 运行项目\n进入项目根目录，alikafka_demo_202502061910目录：\n```shell\ncd ./aliyun/aliyun_alikafka_demo/aliyun_china_pt_7111/alikafka_demo_202502061910\n```\n\n初始化 Terraform 项目:\n```shell\n# 将xxx替换为实际backend的ak，将yyy替换为实际backend的sk\nterraform init -backend-config=\"access_key=xxx\" -backend-config=\"secret_key=yyy\"\n```\n\n该命令会下载所需的插件和依赖，并初始化后端配置。\n类似的输出（首次使用某一个provier时，会先下载）：\n```shell\nInitializing the backend...\n\nSuccessfully configured the backend \"oss\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nInitializing modules...\nInitializing provider plugins...\n- Reusing previous version of aliyun/alicloud from the dependency lock file\n- Using previously-installed aliyun/alicloud v1.225.0\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n```\n\n预览计划变更：\n```shell\nterraform plan\n```\n\n```shell\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # module.alikafka_instance.alicloud_alikafka_instance.this will be created\n  + resource \"alicloud_alikafka_instance\" \"this\" {\n      + config            = (known after apply)\n      + deploy_type       = 5\n      + disk_size         = 500\n      + disk_type         = 1\n      + eip_max           = (known after apply)\n      + end_point         = (known after apply)\n      + group_left        = (known after apply)\n      + group_used        = (known after apply)\n      + id                = (known after apply)\n      + io_max            = 20\n      + io_max_spec       = (known after apply)\n      + is_partition_buy  = (known after apply)\n      + name              = \"alikafka-testV2\"\n      + paid_type         = \"PostPaid\"\n      + partition_left    = (known after apply)\n      + partition_num     = 50\n      + partition_used    = (known after apply)\n      + resource_group_id = (known after apply)\n      + security_group    = (known after apply)\n      + service_version   = (known after apply)\n      + spec_type         = \"normal\"\n      + status            = (known after apply)\n      + topic_left        = (known after apply)\n      + topic_num_of_buy  = (known after apply)\n      + topic_quota       = (known after apply)\n      + topic_used        = (known after apply)\n      + vpc_id            = (known after apply)\n      + vswitch_id        = \"vsw-bp1co65f3q2s0bis9yfkg\"\n      + zone_id           = (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\nNote: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now.\n```\n\n执行变更：\n```shell\nterraform apply\n```\n\n```shell\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # module.alikafka_instance.alicloud_alikafka_instance.this will be created\n  + resource \"alicloud_alikafka_instance\" \"this\" {\n      + config            = (known after apply)\n      + deploy_type       = 5\n      + disk_size         = 500\n      + disk_type         = 1\n      + eip_max           = (known after apply)\n      + end_point         = (known after apply)\n      + group_left        = (known after apply)\n      + group_used        = (known after apply)\n      + id                = (known after apply)\n      + io_max            = 20\n      + io_max_spec       = (known after apply)\n      + is_partition_buy  = (known after apply)\n      + name              = \"alikafka-testV2\"\n      + paid_type         = \"PostPaid\"\n      + partition_left    = (known after apply)\n      + partition_num     = 50\n      + partition_used    = (known after apply)\n      + resource_group_id = (known after apply)\n      + security_group    = (known after apply)\n      + service_version   = (known after apply)\n      + spec_type         = \"normal\"\n      + status            = (known after apply)\n      + topic_left        = (known after apply)\n      + topic_num_of_buy  = (known after apply)\n      + topic_quota       = (known after apply)\n      + topic_used        = (known after apply)\n      + vpc_id            = (known after apply)\n      + vswitch_id        = \"vsw-bp1co65f3q2s0bis9yfkg\"\n      + zone_id           = (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Creating...\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [10s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [20s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [30s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [40s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [50s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m0s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m10s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m20s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m30s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m40s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m50s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m0s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m10s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m20s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m30s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m40s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m50s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m0s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m10s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m20s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m30s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m40s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m50s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m0s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m10s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m20s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m30s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m40s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Creation complete after 4m45s [id=alikafka_post-cn-0gx44g6dm006]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n```\n\n### 验证创建\n在前端查看是否成功创建实例：\n![img.png](img.png)\n\n### 销毁资源\n执行下面的命令进行销毁：\n```shell\nterraform destroy\n```\n\n```shell\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Refreshing state... [id=alikafka_post-cn-0gx44g6dm006]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  - destroy\n\nTerraform will perform the following actions:\n\n  # module.alikafka_instance.alicloud_alikafka_instance.this will be destroyed\n  - resource \"alicloud_alikafka_instance\" \"this\" {\n      - config            = jsonencode(\n            {\n              - \"cloud.maxTieredStoreSpace\"           = \"0\"\n              - \"enable.acl\"                          = \"false\"\n              - \"enable.compact\"                      = \"true\"\n              - \"enable.tiered\"                       = \"false\"\n              - \"enable.vpc_sasl_ssl\"                 = \"false\"\n              - \"kafka.log.retention.hours\"           = \"72\"\n              - \"kafka.message.max.bytes\"             = \"1048576\"\n              - \"kafka.offsets.retention.minutes\"     = \"10080\"\n              - \"kafka.ssl.bit\"                       = \"1024\"\n              - \"message.timestamp.difference.max.ms\" = \"9223372036854775807\"\n              - \"message.timestamp.type\"              = \"CreateTime\"\n            }\n        ) -> null\n      - deploy_type       = 5 -> null\n      - disk_size         = 500 -> null\n      - disk_type         = 1 -> null\n      - eip_max           = 0 -> null\n      - end_point         = \"172.31.1.64:9092,172.31.1.66:9092,172.31.1.65:9092\" -> null\n      - group_left        = 2100 -> null\n      - group_used        = 0 -> null\n      - id                = \"alikafka_post-cn-0gx44g6dm006\" -> null\n      - io_max            = 20 -> null\n      - io_max_spec       = \"alikafka.hw.2xlarge\" -> null\n      - is_partition_buy  = 1 -> null\n      - name              = \"alikafka-testV2\" -> null\n      - paid_type         = \"PostPaid\" -> null\n      - partition_left    = 1050 -> null\n      - partition_num     = 50 -> null\n      - partition_used    = 0 -> null\n      - resource_group_id = \"rg-acfmzpn54i5ejry\" -> null\n      - security_group    = \"sg-bp184o2lwjnssf12wf3w\" -> null\n      - service_version   = \"2.2.0\" -> null\n      - spec_type         = \"normal\" -> null\n      - status            = 5 -> null\n      - tags              = {} -> null\n      - topic_left        = 1050 -> null\n      - topic_num_of_buy  = 1050 -> null\n      - topic_quota       = 1050 -> null\n      - topic_used        = 0 -> null\n      - vpc_id            = \"vpc-bp1sro6pb0sec14x7s05l\" -> null\n      - vswitch_id        = \"vsw-bp1co65f3q2s0bis9yfkg\" -> null\n      - zone_id           = \"zonei\" -> null\n        # (1 unchanged attribute hidden)\n    }\n\nPlan: 0 to add, 0 to change, 1 to destroy.\n\nDo you really want to destroy all resources?\n  Terraform will destroy all your managed infrastructure, as shown above.\n  There is no undo. Only 'yes' will be accepted to confirm.\n\n  Enter a value: yes\n\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Destroying... [id=alikafka_post-cn-0gx44g6dm006]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 10s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 20s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 30s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 40s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 50s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m1s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m11s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m21s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m31s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m41s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m51s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m1s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m11s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m21s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m31s elapsed]\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Destruction complete after 2m37s\n\nDestroy complete! Resources: 1 destroyed.\n```\n\n\n"},{"title":"Npd代码结构了解","url":"/2025/07/devops/npd/npd-dai-ma-jie-gou-liao-jie/","content":"# 1 pkg目录结构\n这里是各个monitor的实现。\n\n```shell\n.\n├── custompluginmonitor             -> 用于实现自定义监视插件。node-problem-detector 支持通过插件监视特定状态或事件，此目录中包含与用户自定义插件相关的代码。\n├── exporters                       -> 用于包含导出器（exporters）的实现，导出器负责将监视到的数据发送到外部系统，比如监控系统（例如 Prometheus）。导出器在数据收集和监控集成中起到关键作用。\n├── healthchecker                   -> 实现健康检查逻辑，确保 node-problem-detector 本身和其他依赖的组件处于健康状态。这是监控和维护软件稳定性的重要部分。\n├── logcounter                      -> 实现了 logcounter 的功能，通常用于计数和分析系统日志，以检测异常行为或问题。它可以与系统日志监控结合使用。\n├── problemdaemon                   -> 实现问题守护程序。这个守护程序负责监测和识别节点中的问题，通过问题发现机制收集指标并生成告警。\n├── problemdetector                 -> 包含问题检测的核心逻辑。这是 node-problem-detector 的主要功能部分，负责从不同的来源收集信息，并根据这些信息识别和报告节点问题。\n├── problemmetrics                  -> 包含问题指标的实现，用于收集、处理和导出有关节点状态和问题的数据。这些指标可以展示在监控界面上，帮助用户理解系统的健康状况。\n├── systemlogmonitor                -> 实现了系统日志监控的功能。它处理系统日志，分析日志内容，以检测可能导致问题的事件和错误。\n├── systemstatsmonitor              -> 用于监控系统的各种统计数据，例如 CPU 使用率、内存使用情况等。它负责收集和报告这些统计信息，以帮助验证节点的健康状态。\n├── types                           -> 用于定义项目中使用的各种类型结构体和常量。这些类型通常用于数据传输和处理，为核心逻辑提供数据模型。\n├── util                            -> 包含一些公用工具函数和库，这些函数可以在其他模块中复用，提高代码的可维护性和可读性。\n└── version                         -> 保存与版本相关的信息和结构。它用于管理版本号并提供版本信息的功能\n```\n\n# 2 problemdaemon\n## 2.1 Register\npkg/problemdaemon/problem_daemon.go:32\n```shell\nvar (\n        handlers = make(map[types.ProblemDaemonType]types.ProblemDaemonHandler)\n)\n\n// Register registers a problem daemon factory method, which will be used to create the problem daemon.\nfunc Register(problemDaemonType types.ProblemDaemonType, handler types.ProblemDaemonHandler) {\n        handlers[problemDaemonType] = handler\n}\n```\n\n注册一个问题守护进程工厂方法。\n参数：\n- problemDaemonType: 需要注册的守护进程类型。\n- handler: 对应的处理器，其中包含创建守护进程的方法。\n  作用：将指定类型的处理器添加到 handlers 映射中，以便后续根据类型获取对应的处理器。\n\n## 2.2 GetProblemDaemonNames\npkg/problemdaemon/problem_daemon.go:37\n```shell\n// GetProblemDaemonNames retrieves all available problem daemon types.\nfunc GetProblemDaemonNames() []types.ProblemDaemonType {\n        problemDaemonTypes := []types.ProblemDaemonType{}\n        for problemDaemonType := range handlers {\n                problemDaemonTypes = append(problemDaemonTypes, problemDaemonType)\n        }\n        return problemDaemonTypes\n}\n```\n\n检索所有可用的问题守护进程类型。\n\n返回值：返回一个包含所有注册的守护进程类型的切片。\n\n实现细节：遍历 handlers 映射，将每种问题守护进程类型添加到返回的切片中。\n\n## 2.3 GetProblemDaemonHandlerOrDie\npkg/problemdaemon/problem_daemon.go:46\n```shell\n// GetProblemDaemonHandlerOrDie retrieves the ProblemDaemonHandler for a specific type of problem daemon, panic if error occurs..\nfunc GetProblemDaemonHandlerOrDie(problemDaemonType types.ProblemDaemonType) types.ProblemDaemonHandler {\n        handler, ok := handlers[problemDaemonType]\n        if !ok {\n                panic(fmt.Sprintf(\"Problem daemon handler for %v does not exist\", problemDaemonType))\n        }\n        return handler\n}\n```\n\n获取特定类型问题守护进程的处理器。\n\n参数：问题守护进程类型。\n\n返回值：返回对应的处理器。\n\n实现细节：如果指定类型没有找到对应的处理器，则触发恐慌（panic），这通常用于在初始化时确保配置的有效性。\n\n## 2.4 NewProblemDaemons\npkg/problemdaemon/problem_daemon.go:55\n\n```shell\n// NewProblemDaemons creates all problem daemons based on the configurations provided.\nfunc NewProblemDaemons(monitorConfigPaths types.ProblemDaemonConfigPathMap) []types.Monitor {\n        problemDaemonMap := make(map[string]types.Monitor)\n        for problemDaemonType, configs := range monitorConfigPaths {\n                for _, config := range *configs {\n                        if _, ok := problemDaemonMap[config]; ok {\n                                // Skip the config if it's duplicated.\n                                klog.Warningf(\"Duplicated problem daemon configuration %q\", config)\n                                continue\n                        }\n                        problemDaemonMap[config] = handlers[problemDaemonType].CreateProblemDaemonOrDie(config)\n                }\n        }\n\n        problemDaemons := []types.Monitor{}\n        for _, problemDaemon := range problemDaemonMap {\n                problemDaemons = append(problemDaemons, problemDaemon)\n        }\n        return problemDaemons\n}\n```\n\n根据提供的配置创建所有问题守护进程。\n\n参数：\n\nmonitorConfigPaths: 包含每种问题守护进程类型及其对应配置路径的映射。\n返回值：返回创建的所有 Monitor 实例的切片。\n实现细节\n- 创建空映射：使用 problemDaemonMap 存储已创建的守护进程，以避免重复。\n- 遍历配置：\n    - 对于配置中的每种问题守护进程类型及其相应的配置路径，检查是否已经创建。\n    - 如果配置已存在，则记录警告并跳过。\n    - 使用注册的处理器创建新的问题守护进程并将其添加到映射中。\n- 返回值构建：将地图中的所有守护进程转换为切片并返回。\n\n# 3 problem_detector\n## 3.1 problemDetector定义\npkg/problemdetector/problem_detector.go:33\n```shell\n// ProblemDetector collects statuses from all problem daemons and update the node condition and send node event.\ntype ProblemDetector interface {\n        Run(context.Context) error\n}\n\ntype problemDetector struct {\n        monitors  []types.Monitor\n        exporters []types.Exporter\n}\n```\n\n字段：\n\nmonitors: 存储监控器的切片，用于收集问题状态。\n\nexporters: 存储导出器的切片，用于将收集到的状态发送到外部系统或用户。\n\n方法说明： Run(context.Context) error 启动问题检测器，并在上下文被取消或出现错误时返回错误。\n\n\n## 3.2 NewProblemDetector\npkg/problemdetector/problem_detector.go:40\n\n```shell\n// NewProblemDetector creates the problem detector. Currently we just directly passed in the problem daemons, but\n// in the future we may want to let the problem daemons register themselves.\nfunc NewProblemDetector(monitors []types.Monitor, exporters []types.Exporter) ProblemDetector {\n        return &problemDetector{\n                monitors:  monitors,\n                exporters: exporters,\n        }\n}\n```\n创建一个新的问题检测器实例。\n\n参数：\n\nmonitors: 监控器数组，用于检测不同问题。\n\nexporters: 导出器数组，用于将状态发送给外部系统。\n\n返回值：返回实现 ProblemDetector 接口的新的 problemDetector 实例。\n\n## 3.3 Run\npkg/problemdetector/problem_detector.go:48\n```shell\n// Run starts the problem detector.\nfunc (p *problemDetector) Run(ctx context.Context) error {\n        // Start the log monitors one by one.\n        var chans []<-chan *types.Status\n        failureCount := 0\n        for _, m := range p.monitors {\n                ch, err := m.Start()\n                if err != nil {\n                        // Do not return error and keep on trying the following config files.\n                        klog.Errorf(\"Failed to start problem daemon %v: %v\", m, err)\n                        failureCount++\n                        continue\n                }\n                if ch != nil {\n                        chans = append(chans, ch)\n                }\n        }\n        allMonitors := p.monitors\n\n        if len(allMonitors) == failureCount {\n                return fmt.Errorf(\"no problem daemon is successfully setup\")\n        }\n\n        defer func() {\n                for _, m := range allMonitors {\n                        m.Stop()\n                }\n        }()\n\n        ch := groupChannel(chans)\n        klog.Info(\"Problem detector started\")\n\n        for {\n                select {\n                case <-ctx.Done():\n                        return nil\n                case status := <-ch:\n                        for _, exporter := range p.exporters {\n                                exporter.ExportProblems(status)\n                        }\n                }\n        }\n}\n```\n\n执行问题检测器的主要运行逻辑。\n\n实现细节：\n- 开始监控器：遍历并启动每个监控器。\n- 如果启动失败，记录错误并增加失败计数。\n- 如果成功，则将返回的通道添加到 chans 切片。\n- 检查失败情况：如果所有监控器都失败，则返回错误。\n- 延迟关闭：在函数结束时停止所有监控器。\n- 通道组合：调用 groupChannel 函数，用于将所有监控器的状态通道组合成单一通道。\n- 监听状态：使用 select 不断监听上下文和监控器的状态。\n- 如果收到状态，从所有导出器导出问题。\n\n## 3.4 groupChannel\npkg/problemdetector/problem_detector.go:91\n```shell\nfunc groupChannel(chans []<-chan *types.Status) <-chan *types.Status {\n        statuses := make(chan *types.Status)\n        for _, ch := range chans {\n                go func(c <-chan *types.Status) {\n                        for status := range c {\n                                statuses <- status\n                        }\n                }(ch)\n        }\n        return statuses\n}\n```\n\n将多个状态通道组合为一个通道。\n\n参数：切片 chans，包含多个状态通道。\n\n返回值：返回一个新的通道 statuses，用于接收来自所有监控器的状态。\n\n实现细节\n\n启动多个 goroutine，每个 goroutine 从一个监控器的通道读取状态，并将其转发到新的 statuses 通道中。\n\n# 4 problemmetrics\n## 4.1 ProblemMetricsManager定义\npkg/problemmetrics/problem_metrics.go:40\n```shell\n// ProblemMetricsManager manages problem-converted metrics.\n// ProblemMetricsManager is thread-safe.\ntype ProblemMetricsManager struct {\n        problemCounter           metrics.Int64MetricInterface\n        problemGauge             metrics.Int64MetricInterface\n        problemTypeToReason      map[string]string\n        problemTypeToReasonMutex sync.Mutex\n}\n```\n\n字段：\n- problemCounter: 一个整型指标接口，用于计数特定问题发生的次数。\n- problemGauge: 一个整型指标接口，用于表示特定问题是否对节点产生影响。\n- problemTypeToReason: 一个映射，保存问题类型与原因的关系。\n- problemTypeToReasonMutex: 一个互斥锁，用于保护 problemTypeToReason 的并发访问。\n\n## 4.2 init\npkg/problemmetrics/problem_metrics.go:31\n```shell\n// GlobalProblemMetricsManager is a singleton of ProblemMetricsManager,\n// which should be used to manage all problem-converted metrics across all\n// problem daemons.\nvar GlobalProblemMetricsManager *ProblemMetricsManager\n\nfunc init() {\n        GlobalProblemMetricsManager = NewProblemMetricsManagerOrDie()\n}\n```\n\nGlobalProblemMetricsManager: 全局唯一的 ProblemMetricsManager 实例，用于管理所有问题相关的指标。\n\ninit 函数: 在程序启动时初始化 GlobalProblemMetricsManager，确保其可用。\n\n## 4.3 NewProblemMetricsManagerOrDie\npkg/problemmetrics/problem_metrics.go:47\n```shell\nfunc NewProblemMetricsManagerOrDie() *ProblemMetricsManager {\n        pmm := ProblemMetricsManager{}\n\n        var err error\n        pmm.problemCounter, err = metrics.NewInt64Metric(\n                metrics.ProblemCounterID,\n                string(metrics.ProblemCounterID),\n                \"Number of times a specific type of problem have occurred.\",\n                \"1\",\n                metrics.Sum,\n                []string{\"reason\"})\n        if err != nil {\n                klog.Fatalf(\"Failed to create problem_counter metric: %v\", err)\n        }\n\n        pmm.problemGauge, err = metrics.NewInt64Metric(\n                metrics.ProblemGaugeID,\n                string(metrics.ProblemGaugeID),\n                \"Whether a specific type of problem is affecting the node or not.\",\n                \"1\",\n                metrics.LastValue,\n                []string{\"type\", \"reason\"})\n        if err != nil {\n                klog.Fatalf(\"Failed to create problem_gauge metric: %v\", err)\n        }\n\n        pmm.problemTypeToReason = make(map[string]string)\n\n        return &pmm\n}\n```\n\n创建并初始化 ProblemMetricsManager 实例。\n\n实现细节：\n\n创建 problemCounter 指标，用于记录特定问题的发生次数，接收标签 reason。\n\n创建 problemGauge 指标，用于指示特定问题是否正在影响节点，接收标签 type 和 reason。\n\n如果创建指标失败，则程序会崩溃（Fatalf）。\n\n初始化问题类型到原因的映射。\n\n## 4.4 IncrementProblemCounter\npkg/problemmetrics/problem_metrics.go:79\n```shell\n// IncrementProblemCounter increments the value of a problem counter.\nfunc (pmm *ProblemMetricsManager) IncrementProblemCounter(reason string, count int64) error {\n        if pmm.problemCounter == nil {\n                return errors.New(\"problem counter is being incremented before initialized.\")\n        }\n\n        return pmm.problemCounter.Record(map[string]string{\"reason\": reason}, count)\n}\n```\n\n增加问题计数器的值，根据 reason 标签记录统计数据。\n\n参数：\n\nreason: 问题发生的原因。\n\ncount: 要增加的数量。\n\n返回值：返回可能的错误。\n\n## 4.5 SetProblemGauge\npkg/problemmetrics/problem_metrics.go:88\n```shell\n// SetProblemGauge sets the value of a problem gauge.\nfunc (pmm *ProblemMetricsManager) SetProblemGauge(problemType string, reason string, value bool) error {\n        if pmm.problemGauge == nil {\n                return errors.New(\"problem gauge is being set before initialized.\")\n        }\n\n        pmm.problemTypeToReasonMutex.Lock()\n        defer pmm.problemTypeToReasonMutex.Unlock()\n\n        // We clear the last reason, because the expected behavior is that at any point of time,\n        // for each type of permanent problem, there should be at most one reason got set to 1.\n        // This behavior is consistent with the behavior of node condition in Kubernetes.\n        // However, problemGauges with different \"type\" and \"reason\" are considered as different\n        // metrics in Prometheus. So we need to clear the previous metrics explicitly.\n        if lastReason, ok := pmm.problemTypeToReason[problemType]; ok {\n                err := pmm.problemGauge.Record(map[string]string{\"type\": problemType, \"reason\": lastReason}, 0)\n                if err != nil {\n                        return fmt.Errorf(\"failed to clear previous reason %q for type %q: %v\",\n                                problemType, lastReason, err)\n                }\n        }\n\n        pmm.problemTypeToReason[problemType] = reason\n\n        var valueInt int64\n        if value {\n                valueInt = 1\n        }\n        return pmm.problemGauge.Record(map[string]string{\"type\": problemType, \"reason\": reason}, valueInt)\n}\n```\n\n设置问题量规的值，指示特定问题对节点的影响。\n\n参数：\n- problemType: 问题类型（例如，节点不可用）。\n- reason: 具体原因。\n- value: 表示问题的状态，true表示问题存在 (1)，false表示问题消失 (0)。\n\n实现细节：\n- 使用互斥锁保护对 problemTypeToReason 的并发访问。\n- 清除之前的原因（如果存在），保证每个问题类型在任何时刻都最多只有一个原因被设置为 1。\n- 记录新的量规值。\n\n# 5 exporters\n目录结构：\n```shell\n.\n├── k8sexporter\n│   ├── condition\n│   ├── k8s_exporter.go\n│   └── problemclient\n├── prometheusexporter\n│   └── prometheus_exporter.go\n├── register.go\n├── register_test.go\n└── stackdriver\n    ├── config\n    ├── gce\n    ├── stackdriver_exporter.go\n    └── stackdriver_exporter_test.go\n```\n这里就涉及到了三个exporter，分别是k8sexporter、prometheusexporter、stackdriver_exporter\n\n这三个是比较独立的，因为能力是完全不一样的，\n\n- exporter：专注于将types.Status的event和conditation上报，并且会有一些http服务接口开放(healthz、conditions、/debug/pprof)，\n- prometheusexporter：直接用的prometheuse的库，简单理解就是为了让prometheuse可以采集到数据\n- stackdriver_exporter：处理与 Google Stackdriver 的集成，Stackdriver 是 Google Cloud 提供的监控和管理平台，该exporter负责将指标数据发送到\n  Stackdriver，以便在 Google Cloud 环境中监控应用程序和集群状态。\n\n这里来说说我的理解吧，prometheusexporter和stackdriver_exporter这两个exporter是为了将metric数据发送到对应的监控平台，而k8sexporter则是为了将types.Status发送到k8s，目前已经开始向register继承方向延展，如stackdriver_exporter已经使用该模式并且已经统一了struct的实现方法，但是prometheusexporter和k8sexporter还没完成演变，目前还是通过package直接调用的方式使用的。\n\n这个模块就不进行代码级别解析了，整体是非常简单的。\n\n# 6 healthchecker\n目录结构：\n```shell\n├── health_checker.go\n├── health_checker_darwin.go\n├── health_checker_linux.go\n├── health_checker_test.go\n├── health_checker_windows.go\n└── types\n├── types.go\n├── types_test.go\n├── types_unix.go\n└── types_windows.go\n```\n\n6.1 healthChecker定义\npkg/healthchecker/health_checker.go:31\n```shell\ntype healthChecker struct {\n  component       string\n  service         string\n  enableRepair    bool\n  healthCheckFunc func () (bool, error)\n  // The repair is \"best-effort\" and ignores the error from the underlying actions.\n  // The bash commands to kill the process will fail if the service is down and hence ignore.\n  repairFunc         func ()\n  uptimeFunc         func () (time.Duration, error)\n  crictlPath         string\n  healthCheckTimeout time.Duration\n  coolDownTime       time.Duration\n  loopBackTime       time.Duration\n  logPatternsToCheck map[string]int\n}\n```\n\n字段含义：\n- component：被检查的 Kubernetes 组件名称（如 Kubelet、Kube Proxy 等）。\n- service：服务名称或标识符。\n- enableRepair：布尔值，指示是否在发现服务不健康时尝试修复它。\n- healthCheckFunc：一个函数，用于执行健康检查，返回健康状态和可能的错误。\n- repairFunc：一个函数，用于尝试修复不健康的服务。\n- uptimeFunc：一个函数，用于获取服务的运行时间。\n- crictlPath：指向 CRI 工具的路径。\n- healthCheckTimeout：健康检查的超时设置。\n- coolDownTime：在尝试修复之前要求服务必须正常运行的时间。\n- loopBackTime：用于回溯检查日志的时间段。\n- logPatternsToCheck：一个映射，包含要检查的日志模式及其出现次数阈值。\n\n## 6.2 NewHealthChecker\npkg/healthchecker/health_checker.go:48\n```shell\n// NewHealthChecker returns a new health checker configured with the given options.\nfunc NewHealthChecker(hco *options.HealthCheckerOptions) (types.HealthChecker, error) {\n  hc := &healthChecker{\n    component:          hco.Component,\n    enableRepair:       hco.EnableRepair,\n    crictlPath:         hco.CriCtlPath,\n    healthCheckTimeout: hco.HealthCheckTimeout,\n    coolDownTime:       hco.CoolDownTime,\n    service:            hco.Service,\n    loopBackTime:       hco.LoopBackTime,\n    logPatternsToCheck: hco.LogPatterns.GetLogPatternCountMap(),\n  }\n  hc.healthCheckFunc = getHealthCheckFunc(hco)\n  hc.repairFunc = getRepairFunc(hco)\n  hc.uptimeFunc = getUptimeFunc(hco.Service)\n  return hc, nil\n}\n```\nNewHealthChecker会根据输入的options.HealthCheckerOptions来进行types.HealthChecker的构造。\n\n## 6.3 CheckHealth\npkg/healthchecker/health_checker.go:67\n```shell\n// CheckHealth checks for the health of the component and tries to repair if enabled.\n// Returns true if healthy, false otherwise.\nfunc (hc *healthChecker) CheckHealth() (bool, error) {\n        healthy, err := hc.healthCheckFunc()\n        if err != nil {\n                return healthy, err\n        }\n        logPatternHealthy, err := logPatternHealthCheck(hc.service, hc.loopBackTime, hc.logPatternsToCheck)\n        if err != nil {\n                return logPatternHealthy, err\n        }\n        if healthy && logPatternHealthy {\n                return true, nil\n        }\n\n        // The service is unhealthy.\n        // Attempt repair based on flag.\n        if hc.enableRepair {\n                // repair if the service has been up for the cool down period.\n                uptime, err := hc.uptimeFunc()\n                if err != nil {\n                        klog.Infof(\"error in getting uptime for %v: %v\\n\", hc.component, err)\n                        return false, nil\n                }\n                klog.Infof(\"%v is unhealthy, component uptime: %v\\n\", hc.component, uptime)\n                if uptime > hc.coolDownTime {\n                        klog.Infof(\"%v cooldown period of %v exceeded, repairing\", hc.component, hc.coolDownTime)\n                        hc.repairFunc()\n                }\n        }\n        return false, nil\n}\n```\n\n检查组件的健康状态，返回是否健康，并在不健康的情况下尝试修复。\n\n逻辑：\n- 调用 healthCheckFunc 执行健康检查。\n- 使用 logPatternHealthCheck 检查日志模式。\n- 如果组件不健康且启用了修复，检查组件的运行时间并调用修复函数。\n\n## 6.4 logPatternHealthCheck\npkg/healthchecker/health_checker.go:100\n```shell\n// logPatternHealthCheck checks for the provided logPattern occurrences in the service logs.\n// Returns true if the pattern is empty or does not exist logThresholdCount times since start of service, false otherwise.\nfunc logPatternHealthCheck(service string, loopBackTime time.Duration, logPatternsToCheck map[string]int) (bool, error) {\n        if len(logPatternsToCheck) == 0 {\n                return true, nil\n        }\n        uptimeFunc := getUptimeFunc(service)\n        klog.Infof(\"Getting uptime for service: %v\\n\", service)\n        uptime, err := uptimeFunc()\n        if err != nil {\n                klog.Warningf(\"Failed to get the uptime: %+v\", err)\n                return true, err\n        }\n\n        logStartTime := time.Now().Add(-uptime).Format(types.LogParsingTimeLayout)\n        if loopBackTime > 0 && uptime > loopBackTime {\n                logStartTime = time.Now().Add(-loopBackTime).Format(types.LogParsingTimeLayout)\n        }\n        for pattern, count := range logPatternsToCheck {\n                healthy, err := checkForPattern(service, logStartTime, pattern, count)\n                if err != nil || !healthy {\n                        return healthy, err\n                }\n        }\n        return true, nil\n}\n```\n检查服务日志中指定模式的出现次数。\n\n参数：\n- service：被监控的服务名称。\n- loopBackTime：用于确定要检查的日志时间范围。\n- logPatternsToCheck：要检查的日志模式及其出现的阈值。\n  返回值：布尔值表示健康状态和可能的错误。\n\n## 6.5 healthCheckEndpointOKFunc\npkg/healthchecker/health_checker.go:126\n```shell\n// healthCheckEndpointOKFunc returns a function to check the status of an http endpoint\nfunc healthCheckEndpointOKFunc(endpoint string, timeout time.Duration) func() (bool, error) {\n        return func() (bool, error) {\n                httpClient := http.Client{Timeout: timeout}\n                response, err := httpClient.Get(endpoint)\n                if err != nil || response.StatusCode != http.StatusOK {\n                        return false, nil\n                }\n                return true, nil\n        }\n}\n```\n\n返回一个函数，该函数检查给定 HTTP 端点的健康状态。\n\n参数：端点 URI 和超时时间。\n\n返回值：健康检查函数。\n\n## 6.6 getHealthCheckFunc\npkg/healthchecker/health_checker.go:138\n```shell\n// getHealthCheckFunc returns the health check function based on the component.\nfunc getHealthCheckFunc(hco *options.HealthCheckerOptions) func() (bool, error) {\n        switch hco.Component {\n        case types.KubeletComponent:\n                return healthCheckEndpointOKFunc(types.KubeletHealthCheckEndpoint(), hco.HealthCheckTimeout)\n        case types.KubeProxyComponent:\n                return healthCheckEndpointOKFunc(types.KubeProxyHealthCheckEndpoint(), hco.HealthCheckTimeout)\n        case types.DockerComponent:\n                return func() (bool, error) {\n                        if _, err := execCommand(hco.HealthCheckTimeout, getDockerPath(), \"ps\"); err != nil {\n                                return false, nil\n                        }\n                        return true, nil\n                }\n        case types.CRIComponent:\n                return func() (bool, error) {\n                        _, err := execCommand(\n                                hco.HealthCheckTimeout,\n                                hco.CriCtlPath,\n                                \"--timeout=\"+hco.CriTimeout.String(),\n                                \"--runtime-endpoint=\"+hco.CriSocketPath,\n                                \"pods\",\n                                \"--latest\",\n                        )\n                        if err != nil {\n                                return false, nil\n                        }\n                        return true, nil\n                }\n        default:\n                klog.Warningf(\"Unsupported component: %v\", hco.Component)\n        }\n\n        return nil\n}\n```\n\n根据组件类型返回相应的健康检查方法。\n\n参数：健康检查选项。\n\n返回值：执行健康检查的函数。\n\n## 6.7 execCommand\npkg/healthchecker/health_checker.go:174\n```shell\n// execCommand executes the bash command and returns the (output, error) from command, error if timeout occurs.\nfunc execCommand(timeout time.Duration, command string, args ...string) (string, error) {\n        ctx, cancel := context.WithTimeout(context.Background(), timeout)\n        defer cancel()\n        cmd := exec.CommandContext(ctx, command, args...)\n        out, err := cmd.CombinedOutput()\n        if err != nil {\n                klog.Infof(\"command %v failed: %v, %s\\n\", cmd, err, string(out))\n                return \"\", err\n        }\n\n        return strings.TrimSuffix(string(out), \"\\n\"), nil\n}\n```\n执行给定的命令，并在指定的超时时间内返回输出和错误。\n\n参数：超时时间、命令和参数。\n\n返回值：命令的输出和错误信息。\n\n## 6.8 getUptimeFunc\npkg/healthchecker/health_checker_linux.go:32\n```shell\n// getUptimeFunc returns the time for which the given service has been running.\nfunc getUptimeFunc(service string) func() (time.Duration, error) {\n        return func() (time.Duration, error) {\n                // Using InactiveExitTimestamp to capture the exact time when systemd tried starting the service. The service will\n                // transition from inactive -> activating and the timestamp is captured.\n                // Source : https://www.freedesktop.org/wiki/Software/systemd/dbus/\n                // Using ActiveEnterTimestamp resulted in race condition where the service was repeatedly killed by plugin when\n                // RestartSec of systemd and invoke interval of plugin got in sync. The service was repeatedly killed in\n                // activating state and hence ActiveEnterTimestamp was never updated.\n                out, err := execCommand(types.CmdTimeout, \"systemctl\", \"show\", service, \"--property=InactiveExitTimestamp\")\n\n                if err != nil {\n                        return time.Duration(0), err\n                }\n                val := strings.Split(out, \"=\")\n                if len(val) < 2 {\n                        return time.Duration(0), errors.New(\"could not parse the service uptime time correctly\")\n                }\n                t, err := time.Parse(types.UptimeTimeLayout, val[1])\n                if err != nil {\n                        return time.Duration(0), err\n                }\n                return time.Since(t), nil\n        }\n}\n```\n\n返回一个函数，计算指定服务的运行时间。\n\n参数：\n- service string - 被监控服务的名称。\n- 返回值：返回一个函数，该函数返回服务运行的持续时间或错误。\n\n- 实现细节\n- 使用 systemctl show 命令获取 InactiveExitTimestamp，这是服务从 inactive 状态转换时的时间戳。\n- 解析命令的输出，将时间戳转换为 time.Time 类型，并计算自该时间以来的持续时间。\n- 此方法可用于获取服务的当前运行时间，以便评估是否在修复之前达到冷却时间。\n\n## 6.9 getRepairFunc\npkg/healthchecker/health_checker_linux.go:58\n```shell\n// getRepairFunc returns the repair function based on the component.\nfunc getRepairFunc(hco *options.HealthCheckerOptions) func() {\n        // Use `systemctl kill` instead of `systemctl restart` for the repair function.\n        // We start to rely on the kernel message difference for the two commands to\n        // indicate if the component restart is due to an administrative plan (restart)\n        // or a system issue that needs repair (kill).\n        // See https://github.com/kubernetes/node-problem-detector/issues/847.\n        switch hco.Component {\n        case types.DockerComponent:\n                // Use \"docker ps\" for docker health check. Not using crictl for docker to remove\n                // dependency on the kubelet.\n                return func() {\n                        execCommand(types.CmdTimeout, \"pkill\", \"-SIGUSR1\", \"dockerd\")\n                        execCommand(types.CmdTimeout, \"systemctl\", \"kill\", \"--kill-who=main\", hco.Service)\n                }\n        default:\n                // Just kill the service for all other components\n                return func() {\n                        execCommand(types.CmdTimeout, \"systemctl\", \"kill\", \"--kill-who=main\", hco.Service)\n                }\n        }\n}\n```\n\n根据服务的组件类型返回一个演示修复功能的方法。\n\n参数：\n- hco *options.HealthCheckerOptions - 健康检查的选项。\n- 返回值：返回一个函数，该函数执行相应修复操作。\n\n实现细节\n- 对于 Docker 组件，通过 pkill 发送 SIGUSR1 信号以更优雅地停止 Docker 守护进程。\n- 对于其他组件，调用 systemctl kill 命令终止主要进程。\n- 这种设计使得修复功能能够根据不同的组件采取合适的处理方式。\n\n## 6.10 checkForPattern\npkg/healthchecker/health_checker_linux.go:82\n```shell\n// checkForPattern returns (true, nil) if logPattern occurs less than logCountThreshold number of times since last\n// service restart. (false, nil) otherwise.\nfunc checkForPattern(service, logStartTime, logPattern string, logCountThreshold int) (bool, error) {\n        out, err := execCommand(types.CmdTimeout, \"/bin/sh\", \"-c\",\n                // Query service logs since the logStartTime\n                `journalctl --unit \"`+service+`\" --since \"`+logStartTime+\n                        // Grep the pattern\n                        `\" | grep -i \"`+logPattern+\n                        // Get the count of occurrences\n                        `\" | wc -l`)\n        if err != nil {\n                return true, err\n        }\n        occurrences, err := strconv.Atoi(out)\n        if err != nil {\n                return true, err\n        }\n        if occurrences >= logCountThreshold {\n                klog.Infof(\"%s failed log pattern check, %s occurrences: %v\", service, logPattern, occurrences)\n                return false, nil\n        }\n        return true, nil\n}\n```\n检查指定服务的日志中是否包含特定模式，并返回其出现次数是否达到阈值。\n\n参数：\n- service：被监控服务的名称。\n- logStartTime：起始时间，用于过滤日志。\n- logPattern：需要检查的日志模式。\n- logCountThreshold：模式出现的最大次数阈值。\n- 返回值：返回一个布尔值，指示是否健康，以及可能的错误。\n\n实现细节\n- 使用 journalctl 命令获取从指定时间开始的服务日志，并通过 grep 过滤符合条件的日志模式。\n- 使用 wc -l 计算匹配的行数。\n- 如果日志模式出现的次数大于阈值，记录相关信息并返回健康检查失败的结果。\n\n# 7 logcounter\n## 7.1 logCounter定义\npkg/logcounter/log_counter.go:42\n```shell\ntype logCounter struct {\n        logCh         <-chan *systemtypes.Log\n        buffer        systemlogmonitor.LogBuffer\n        pattern       string\n        revertPattern string\n        clock         clock.Clock\n}\n```\n字段解释：\n- logCh：只读通道，用于接收系统日志。\n- buffer：存储日志的缓冲区，以便分析和计数。\n- pattern：处理日志时使用的匹配模式。\n- revertPattern：可用来反向计数的模式。\n- clock：用于获取当前时间的时钟，支持模拟时钟功能。\n\n## 7.2 NewJournaldLogCounter\npkg/logcounter/log_counter.go:50\n```shell\nfunc NewJournaldLogCounter(options *options.LogCounterOptions) (types.LogCounter, error) {\n        watcher := journald.NewJournaldWatcher(watchertypes.WatcherConfig{\n                Plugin:       \"journald\",\n                PluginConfig: map[string]string{journaldSourceKey: options.JournaldSource},\n                LogPath:      options.LogPath,\n                Lookback:     options.Lookback,\n                Delay:        options.Delay,\n        })\n        logCh, err := watcher.Watch()\n        if err != nil {\n                return nil, fmt.Errorf(\"error watching journald: %v\", err)\n        }\n        return &logCounter{\n                logCh:         logCh,\n                buffer:        systemlogmonitor.NewLogBuffer(bufferSize),\n                pattern:       options.Pattern,\n                revertPattern: options.RevertPattern,\n                clock:         clock.RealClock{},\n        }, nil\n}\n```\n\n初始化并返回一个新的 logCounter 实例。\n\n参数：\n- options *options.LogCounterOptions - 包含配置信息（例如日志源、路径、模式等）。\n- 返回值：返回一个实现了 types.LogCounter 接口的 logCounter 实例。\n\n实现细节\n- 创建一个 journald 日志观察者实例来监控指定的日志路径。\n- 如果监视操作成功，建立日志通道并创建 logCounter 的实例，设置缓冲区、匹配模式等。\n\n## 7.3 Count \npkg/logcounter/log_counter.go:71\n```shell\nfunc (e *logCounter) Count() (count int, err error) {\n        start := e.clock.Now()\n        for {\n                select {\n                case log, ok := <-e.logCh:\n                        if !ok {\n                                err = fmt.Errorf(\"log channel closed unexpectedly\")\n                                return\n                        }\n                        if start.Before(log.Timestamp) {\n                                return\n                        }\n                        e.buffer.Push(log)\n                        if len(e.buffer.Match(e.pattern)) != 0 {\n                                count++\n                        }\n                        if e.revertPattern != \"\" && len(e.buffer.Match(e.revertPattern)) != 0 {\n                                count--\n                        }\n                case <-e.clock.After(timeout):\n                        return\n                }\n        }\n}\n```\n统计在运行期间匹配的日志条目数量。\n\n返回值：返回符合条件的日志计数及任何潜在的错误。\n\n实现细节\n- 使用当前时间 (start) 记录开始时刻。\n- 在无限循环中，使用 select 语句监听日志通道和超时事件。\n- 如果从 logCh 读取到日志：\n  - 检查通道是否已经关闭。\n  - 如果读取到的日志时间戳晚于开始时间，则停止计数。\n  - 将日志条目压入缓冲区。\n  - 检查当前日志是否与模式匹配，如果匹配则计数加一；如果匹配反向模式，则计数减一。\n  - 如果在超时时间内没有收到新日志，结束循环并返回计数。"},{"title":"NPD介绍","url":"/2025/07/devops/npd/npd-jie-shao/","content":"\n# 1 简介\n官方文档库地址：https://github.com/kubernetes/node-problem-detector\n\nnode-problem-detector 旨在使集群管理堆栈中的上游层能够看到各种节点问题。\n\n它是一个在每个节点上运行的守护进程，检测节点问题并将其报告给apiserver。\n\nnode-problem-detector 可以作为 DaemonSet 运行，也可以独立运行。\n\n现在它作为 GKE集群中默认启用的Kubernetes Addon运行。它也作为AKS Linux Extension的一部分在 AKS 中默认启用。\n\n## 1.2 背景\n大量节点问题可能会影响节点上运行的 pod，例如：\n- 基础设施守护进程问题：ntp 服务关闭；\n- 硬件问题：CPU、内存或磁盘损坏；\n- 内核问题：内核死锁、文件系统损坏；\n- 容器运行时问题：运行时守护进程无响应；\n- …\n\n目前，这些问题对于集群管理堆栈中的上游层是不可见的，因此 Kubernetes 将继续将 pod 调度到坏节点。\n为了解决这个问题，我们引入了这个新的守护进程node-problem-detector来从各种守护进程收集节点问题，并将它们提供给上游层。一旦上游层能够看到这些问题，我们就可以讨论 [remedy systems](https://github.com/kubernetes/node-problem-detector?tab=readme-ov-file#remedy-systems)了。\n\n## 1.3 Problem API\nnode-problem-detector 使用Event并向NodeConditionapiserver 报告问题。\n- NodeCondition：导致节点无法用于 pod 的永久性问题应报告为NodeCondition。\n- Event：对 pod 影响有限但具有参考意义的临时问题应报告为Event。\n\n## 1.4 Problem Daemon\n问题守护进程是 node-problem-detector 的一个子守护进程。它监视特定类型的节点问题并将其报告给 node-problem-detector。\n守护进程可能是：\n- 专为 Kubernetes 专用用例设计的微型守护进程。\n- 与节点问题检测器集成的现有节点健康监控守护进程。\n\n目前，问题守护进程在 node-problem-detector 二进制文件中以 goroutine 的形式运行。\n\n未来，我们会将 node-problem-detector和问题守护进程分离到不同的容器中，并使用 pod 规范将它们组合在一起。\n\n每种类型的问题守护进程都可以通过设置相应的构建标签在编译时禁用。\n\n如果在编译时禁用它们，则它们的所有构建依赖项、全局变量和后台goroutine 都将从编译的可执行文件中剔除。\n\n支持的守护进程列表：\n\n|  问题守护进程类型 |  节点状态 |  解释 |  配置 |  禁用构建标签 |\n|---|---|---|---|---|\n|  [SystemLogMonitor](https://github.com/kubernetes/node-problem-detector/tree/master/pkg/systemlogmonitor) |  KernelDeadlock ReadonlyFilesystem FrequentKubeletRestart FrequentDockerRestart FrequentContainerdRestart | 系统日志监视器监视系统日志并根据预定义的规则报告问题和指标。  |  [filelog](https://github.com/kubernetes/node-problem-detector/blob/master/config/kernel-monitor-filelog.json), [kmsg](https://github.com/kubernetes/node-problem-detector/blob/master/config/kernel-monitor.json), [kernel](https://github.com/kubernetes/node-problem-detector/blob/master/config/kernel-monitor-counter.json), [abrt](https://github.com/kubernetes/node-problem-detector/blob/master/config/systemd-monitor-counter.json), [systemd](https://github.com/kubernetes/node-problem-detector/blob/master/config/systemd-monitor-counter.json) | disable_system_log_monitor  |\n|  [SystemStatsMonitor](https://github.com/kubernetes/node-problem-detector/tree/master/pkg/systemstatsmonitor) | None(Could be added in the future)  |  节点问题检测器的系统统计监视器，用于收集各种与健康相关的系统统计信息作为指标。请参阅[此处](https://docs.google.com/document/d/1SeaUz6kBavI283Dq8GBpoEUDrHA2a795xtw0OvjM568/edit)的提案。 | [system-stats-monitor](https://github.com/kubernetes/node-problem-detector/blob/master/config/system-stats-monitor.json)  |  disable_system_stats_monitor |\n| [CustomPluginMonitor](https://github.com/kubernetes/node-problem-detector/tree/master/pkg/custompluginmonitor)  |  On-demand(According to users configuration), existing example: NTPProblem | 一个自定义的 node-problem-detector 插件监控器，用于调用和检查各种节点问题，并使用用户定义的检查脚本。请参阅[此处](https://docs.google.com/document/d/1jK_5YloSYtboj-DtfjmYKxfNnUxCAvohLnsH5aGCAYQ/edit#)的提案。  |  [example](https://github.com/kubernetes/node-problem-detector/blob/4ad49bbd84b8ced45ac825eac01ec93d9235935e/config/custom-plugin-monitor.json) | disable_custom_plugin_monitor  |\n|[HealthChecker](https://github.com/kubernetes/node-problem-detector/tree/master/pkg/healthchecker)|  KubeletUnhealthy ContainerRuntimeUnhealthy | 节点问题检测器的健康检查器用于检查 kubelet 和容器运行时的健康状况。  |   |   |\n\n## 1.5 Exporter\nexporter 是 node-problem-detector 的一个组件。\n\n它向某些后端报告节点问题 和/或 指标。\n\n其中一些可以在编译时使用构建标记禁用。支持的导出器列表：\n\n| Exporter  | 解释  | 禁用构建标签  |\n|---|---|---|\n|  Kubernetes exporter | Kubernetes 导出器向 Kubernetes API 服务器报告节点问题：临时问题报告为事件，永久问题报告为节点状况。  |   |\n| Prometheus exporter  |  Prometheus 导出器将节点问题和指标本地报告为 Prometheus 指标 |   |\n|  [Stackdriver exporter](https://github.com/kubernetes/node-problem-detector/blob/master/config/exporter/stackdriver-exporter.json) | Stackdriver 导出器向 Stackdriver Monitoring API 报告节点问题和指标。  | disable_stackdriver_exporter  |\n\n## 1.6 用法\n### 1.6.1 标志\n- –version：打印节点问题检测器的当前版本。\n- –hostname-override：node-problem-detector 用于更新状态和发出事件的自定义节点名称。node-problem-detector 首先从hostname-override获取节点名称，然后从NODE_NAME环境变量 获取，最后返回到os.Hostname。\n\n### 1.6.2 对于系统日志监控\n- –config.system-log-monitor：系统日志监视器配置文件路径列表，以逗号分隔，例如 config/kernel-monitor.json。节点问题检测器将为每个配置启动单独的日志监视器。您可以使用不同的日志监视器来监视不同的系统日志。\n\n### 1.6.3 对于系统统计监控\n- –config.system-stats-monitor：系统统计监控配置文件的路径列表，以逗号分隔，例如 [config/system-stats-monitor.json](https://github.com/kubernetes/node-problem-detector/blob/master/config/system-stats-monitor.json)。节点问题检测器将为每个配置启动一个单独的系统统计监控器。您可以使用不同的系统统计监控器来监控与问题相关的不同系统统计信息。\n\n### 1.6.4 对于自定义插件监视器\n- –config.custom-plugin-monitor：自定义插件监控配置文件路径列表，以逗号分隔，例如 [config/custom-plugin-monitor.json](https://github.com/kubernetes/node-problem-detector/blob/master/config/custom-plugin-monitor.json)。节点问题检测器将为每个配置启动一个单独的自定义插件监控。您可以使用不同的自定义插件监控来监控不同的节点问题。\n\n### 1.6.5 对于健康检查者\n- –enable-k8s-exporter：启用向 Kubernetes API 服务器报告，默认为true。\n- –apiserver-override：用于自定义 node-problem-detector 如何连接 apiserver 的 URI 参数。如果–enable-k8s-exporter是，则忽略此参数。格式与[Heapster](https://github.com/kubernetes/heapster) 的标志false相同。例如，要无需身份验证即可运行，请使用以下配置： [source](https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes)\n\n```shell\nhttp://APISERVER_IP:APISERVER_PORT?inClusterConfig=false\n```\n\n请参阅[heapster 文档](https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes)\n以获取可用选项的完整列表。\n- –address：绑定节点问题检测服务器的地址。\n- –port：绑定节点问题检测服务器的端口。使用 0 表示禁用。\n\n### 1.6.6 对于 Prometheus exporter\n- –prometheus-address：绑定Prometheus抓取端点的地址，默认为127.0.0.1。\n- –prometheus-port：绑定 Prometheus 抓取端点的端口，默认为 20257。使用 0 表示禁用。\n\n### 1.6.7 对于 Stackdriver exporter\n\n- –exporter.stackdriver：Stackdriver 导出器配置文件的路径，例如config/exporter/stackdriver-exporter.json，默认为空字符串。设置为空字符串即可禁用。\n\n### 1.6.8 已弃用的标志\n- –system-log-monitors：系统日志监控配置文件的路径列表，以逗号分隔。此选项已弃用，由–config.system-log-monitor替代，并将被删除。如果同时设置–system-log-monitors和 –config.system-log-monitor ， NPD 将崩溃。\n- –custom-plugin-monitors：自定义插件监控配置文件的路径列表，以逗号分隔。此选项已弃用，由–config.custom-plugin-monitor替代，并将被删除。如果同时设置–custom-plugin-monitors和 –config.custom-plugin-monitor，NPD 将崩溃。\n\n## 1.7 构建镜像\n- 安装libsystemdARM GCC 工具链的开发依赖项\n    - Debian / Ubuntu：apt install libsystemd-dev gcc-aarch64-linux-gnu\n- git clone git@github.com:kubernetes/node-problem-detector.git\n- 在顶层目录中运行make。它将：\n    - 构建二进制文件。\n    - 构建docker镜像。二进制文件和config/被复制到docker镜像中。\n      \n如果您不需要某些类别的问题守护进程，您可以选择在编译时禁用它们。这是保持 node-problem-detector运行时紧凑且没有不必要代码（例如全局变量、goroutines 等）的最佳方法。\n\n您可以通过BUILD_TAGS在运行之前设置环境变量来实现这一点make。例如：\n```shell\nBUILD_TAGS=\"disable_custom_plugin_monitor disable_system_stats_monitor\" make\n```\n\n上述命令将在不使用[自定义插件监视器](https://github.com/kubernetes/node-problem-detector/tree/master/pkg/custompluginmonitor)和[系统统计监视器](https://github.com/kubernetes/node-problem-detector/tree/master/pkg/systemstatsmonitor)的情况下编译node-problem-detector 。查看[问题守护进程](https://github.com/kubernetes/node-problem-detector#problem-daemon)部分，了解如何在编译时禁用每个问题守护进程。\n\n\n## 1.8 推送镜像\n`make push`将 docker 镜像上传到注册表。默认情况下，镜像将上传到 `staging-k8s.gcr.io`。可以轻松修改`Makefile`以将镜像推送到另一个注册表。\n\n## 1.9 安装\n将 node-problem-detector\n安装到集群中的最简单方法是使用[Helm 图表](https://github.com/deliveryhero/helm-charts/tree/master/stable/node-problem-detector)：\n```shell\nhelm repo add deliveryhero https://charts.deliveryhero.io/\nhelm install --generate-name deliveryhero/node-problem-detector\n```\n\n或者，手动安装 node-problem-detector：\n\n编辑[node-problem-detector.yaml](https://github.com/kubernetes/node-problem-detector/blob/master/deployment/node-problem-detector.yaml)以适应您的环境。将`log`卷设置为您的系统日志目录（由 SystemLogMonitor 使用）。您可以使用 ConfigMap 覆盖`config` pod 内的目录。\n\n编辑[node-problem-detector-config.yaml](https://github.com/kubernetes/node-problem-detector/blob/master/deployment/node-problem-detector-config.yaml)来配置 node-problem-detector。\n- 编辑[rbac.yaml](https://github.com/kubernetes/node-problem-detector/blob/master/deployment/rbac.yaml)以适合您的环境。\n- 使用 创建 ServiceAccount 和 ClusterRoleBinding `kubectl create -f rbac.yaml`。\n- 使用 创建 ConfigMap `kubectl create -f node-problem-detector-config.yaml`。\n- 使用 创建 DaemonSet `kubectl create -f node-problem-detector.yaml`。\n\n## 1.10 开始独立运行\n要独立运行 node-problem-detector，您应该设置inClusterConfig为false，并且教 node-problem-detector 如何 访问 apiserver，也就是apiserver-override。\n\n要使用不安全的 apiserver 连接独立运行 node-problem-detector：\n```shell\nnode-problem-detector --apiserver-override=http://APISERVER_IP:APISERVER_INSECURE_PORT?inClusterConfig=false\n```\n\n更多场景请见[此处](https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes)\n\n## 1.11 试用\n您可以在正在运行的集群中尝试使用 node-problem-detector，方法是将消息注入到 node-problem-detector 正在监视的日志中。\n\n例如，假设 node-problem-detector 正在使用KernelMonitor。\n\n在您的机器上运行`kubectl get events -w`。在节点上运行`sudo sh -c \"echo 'kernel: BUG: unable to handle kernel NULL pointer dereference at TESTING' >> /dev/kmsg\"`。然后您应该会看到该KernelOopss事件。\n\n添加新规则或开发节点问题检测器时，在独立模式下在本地工作站上进行测试可能更容易。\n\n对于 API 服务器，一种简单的方法是使用kubectl proxy正在运行的集群的 API 服务器在本地可用。\n\n您会收到一些错误，因为 API 服务器无法识别您的本地工作站。但无论如何，您仍然应该能够测试您的新规则。\n\n例如，测试KernelMonitor规则：\n- `make`（本地构建node-problem-detector）\n- `kubectl proxy --port=8080`（使正在运行的集群的 API 服务器在本地可用）\n- 将KernelMonitor更新到您的本地内核日志目录`logPath`。例如，在某些 Linux 系统上，它是`/run/log/journal`, 而不是`/var/log/journal`。\n- `./bin/node-problem-detector --logtostderr --apiserver-override=http://127.0.0.1:8080?inClusterConfig=false --config.system-log-monitor=config/kernel-monitor.json --config.system-stats-monitor=config/system-stats-monitor.json --port=20256 --prometheus-port=2025`（或指向任何 API 服务器地址：端口和 Prometheus 端口）\n- `sudo sh -c \"echo 'kernel: BUG: unable to handle kernel NULL pointer dereference at TESTING' >> /dev/kmsg\"`\n- 您可以`KernelOops`在节点问题检测器日志中看到事件。\n- `sudo sh -c \"echo 'kernel: INFO: task docker:20744 blocked for more than 120 seconds.' >> /dev/kmsg\"`\n- 您可以在node-problem-detector log看DockerHung event和状态\n- 您可以在[http://127.0.0.1:20256/conditions查看DockerHung状态](http://127.0.0.1:20256/conditions查看%60DockerHung%60状态)\n- 您可以在[http://127.0.0.1:20257/metrics](http://127.0.0.1:20257/metrics上查看)上查看 Prometheus 格式的磁盘相关系统指标。\n  \n注： 您可以在[test/kernel_log_generator/problems](https://github.com/kubernetes/node-problem-detector/tree/master/test/kernel_log_generator/problems)下查看更多规则示例。\n- 对于[KernelMonitor](https://github.com/kubernetes/node-problem-detector/blob/master/config/kernel-monitor.json)\n  消息注入，所有消息都应该有kernel: 前缀（另请注意，后面有一个空格:）；或者使用[generator.sh](https://github.com/kubernetes/node-problem-detector/blob/master/test/kernel_log_generator/generator.sh)。\n- 要将其他日志（如 systemd 日志）注入 journald，请使用`echo 'Some systemd message' | systemd-cat -t systemd`。\n\n## 1.12 Remedy Systems\n\nRemedy Systems是一个或多个旨在尝试解决node-problem-detector检测到的问题的过程。\n\nRemedy Systems会观察node-problem-detector发出的事件 和/或 节点状况，并采取措施使 Kubernetes 集群恢复健康状态。\n\n补救系统有以下几种：\n- [Draino](https://github.com/planetlabs/draino) 根据标签和节点条件自动排空 Kubernetes节点。与所有提供的标签和任何提供的节点条件匹配的节点将被阻止立即接受新 pod（也称为“封锁”），并在可配置的时间后[排空](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)。Draino可以与 [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)结合使用，以自动终止排空的节点。请参阅 [此问题](https://github.com/kubernetes/node-problem-detector/issues/199) ，了解 Draino的示例生产用例。\n- [Descheduler](https://github.com/kubernetes-sigs/descheduler) 取消调度策略RemovePodsViolatingNodeTaints会驱逐节点上违反NoSchedule污染的Pod。必须启用k8s调度程序的TaintNodesByCondition功能。\n- [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) 可用于自动终止耗尽的节点。\n- [mediK8S](https://github.com/medik8s) 是一个基于[Node Health Check Operator (NHC)](https://github.com/medik8s/node-healthcheck-operator)构建的自动修复系统的总体项目，该系统监控节点状况并使用修复 API将修复委托给外部修复程序。\n- [Poison-Pill](https://github.com/medik8s/poison-pill)是一个修复程序，它将重新启动节点并确保所有有状态的工作负载都得到重新安排。如果集群具有足够的健康容量，NHC支持有条件地进行修复，或者手动暂停任何操作以最大限度地减少集群中断。\n- [Cluster API](https://cluster-api.sigs.k8s.io/) 的[MachineHealthCheck](https://cluster-api.sigs.k8s.io/developer/architecture/controllers/machine-health-check)负责修复不健康的机器。\n\n\n\n\n\n\n"},{"title":"BKCI 简介","url":"/2025/07/devops/blueking/bkci-jian-jie/","content":"\n# 文档\n\nBK-CI官方文档：https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/intro/README.md\ngithub仓库地址：https://github.com/TencentBlueKing/bk-ci\n\n\n# 导航\n| 🐤 了解基本概念  | 👉 使用 BKCI  | 🚀 部署 BKCI  |\n|---|---|---|\n| [BKCI 是什么？](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/intro/bkci.md)  | [创建你的第一条流水线](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Quickstarts/Create-your-first-pipeline.md)  | [BKCI 硬件规格指南](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Setup/system-requirements/hardware.md)  |\n|  [BKCI 组件](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/intro/terminology/components.md) |  [关联你的第一个代码库](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Quickstarts/Link-your-first-repo.md) | [BKCI 系统要求](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Setup/system-requirements/system.md)  |\n|  [快速熟悉流水线](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/intro/terminology/Learn-pipeline-in-5min.md) | [为你的Git工程开启CI](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Quickstarts/Enable-ci.md)  |   |\n|  [术语解释](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/intro/terminology/Learn-pipeline-in-5min.md) | [示例](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Quickstarts/Case/Examples/vars-usage.md)  |   |\n|   | [API接口](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/rest-api/read-before-use.md)  |   |\n\n\n| 📔 产品功能  | 🏪 研发商店  |\n|---|---|\n|  [流水线](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Pipeline/pipeline-list.md) |  [浏览研发商店](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Store/home.md) |\n|  [控制台](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Console/Console.md) | [开发一个流水线插件](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Store/start-new-task.md)  |\n|  [凭证管理](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Ticket/ticket.md) |  [在 BKCI 里使用商店插件](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Store/upload-new-task.md) |\n|  [构建资源](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Pools/host-to-bkci.md) |   |\n\n\n# 相关文档\n蓝鲸学习社区：https://bk.tencent.com/s-mart/communities\n蓝鲸官方文档：https://bk.tencent.com/docs/\n\n**蓝鲸体系**\n蓝鲸简介：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/intro.md\n核心优势：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/advantages.md\n体系架构：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/solution.md\nCI领域：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/ci_intro.md\nCD领域：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/cd_intro.md\nCO领域：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/co_intro.md","tags":["devops","blueking"],"categories":["blueking"]},{"title":"BK-CI插件开发指引","url":"/2025/07/devops/blueking/bk-ci-cha-jian-kai-fa-zhi-yin/","content":"\n# 写在前面\n\n> 开发插件前，先进入插件工作台初始化一个插件，确定插件在平台中的唯一标识\n\n# 工作台\n可以在这里进行新增/发布/下架等管理插件的操作\n\n## 功能区介绍\n![img.png](img.png)\n\n1. 切换资源类型\n2. 新增插件\n3. 单个插件的管理入口\n4. 升级、下架、删除插件快捷入口\n5. 指引文档和插件 UI 调试工具入口\n\n## 新增插件\n![img_1.png](img_1.png)\n\n1. 标识\n- 插件在平台中的唯一标识，建议取和插件功能相关的可读性好的英文标识\n2. 调试项目\n- 插件发布过程中，可以在调试项目下将插件添加到流水线执行，对插件进行测试，保证插件功能满足预期。\n- 建议新增专用的插件调试项目，避免测试过程中影响到业务。\n3. 开发语言\n- 支持四种语言开发插件：\n    - Java（推荐）\n    - Python\n    - Golang\n    - Nodejs\n\n## 开发插件\n> 初始化好插件之后，可以开始开发插件\n\n- 根据开发语言参考对应的开发指引\n  - [Java 插件开发指引](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-guide/java.md)\n  - [Python 插件开发指引](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-guide/python.md)\n  - [Golang 插件开发指引](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-guide/golang.md)\n  - [Nodejs 插件开发指引](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-guide/nodejs.md)\n\n## 插件私有配置\n> 插件级别的敏感信息，如 token、用户名密码、IP、域名等，不建议直接提交到代码库，通过工作台私有配置界面管理\n\n![img_2.png](img_2.png)\n\n# Golang 插件开发\n## 插件开发框架说明\n> 插件最终打包成一个命令行可执行的命令即可，对开发框架无硬性要求 下边以 demo 插件为例示范\n\n## 示例插件代码工程的整体结构如下\n```shell\n|- <你的插件标识>\n    |- cmd\n        |- application\n            |- main.go\n\n    |- hello\n        |- hello.go\n```\n\n## 如何开发插件：\n> 参考 [plugin-demo-golang](https://github.com/ci-plugins/plugin-demo-golang)\n\n- 建插件代码工程\n    - 插件代码建议企业下统一管理。 通用的开源插件可以联系蓝鲸官方放到 [TencentBlueKing](https://github.com/TencentBlueKing) 下，供更多用户使用\n- 实现插件功能\n- 规范：\n    - [插件开发规范](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-standard/plugin-specification.md)\n    - [插件配置规范](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-standard/plugin-config.md)\n- 插件前端不仅可以通过 task.json 进行标准化配置，也可以自定义开发：\n  - [自定义插件 UI 交互指引](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-standard/plugin-custom-ui.md)\n  - [插件输出规范](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-standard/plugin-output.md)\n  - [插件错误码规范](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-standard/plugin-error-code.md)\n  - [插件发布包规范](https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Developer/plugins/plugin-dev-standard/release.md)\n\n## Demo示例研读\n将 [plugin-demo-golang](https://github.com/ci-plugins/plugin-demo-golang) clone到本地。\n来看下项目结构：\n```shell\n.\n├── CONTRIBUTING.md\n├── LICENSE\n├── Makefile\n├── README.md\n├── go.mod\n├── go.sum\n├── i18n\n│   ├── message_en_US.properties\n│   └── message_zh_CN.properties\n├── main.go\n├── task.json\n└── translation\n    └── translation.go\n\n3 directories, 11 files\n```\n\n咦！和上面说的插件代码工程的整体架构不一样：\n```shell\n|- <你的插件标识>\n    |- cmd\n        |- application\n            |- main.go\n\n    |- hello\n        |- hello.go\n```\n\n不过，这不重要，只要插件最终能够打包成一个命令行可执行的命令即可。\n这里i18n是实现中英文国际化使用的，\n```shell\n├── i18n\n│   ├── message_en_US.properties\n│   └── message_zh_CN.properties\n```\n\n看下内容对比：\nmessage_en_US.properties：\n```shell\ninput.desc.label=desc\n100001=input param [{0}] invitated\n```\n\nmessage_zh_CN.properties\n```shell\ninput.desc.label=描述\n100001=输入参数[{0}]非法\n```\n\n那我们就可以猜测，这里是实现输入参数[{0}]非法这句话的中英文，其中[{0}]会使用具体的参数填充。\n然后i18ngenerator则是根据properties文件的配置，生成translation代码，如translation.go：\n```go\n// Code generated by \"i18ngenerator\"; DO NOT EDIT.\n\npackage translation\n\n// Translations\nvar Translations map[string][][]string = make(map[string][][]string)\n\nfunc init() {\n    Translations[\"en-US\"] = [][]string{\n       {\n          \"100001\",\n          \"input param [{0}] invitated\",\n       },\n       {\n          \"input.desc.label\",\n          \"desc\",\n       },\n    }\n    Translations[\"zh-CN\"] = [][]string{\n       {\n          \"100001\",\n          \"输入参数[{0}]非法\",\n       },\n       {\n          \"input.desc.label\",\n          \"描述\",\n       },\n    }\n}\n```\n\n该文件是由i18ngenerator自动生成的，不要自己改。\nmain函数内，实现了一个小的输出功能，简单看下源码，然后去进行测试：\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"io/ioutil\"\n    \"os\"\n    \"runtime\"\n    \"time\"\n\n    \"github.com/ci-plugins/golang-plugin-sdk/api\"\n    \"github.com/ci-plugins/golang-plugin-sdk/log\"\n    \"github.com/ci-plugins/plugin-demo-golang/translation\"\n)\n\n//go:generate i18ngenerator i18n ./translation/translation.go\n\ntype greetingParam struct {\n    UserName string `json:\"userName\"`\n    Greeting string `json:\"greeting\"`\n}\n\nfunc (a *greetingParam) String() string {\n    return fmt.Sprintf(\"userName: %v, greeting: %v\", a.UserName, a.Greeting)\n}\n\nfunc main() {\n    runtime.GOMAXPROCS(4)\n    log.Info(\"atom-demo-glang starts\")\n    defer func() {\n       if err := recover(); err != nil {\n          log.Error(\"panic: \", err)\n          api.FinishBuild(api.StatusError, \"panic occurs\")\n       }\n    }()\n\n    api.InitI18n(translation.Translations, api.GetRuntimeLanguage())\n    msg, err := api.Localize(\"input.desc.label\")\n    if err != nil {\n       log.Error(err)\n    }\n    log.Info(msg)\n\n    helloBuild()\n}\n\nfunc helloBuild() {\n    // 获取单个输入参数\n    userName := api.GetInputParam(\"userName\")\n    log.Info(\"userName: \", userName)\n\n    // 打屏\n    log.Info(\"\\nBuildInfo:\")\n    log.Info(\"Project Name:     \", api.GetProjectDisplayName())\n    log.Info(\"Pipeline Id:      \", api.GetPipelineId())\n    log.Info(\"Pipeline Name:    \", api.GetPipelineName())\n    log.Info(\"Pipeline Version: \", api.GetPipelineVersion())\n    log.Info(\"Build Id:         \", api.GetPipelineBuildId())\n    log.Info(\"Build Num:        \", api.GetPipelineBuildNumber())\n    log.Info(\"Start Type:       \", api.GetPipelineStartType())\n    log.Info(\"Start UserId:     \", api.GetPipelineStartUserId())\n    log.Info(\"Start UserName:   \", api.GetPipelineStartUserName())\n    log.Info(\"Start Time:       \", api.GetPipelineStartTimeMills())\n    log.Info(\"Workspace:        \", api.GetWorkspace())\n\n    // 输入参数解析到对象\n    paramData := new(greetingParam)\n    api.LoadInputParam(paramData)\n    log.Info(fmt.Sprintf(\"\\n%v，%v\\n\", paramData.Greeting, paramData.UserName))\n\n    // 业务逻辑\n    log.Info(\"start build\")\n    build()\n    time.Sleep(2 * time.Second)\n\n    // 输出\n    // 字符串输出\n    strData := api.NewStringData(\"test\")\n    api.AddOutputData(\"strData_01\", strData)\n\n    // 文件归档输出\n    artifactData := api.NewArtifactData()\n    artifactData.AddArtifact(\"result.dat\")\n    api.AddOutputData(\"artifactData_02\", artifactData)\n\n    // 报告输出\n    reportData := api.NewReportData(\"label_01\", api.GetWorkspace()+\"/report\", \"report.htm\")\n    api.AddOutputData(\"report_01\", reportData)\n\n    api.WriteOutput()\n    log.Info(\"build done\")\n}\n\nfunc build() {\n    log.Info(\"write result.dat\")\n    ioutil.WriteFile(api.GetWorkspace()+\"/result.dat\", []byte(\"content\"), 0644)\n    log.Info(\"write report.htm\")\n    os.Mkdir(api.GetWorkspace()+\"/report\", os.ModePerm)\n    ioutil.WriteFile(api.GetWorkspace()+\"/report/report.htm\", []byte(\"<html><head><title>Report</title></head><body><H1>This is a Report</H1></body></html>\"), 0644)\n}\n\n```\n\n在根目录下已经给我们预设了一个task.json文件，后面可以简单修改下这个文件来实现测试：\n```go\n{\n  \"atomCode\": \"goDemo\",\n  \"execution\": {\n    \"language\": \"golang\",\n    \"packagePath\": \"goDemo\",\n    \"demands\": [\n      \"chmod +x goDemo\"\n    ],\n    \"target\": \"./goDemo\"\n  },\n  \"input\": {\n    \"greeting\": {\n      \"label\": \"欢迎词\",\n      \"default\": \"Glad to see you\",\n      \"placeholder\": \"欢迎词\",\n      \"type\": \"vuex-input\",\n      \"desc\": \"欢迎词\",\n      \"required\": true,\n      \"disabled\": false,\n      \"hidden\": false,\n      \"isSensitive\": false\n    },\n    \"userName\": {\n      \"label\": \"姓名\",\n      \"default\": \"Mr. Huang\",\n      \"placeholder\": \"姓名\",\n      \"type\": \"vuex-input\",\n      \"desc\": \"姓名\",\n      \"required\": true,\n      \"disabled\": false,\n      \"hidden\": false,\n      \"isSensitive\": false\n    }\n  },\n  \"output\": {\n    \"strData_01\": {\n      \"description\": \"测试\",\n      \"type\": \"string\",\n      \"isSensitive\": false\n    }\n  }\n}\n\n```\n\n## 如何打包发布\n\n1. 进入插件代码工程目录下\n2. 打包\n- 如果按照正常的demo的目录结构是需要进入cmd/application内执行build命令，因为main在此\n\n```shell\ncd cmd/application\nGO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/${executable}\n```\n- 这次用的demo则直接在根目录下执行build命令，因为main在根目录下\n\n```shell\nGO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/${executable}\n```\n\n这里go build -o bin/${executable}会在bin目录下，生成可执行文件，文件名是${executable}，即项目名。\n也可以自定义一个名字，如`GO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/kingtest`\n1. 在任意位置新建文件夹，命名示例：release_pkg = <你的插件标识>_release\n2. 将步骤 2 生产的执行包拷贝到 下\n3. 添加 task.json 文件到 下 task.json 见示例，按照插件功能配置。\n\n```shell\nmkdir kingtest_release\ncp bin/kingtest kingtest_release/kingtest\ntouch kingtest_release/task.json\n```\n\n- 插件配置规范\n- task.json 示例：\n\n```shell\n{\n  \"atomCode\": \"king-test\",                  # atomCode 要与工作台录入的一致\n  \"execution\": {\n    \"language\": \"golang\",\n    \"packagePath\": \"kingtest\",              # 发布包中插件安装包的相对路径\n    \"demands\": [\n      \"echo start run chmod +x kingtest\",   # 插件启动前需要执行的安装命令，顺序执行\n      \"chmod +x kingtest\",                  # 插件启动前需要执行的安装命令，顺序执行\n      \"echo stop run chmod +x kingtest\",    # 插件启动前需要执行的安装命令，顺序执行\n    ],\n    \"target\": \"./kingtest\"\n  },\n  \"input\": {\n    \"greeting\": {\n      \"label\": \"欢迎词\",\n      \"default\": \"Glad to see you\",\n      \"placeholder\": \"欢迎词\",\n      \"type\": \"vuex-input\",\n      \"desc\": \"欢迎词\",\n      \"required\": true,\n      \"disabled\": false,\n      \"hidden\": false,\n      \"isSensitive\": false\n    },\n    \"userName\": {\n      \"label\": \"姓名\",\n      \"default\": \"Mr. Huang\",\n      \"placeholder\": \"姓名\",\n      \"type\": \"vuex-input\",\n      \"desc\": \"姓名\",\n      \"required\": true,\n      \"disabled\": false,\n      \"hidden\": false,\n      \"isSensitive\": false\n    }\n  },\n  \"output\": {\n    \"strData_01\": {\n      \"description\": \"测试\",\n      \"type\": \"string\",\n      \"isSensitive\": false\n    }\n  }\n}\n```\n\n4. 在 目录下，把所有文件打成 zip 包即可\n\n```shell\ncd kingtest_release && zip kingtest_release.zip kingtest task.json\n```\n\nzip包结构示例：\n```shell\n|- kingtest_release.zip         # 发布包\n   |- kingtest                  # 插件执行包\n   |- task.json                 # 插件配置文件\n```\n\n打包完成后，在插件工作台提单发布，即可测试或发布插件\n\n# 上传一个流水线插件\n\n> 开发好插件之后，通过研发商店工作台，将插件发布到研发商店，提供给用户添加到流水线中使用。\n\n## 入口\n在工作台列表，点击如下入口发起发布流程：\n![img_3.png](img_3.png)\n\n首次发布时，入口名为上架\n后续更新版本时，入口名为升级\n或者在插件发布管理->版本管理界面发起发布流程：\n![img_4.png](img_4.png)\n\n## 填写插件相关信息/上传插件发布包\n上架/升级插件时，可以修改插件的基本信息，如下所示：\n![img_5.png](img_5.png)\n\n1. 适用 Job 类型：\n- 和流水线 Job 类型对应，请按照插件实际适用情况选择\n- 若选错，需新增版本修改\n2. 发布包：\n- task.json 中的 atomCode 需和 新增插件时填写的标识一致，否则上传会失败\n\n## 测试/发布插件\n> 填写好信息，提交后，进入发布流程，可以测试->重新传包->测试，直至插件满足预期后，手动继续流程将插件发布到研发商店\n\n![img_6.png](img_6.png)\n\n1. 测试：点击后跳转到插件调试项目的流水线服务下，可以将当前插件添加到流水线，验证 UI、功能是否满足预期\n2. 重新传包：当测试发现问题，修复后，重新上传发布包，再次进行测试\n3. 继续：测试 OK，满足预期后，确认提交发布\n4. 取消发布：发布过程中，随时可以终止发布\n\n## 遇见的几个错误\n### 无权限执行\n在测试中遇见一个问题：无权限执行\n![img_7.png](img_7.png)\n\n在`execution->demands`增加一个命令`chmod +x kingtest`即可解决\n\n### 发布进度里重新传包持续报错task.json格式错误\n还有个问题，在发布进度里重新传包时，一直报错task.json格式错误，但实际格式是对的！\n触发的原因暂不知道，但是确实是一个隐藏的bug。\n![img_8.png](img_8.png)\n\n直接点击继续，然后走升级插件的方式可以正常使用。\n\n### cannot execute binary file: Exec format error\n这里的问题是我造成的，最初我在mac环境下编译的可执行文件，命令是：\n```shell\n# mac下执行\ngo build -o bin/kingtest\n```\n\n但是插件里选择的编译环境是linux。\n![img_9.png](img_9.png)\n\n解决方式：让插件选择的编译环境和可执行文件的平台统一。\n我这里选择重新编译下可执行文件，采用在mac平台交叉编译linux平台可执行文件的方式。\n```shell\nGO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/kingtest\n```\n\n也可以新建一个插件，编译环境选择mac。\n\n## 运行结果\n流水线结果：\n![img_10.png](img_10.png)\n\n输出结果：\n```shell\n[Plugin info]\n=====================================================================\nTask           : king-test\nDescription    : bk插件测试\nVersion        : 1.0.3\nAuthor         : huari\nHelp           : More Information\n=====================================================================\n-----\n[Input]\ninput(normal): (欢迎词)greeting=Glad to see you\ninput(normal): (姓名)userName=Mr. Huang\n-----\n[Install plugin]\n-----\nstart run chmod +x kingtest\nstop run chmod +x kingtest\natom-demo-glang starts\n描述\nuserName:  Mr. Huang\n\nBuildInfo:\nProject Name:      GOPS\nPipeline Id:       p-8967ed52b08847c8a5b0140937db0975\nPipeline Name:     king-test\nPipeline Version:  11\nBuild Id:          b-78947b5c39f34b32bbafb803042d1e22\nBuild Num:         15\nStart Type:        MANUAL\nStart UserId:      huari\nStart UserName:    huari\nStart Time:        1733298107286\nWorkspace:         /data/devops/workspace\n\nGlad to see you，Mr. Huang\nstart build\nwrite result.dat\nwrite report.htm\nbuild done\n[Output]\n1 file match: \n  /data/devops/workspace/result.dat\nprepare to upload 7 B\n1/1 file(s) finished\noutput(except): artifactData_02=result.dat\n入口文件检测完成\n上传自定义产出物成功，共产生了1个文件\noutput(except): report_01=report.htm\noutput(normal): strData_01=test\n-----\n\n```\n\n\n\n\n\n\n"}]