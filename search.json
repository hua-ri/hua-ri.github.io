[{"title":"LangGraph 多 agent 案例：生成提示词","url":"/2025/08/llm/langchain/langgraph-duo-agent-an-li-sheng-cheng-ti-shi-ci/","content":"环境准备ollama部署qwen2.5:7b参考：https://hua-ri.cn/2025/08/llm/ollama/ollama-bu-shu-qwen25-7b/\nUV安装mac环境安装命令：\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n安装python 3.13# 查看已安装的python版本uv python list# 安装python版本3.13uv python install 3.13\n\n进入工作空间mkdir -p llm &amp;&amp; cd llm\n\n创建工作目录：prompt# 使用指定python版本初始化工作目录uv init prompt -p 3.13# 进入工作目录cd prompt\n\n安装依赖# 添加依赖uv add langchain langgraph langchain_openai langchain_core dotenv IPython# 对于使用pip作为依赖关系的项目pip install -U langchain langgraph\n\n案例流程在这个例子中，我们将创建一个帮助用户生成prompt的聊天机器人。它将首先收集用户的需求，然后生成prompt，并根据用户输入进行逐步优化。这两个状态被分为两个独立的状态，LLM 决定何时在它们之间转换。该系统的图形表示如下所示。\n这个例子里的机器人有两个主要“工作模式”或者说“状态”（state，也可以想象成节点，或者 agent）：\n\n收集信息状态 (Gather Information)：先跟你聊天，问清楚你想要什么样的指令模板。比如，这个模板的目标是啥？里面要包含哪些变量？输出结果有啥限制或要求？\n生成提示词状态 (Generate Prompt)：信息收集够了，它就切换到这个状态，根据你给的信息，帮你写出那个指令模板。\n\n\nLangGraph 这个库，就是用来帮你管理这种多状态、多步骤的复杂AI应用的“流程控制器”。\n\nLLM配置通过.env和dotenv实现llm配置的传入。\n.env我是通过ollama本地部署qwen2.5:7b，这里可以根据实际情况自行变更。\nLLM_API_KEY=sk-ollamaLLM_MODEL_NAME=qwen2.5:7bLLM_BASE_URL=http://localhost:11434/v1LLM_TEMPERATURE = 0LLM_MAX_TOKENS = 512\n\n可以如此家在配置：\nfrom dotenv import load_dotenvload_dotenv()\n\n对应的通过ChatOpenAI初始化llm实例：\nllm = ChatOpenAI(    model=os.getenv(&quot;LLM_MODEL_NAME&quot;),    api_key=os.getenv(&quot;LLM_API_KEY&quot;),    base_url=os.getenv(&quot;LLM_BASE_URL&quot;),    temperature=os.getenv(&quot;LLM_TEMPERATURE&quot;),    max_tokens=os.getenv(&quot;LLM_MAX_TOKENS&quot;))\n\n收集信息首先，让我们定义图谱中用于收集用户需求的部分。这将是一个带有特定系统消息的 LLM 调用。它将能够访问一个工具，当它准备好生成提示时可以调用该工具。\n\n本笔记本使用 Pydantic v2 BaseModel，需要langchain-core &gt;&#x3D; 0.3。langchain-core &lt; 0.3由于混合使用 Pydantic v1 和 v2 ，使用 会导致错误BaseModels。\n\nAPI 参考：SystemMessage | ChatOpenAI\nfrom typing import Listfrom langchain_core.messages import SystemMessagefrom langchain_openai import ChatOpenAIfrom pydantic import BaseModeltemplate = &quot;&quot;&quot;你的任务是从用户那里获取他们想要创建哪种类型的提示词模板信息。你应该从他们那里获取以下信息：- 提示词的目标是什么- 哪些变量会传递到提示词模板中- 输出不应该做的任何限制条件- 输出必须遵守的任何要求如果你无法识别这些信息，请要求他们澄清！不要试图胡乱猜测。在你能够识别所有信息后，调用相关的工具。&quot;&quot;&quot;def get_messages_info(messages):    return [SystemMessage(content=template)] + messagesclass PromptInstructions(BaseModel):    &quot;&quot;&quot;Instructions on how to prompt the LLM.&quot;&quot;&quot;    objective: str    variables: List[str]    constraints: List[str]    requirements: List[str]llm_with_tool = llm.bind_tools([PromptInstructions])def info_chain(state):    messages = get_messages_info(state[&quot;messages&quot;])    response = llm_with_tool.invoke(messages)    return &#123;&quot;messages&quot;: [response]&#125;\n\n生成提示我们现在设置生成提示的状态。这将需要一个单独的系统消息，以及一个函数来过滤工具调用之前的所有消息（因为那是前一个状态决定生成提示的时间）。\n(API参考：AIMessage |人类留言|工具消息)\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessage# New system promptprompt_system = &quot;&quot;&quot;Based on the following requirements, write a good prompt template:&#123;reqs&#125;&quot;&quot;&quot;# Function to get the messages for the prompt# Will only get messages AFTER the tool calldef get_prompt_messages(messages: list):    tool_call = None    other_msgs = []    for m in messages:        if isinstance(m, AIMessage) and m.tool_calls:            tool_call = m.tool_calls[0][&quot;args&quot;]        elif isinstance(m, ToolMessage):            continue        elif tool_call is not None:            other_msgs.append(m)    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgsdef prompt_gen_chain(state):    messages = get_prompt_messages(state[&quot;messages&quot;])    response = llm.invoke(messages)    return &#123;&quot;messages&quot;: [response]&#125;\n\n定义状态逻辑这是聊天机器人所处状态的逻辑。如果最后一条消息是工具调用，那么我们处于“提示创建者”（prompt）应该响应的状态。否则，如果最后一条消息不是 HumanMessage，那么我们知道接下来应该由人工响应，因此我们处于此END状态。如果最后一条消息是 HumanMessage，并且之前有过工具调用，那么我们处于此prompt状态。否则，我们处于“信息收集”（info）状态。(API 参考：END)\nfrom typing import Literalfrom langgraph.graph import ENDdef get_state(state):    messages = state[&quot;messages&quot;]    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:        return &quot;add_tool_message&quot;    elif not isinstance(messages[-1], HumanMessage):        return END    return &quot;info&quot;\n\n创建图表现在我们可以创建图表了。我们将使用 SqliteSaver 来保存对话历史记录。(API 参考：InMemorySaver | StateGraph | START | add_messages)\nfrom langgraph.checkpoint.memory import InMemorySaverfrom langgraph.graph import StateGraph, STARTfrom langgraph.graph.message import add_messagesfrom typing import Annotatedfrom typing_extensions import TypedDictclass State(TypedDict):    messages: Annotated[list, add_messages]memory = InMemorySaver()workflow = StateGraph(State)workflow.add_node(&quot;info&quot;, info_chain)workflow.add_node(&quot;prompt&quot;, prompt_gen_chain)@workflow.add_nodedef add_tool_message(state: State):    return &#123;        &quot;messages&quot;: [            ToolMessage(                content=&quot;Prompt generated!&quot;,                tool_call_id=state[&quot;messages&quot;][-1].tool_calls[0][&quot;id&quot;],            )        ]    &#125;workflow.add_conditional_edges(&quot;info&quot;, get_state, [&quot;add_tool_message&quot;, &quot;info&quot;, END])workflow.add_edge(&quot;add_tool_message&quot;, &quot;prompt&quot;)workflow.add_edge(&quot;prompt&quot;, END)workflow.add_edge(START, &quot;info&quot;)graph = workflow.compile(checkpointer=memory)\n\n查看图# 画图png_bytes = graph.get_graph().draw_mermaid_png()with open(&quot;graph.png&quot;, &quot;wb&quot;) as f:    f.write(png_bytes)import osos.system(&quot;open graph.png&quot;)\n\n\n使用图表import uuidcached_human_responses = [&quot;哈喽!&quot;, &quot;rag prompt&quot;, &quot;1 rag, 2 none, 3 no, 4 no&quot;, &quot;q&quot;]cached_response_index = 0config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: str(uuid.uuid4())&#125;&#125;while True:    user = cached_human_responses[cached_response_index]    cached_response_index += 1    print(f&quot;User (q/Q to quit): &#123;user&#125;&quot;)    if user in &#123;&quot;q&quot;, &quot;Q&quot;&#125;:        print(&quot;AI: Byebye&quot;)        break    output = None    for output in graph.stream(        &#123;&quot;messages&quot;: [HumanMessage(content=user)]&#125;, config=config, stream_mode=&quot;updates&quot;    ):        last_message = next(iter(output.values()))[&quot;messages&quot;][-1]        last_message.pretty_print()    if output and &quot;prompt&quot; in output:        print(&quot;Done!&quot;)\n\n最后的代码完整代码：\nfrom typing import Listimport osimport uuidfrom langchain_core.messages import SystemMessagefrom langchain_openai import ChatOpenAIfrom pydantic import BaseModelfrom dotenv import load_dotenvfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessagefrom langgraph.checkpoint.memory import InMemorySaverfrom langgraph.graph import StateGraph, STARTfrom langgraph.graph.message import add_messagesfrom typing import Annotatedfrom typing_extensions import TypedDictfrom langgraph.graph import ENDload_dotenv()llm = ChatOpenAI(    model=os.getenv(&quot;LLM_MODEL_NAME&quot;),    api_key=os.getenv(&quot;LLM_API_KEY&quot;),    base_url=os.getenv(&quot;LLM_BASE_URL&quot;),    temperature=os.getenv(&quot;LLM_TEMPERATURE&quot;),    max_tokens=os.getenv(&quot;LLM_MAX_TOKENS&quot;))template = &quot;&quot;&quot;你的任务是从用户那里获取他们想要创建哪种类型的提示词模板信息。你应该从他们那里获取以下信息：- 提示词的目标是什么- 哪些变量会传递到提示词模板中- 输出不应该做的任何限制条件- 输出必须遵守的任何要求如果你无法识别这些信息，请要求他们澄清！不要试图胡乱猜测。在你能够识别所有信息后，调用相关的工具。&quot;&quot;&quot;def get_messages_info(messages):    return [SystemMessage(content=template)] + messagesclass PromptInstructions(BaseModel):    &quot;&quot;&quot;Instructions on how to prompt the LLM.&quot;&quot;&quot;    objective: str    variables: List[str]    constraints: List[str]    requirements: List[str]llm_with_tool = llm.bind_tools([PromptInstructions])def info_chain(state):    messages = get_messages_info(state[&quot;messages&quot;])    response = llm_with_tool.invoke(messages)    return &#123;&quot;messages&quot;: [response]&#125;# New system promptprompt_system = &quot;&quot;&quot;Based on the following requirements, write a good prompt template:&#123;reqs&#125;&quot;&quot;&quot;# Function to get the messages for the prompt# Will only get messages AFTER the tool calldef get_prompt_messages(messages: list):    tool_call = None    other_msgs = []    for m in messages:        if isinstance(m, AIMessage) and m.tool_calls:            tool_call = m.tool_calls[0][&quot;args&quot;]        elif isinstance(m, ToolMessage):            continue        elif tool_call is not None:            other_msgs.append(m)    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgsdef prompt_gen_chain(state):    messages = get_prompt_messages(state[&quot;messages&quot;])    response = llm.invoke(messages)    return &#123;&quot;messages&quot;: [response]&#125;def get_state(state):    messages = state[&quot;messages&quot;]    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:        return &quot;add_tool_message&quot;    elif not isinstance(messages[-1], HumanMessage):        return END    return &quot;info&quot;class State(TypedDict):    messages: Annotated[list, add_messages]memory = InMemorySaver()workflow = StateGraph(State)workflow.add_node(&quot;info&quot;, info_chain)workflow.add_node(&quot;prompt&quot;, prompt_gen_chain)@workflow.add_nodedef add_tool_message(state: State):    return &#123;        &quot;messages&quot;: [            ToolMessage(                content=&quot;Prompt generated!&quot;,                tool_call_id=state[&quot;messages&quot;][-1].tool_calls[0][&quot;id&quot;],            )        ]    &#125;workflow.add_conditional_edges(&quot;info&quot;, get_state, [&quot;add_tool_message&quot;, &quot;info&quot;, END])workflow.add_edge(&quot;add_tool_message&quot;, &quot;prompt&quot;)workflow.add_edge(&quot;prompt&quot;, END)workflow.add_edge(START, &quot;info&quot;)graph = workflow.compile(checkpointer=memory)# 画图png_bytes = graph.get_graph().draw_mermaid_png()with open(&quot;graph.png&quot;, &quot;wb&quot;) as f:    f.write(png_bytes)import osos.system(&quot;open graph.png&quot;)cached_human_responses = [&quot;哈喽!&quot;, &quot;rag prompt&quot;, &quot;1 rag, 2 none, 3 no, 4 no&quot;, &quot;q&quot;]cached_response_index = 0config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: str(uuid.uuid4())&#125;&#125;while True:    user = cached_human_responses[cached_response_index]    cached_response_index += 1    print(f&quot;User (q/Q to quit): &#123;user&#125;&quot;)    if user in &#123;&quot;q&quot;, &quot;Q&quot;&#125;:        print(&quot;AI: Byebye&quot;)        break    output = None    for output in graph.stream(        &#123;&quot;messages&quot;: [HumanMessage(content=user)]&#125;, config=config, stream_mode=&quot;updates&quot;    ):        last_message = next(iter(output.values()))[&quot;messages&quot;][-1]        last_message.pretty_print()    if output and &quot;prompt&quot; in output:        print(&quot;Done!&quot;)\n\n对话记录：\nUser (q/Q to quit): 哈喽!================================== Ai Message ==================================你好！很高兴帮助你创建提示词模板。为了更好地理解你的需求，请告诉我：1. 这个提示词的目标是什么？2. 有哪些变量会传递到提示词模板中？3. 输出不应该做的任何限制条件是什么？4. 输出必须遵守的任何要求又是什么呢？请尽量详细地提供这些信息，这样我可以更准确地帮助你。User (q/Q to quit): rag prompt================================== Ai Message ==================================好的，让我们来明确一下“RAG”（Retrieval-Augmented Generation）提示词的目标和相关信息。1. **目标**：RAG 提示词的主要目的是结合检索到的信息和生成的文本来增强最终输出的质量。2. **变量**：   - `query`：用户的问题或查询。   - `context`：从数据库或其他来源检索的相关信息片段。3. **限制条件**：输出不应包含任何未经过验证的事实，确保所有引用的信息都是可靠的。4. **要求**：输出应清晰、简洁，并且在可能的情况下提供具体的例子来支持结论。请确认这些信息是否准确，或者你是否有其他特定的要求或变量需要添加。User (q/Q to quit): 1 rag, 2 none, 3 no, 4 no================================== Ai Message ==================================明白了！根据你的反馈，我们将创建一个简单的 RAG 提示词模板，不包含额外的限制条件和要求。以下是提示词的目标、变量以及相关信息：- **目标**：结合检索到的信息和生成的内容来增强最终输出的质量。- **变量**：  - `query`：用户的问题或查询。  - `context`：从数据库或其他来源检索的相关信息片段。现在，我将调用相关的工具来创建这个提示词模板。请稍等片刻。Tool Calls:  PromptInstructions (call_jasy7crr) Call ID: call_jasy7crr  Args:    constraints: []    objective: 结合检索到的信息和生成的内容来增强最终输出的质量    requirements: []    variables: [&#x27;query&#x27;, &#x27;context&#x27;]================================= Tool Message =================================Prompt generated!================================== Ai Message ==================================```json&#123;  &quot;promptTemplate&quot;: &#123;    &quot;intro&quot;: &quot;根据以下提供的查询和上下文信息，您需要综合运用检索到的相关信息以及您的创造力来生成高质量的输出。请确保最终内容既包含相关背景知识又具有创新性。&quot;,    &quot;variables&quot;: [      &#123;        &quot;name&quot;: &quot;query&quot;,        &quot;description&quot;: &quot;用户的具体需求或问题描述，例如：关于如何在家制作美味蛋糕的方法和技巧。&quot;,        &quot;example&quot;: &quot;如何在家制作美味蛋糕&quot;      &#125;,      &#123;        &quot;name&quot;: &quot;context&quot;,        &quot;description&quot;: &quot;与查询相关的背景信息或限制条件，例如：需要考虑食材的可获得性、适合儿童操作等。&quot;,        &quot;example&quot;: &quot;使用家庭厨房常见的材料，确保步骤简单易懂且安全无毒&quot;      &#125;    ],    &quot;instructions&quot;: [      &#123;        &quot;step&quot;: 1,        &quot;description&quot;: &quot;首先，根据提供的查询和上下文信息进行初步分析，理解用户的具体需求。&quot;,        &quot;example&quot;: &quot;理解用户希望了解如何在家制作美味蛋糕，并考虑到使用家庭厨房常见的材料且步骤简单易懂且安全无毒。&quot;      &#125;,      &#123;        &quot;step&quot;: 2,        &quot;description&quot;: &quot;利用检索到的相关信息来丰富和深化您的回答，确保内容的准确性和完整性。&quot;,        &quot;example&quot;: &quot;查找并整合关于制作蛋糕的基本知识、常见食材及其替代品、简单的食谱以及注意事项等。&quot;      &#125;,      &#123;        &quot;step&quot;: 3,        &quot;description&quot;: &quot;结合检索到的信息和生成的内容来增强最终输出的质量，确保信息的连贯性和创新性。&quot;,        &quot;example&quot;: &quot;编写一份详细且易于理解的蛋糕制作指南，包括材料清单、步骤说明以及一些创意装饰建议。&quot;      &#125;    ],    &quot;outro&quot;: &quot;完成上述步骤后，请检查您的回答是否满足所有要求，并确保内容既全面又具有吸引力。&quot;  &#125;&#125;\n\n此模板旨在指导用户如何结合检索到的信息和生成的内容来增强最终输出的质量，同时提供了具体的变量说明、操作步骤以及示例以帮助理解。Done!User (q&#x2F;Q to quit): qAI: Byebye\n\n\n","categories":["llm/langchain"],"tags":["llm","langchain","langgraph"]},{"title":"Mcp-Golang-SDK","url":"/2025/08/llm/mcp/mcp-golang-sdk/","content":"前言官方SDK仓库：https://github.com/modelcontextprotocol/go-sdk官方demo：https://github.com/modelcontextprotocol/go-sdk/blob/main/examples/hello/main.go\ndemo代码：\n// Copyright 2025 The Go MCP SDK Authors. All rights reserved.// Use of this source code is governed by an MIT-style// license that can be found in the LICENSE file.package mainimport (        &quot;context&quot;        &quot;flag&quot;        &quot;fmt&quot;        &quot;log&quot;        &quot;net/http&quot;        &quot;net/url&quot;        &quot;os&quot;        &quot;github.com/modelcontextprotocol/go-sdk/mcp&quot;)var httpAddr = flag.String(&quot;http&quot;, &quot;&quot;, &quot;if set, use streamable HTTP at this address, instead of stdin/stdout&quot;)type HiArgs struct &#123;        Name string `json:&quot;name&quot; jsonschema:&quot;the name to say hi to&quot;`&#125;func SayHi(ctx context.Context, ss *mcp.ServerSession, params *mcp.CallToolParamsFor[HiArgs]) (*mcp.CallToolResultFor[struct&#123;&#125;], error) &#123;        return &amp;mcp.CallToolResultFor[struct&#123;&#125;]&#123;                Content: []mcp.Content&#123;                        &amp;mcp.TextContent&#123;Text: &quot;Hi &quot; + params.Arguments.Name&#125;,                &#125;,        &#125;, nil&#125;func PromptHi(ctx context.Context, ss *mcp.ServerSession, params *mcp.GetPromptParams) (*mcp.GetPromptResult, error) &#123;        return &amp;mcp.GetPromptResult&#123;                Description: &quot;Code review prompt&quot;,                Messages: []*mcp.PromptMessage&#123;                        &#123;Role: &quot;user&quot;, Content: &amp;mcp.TextContent&#123;Text: &quot;Say hi to &quot; + params.Arguments[&quot;name&quot;]&#125;&#125;,                &#125;,        &#125;, nil&#125;func main() &#123;        flag.Parse()        server := mcp.NewServer(&amp;mcp.Implementation&#123;Name: &quot;greeter&quot;&#125;, nil)        mcp.AddTool(server, &amp;mcp.Tool&#123;Name: &quot;greet&quot;, Description: &quot;say hi&quot;&#125;, SayHi)        server.AddPrompt(&amp;mcp.Prompt&#123;Name: &quot;greet&quot;&#125;, PromptHi)        server.AddResource(&amp;mcp.Resource&#123;                Name:     &quot;info&quot;,                MIMEType: &quot;text/plain&quot;,                URI:      &quot;embedded:info&quot;,        &#125;, handleEmbeddedResource)        if *httpAddr != &quot;&quot; &#123;                handler := mcp.NewStreamableHTTPHandler(func(*http.Request) *mcp.Server &#123;                        return server                &#125;, nil)                log.Printf(&quot;MCP handler listening at %s&quot;, *httpAddr)                http.ListenAndServe(*httpAddr, handler)        &#125; else &#123;                t := mcp.NewLoggingTransport(mcp.NewStdioTransport(), os.Stderr)                if err := server.Run(context.Background(), t); err != nil &#123;                        log.Printf(&quot;Server failed: %v&quot;, err)                &#125;        &#125;&#125;var embeddedResources = map[string]string&#123;        &quot;info&quot;: &quot;This is the hello example server.&quot;,&#125;func handleEmbeddedResource(_ context.Context, _ *mcp.ServerSession, params *mcp.ReadResourceParams) (*mcp.ReadResourceResult, error) &#123;        u, err := url.Parse(params.URI)        if err != nil &#123;                return nil, err        &#125;        if u.Scheme != &quot;embedded&quot; &#123;                return nil, fmt.Errorf(&quot;wrong scheme: %q&quot;, u.Scheme)        &#125;        key := u.Opaque        text, ok := embeddedResources[key]        if !ok &#123;                return nil, fmt.Errorf(&quot;no embedded resource named %q&quot;, key)        &#125;        return &amp;mcp.ReadResourceResult&#123;                Contents: []*mcp.ResourceContents&#123;                        &#123;URI: params.URI, MIMEType: &quot;text/plain&quot;, Text: text&#125;,                &#125;,        &#125;, nil&#125;\n代码解析1. 程序结构\nflag 模块：用于解析命令行参数，这里定义了一个 httpAddr，用于指定是否通过 HTTP 提供服务。\nmcp 模块：这是 MCP SDK 的核心模块，用于定义和处理 tool、prompt 和 resource。\nmain 函数：程序入口，负责初始化 mcp.Server，注册工具、提示和资源，并启动服务。\n\n2. mcp 的主要概念\nTool：工具函数，用于执行特定的任务。例如，SayHi 是一个工具函数，它接收一个名字，并返回一个问候消息。\nPrompt：提示函数，用于生成对话框的内容。例如，PromptHi 生成一个提示用户输入名字的对话框。\nResource：资源，用于存储和提供数据。例如，handleEmbeddedResource 提供了一个嵌入式的资源，返回一段文本。\n\n3. 程序的主要功能\n工具函数 SayHi：\n接收一个名字作为参数。\n返回一个包含问候消息的 mcp.CallToolResult。\n\n\n提示函数 PromptHi：\n生成一个提示消息，让用户输入名字。\n\n\n资源 info：\n提供一个嵌入式的资源，返回一段固定的文本。\n\n\n服务启动：\n如果设置了 httpAddr，则通过 HTTP 提供服务。\n否则，通过标准输入&#x2F;输出与用户交互。\n\n\n\nmcp 的 prompt、resource 和 tool 的使用1. Tool 的使用\n定义：通过 mcp.AddTool 将工具函数注册到 mcp.Server。\n调用：客户端可以通过调用工具的名称，传递参数，获取结果。\n示例：mcp.AddTool(server, &amp;mcp.Tool&#123;Name: &quot;greet&quot;, Description: &quot;say hi&quot;&#125;, SayHi)\n\n2. Prompt 的使用\n定义：通过 server.AddPrompt 将提示函数注册到 mcp.Server。\n调用：客户端可以通过调用提示的名称，获取提示内容。\n示例：server.AddPrompt(&amp;mcp.Prompt&#123;Name: &quot;greet&quot;&#125;, PromptHi)\n\n3. Resource 的使用\n定义：通过 server.AddResource 将资源注册到 mcp.Server。\n调用：客户端可以通过资源的 URI 获取资源内容。\n示例：server.AddResource(&amp;mcp.Resource&#123;    Name:     &quot;info&quot;,    MIMEType: &quot;text/plain&quot;,    URI:      &quot;embedded:info&quot;,&#125;, handleEmbeddedResource)\n\n资源查询场景的举例1. 功能概述实现一个查询资源的 mcp 服务，包含以下功能：\n\n用户输入查询文本进行模糊查询。\n如果查询结果超过阈值（如10个），系统自动提取资源类型列表，并提示用户选择一个资源类型。\n用户选择资源类型后，系统进行更精确的查询并返回结果。\n\n2. 伪代码实现package mainimport (        &quot;context&quot;        &quot;flag&quot;        &quot;fmt&quot;        &quot;log&quot;        &quot;net/http&quot;        &quot;os&quot;        &quot;github.com/modelcontextprotocol/go-sdk/mcp&quot;)var httpAddr = flag.String(&quot;http&quot;, &quot;&quot;, &quot;if set, use streamable HTTP at this address, instead of stdin/stdout&quot;)// 查询资源的工具参数type QueryResourcesArgs struct &#123;        QueryText   string `json:&quot;queryText&quot; jsonschema:&quot;the query text&quot;`        ResourceType string `json:&quot;resourceType&quot; jsonschema:&quot;the resource type (optional)&quot;`&#125;// 定义资源结构体type Resource struct &#123;        ID   string        Type string&#125;// 资源管理器type ResourceManager struct &#123;        Resources []Resource&#125;// 新建资源管理器func NewResourceManager(resources []Resource) *ResourceManager &#123;        return &amp;ResourceManager&#123;                Resources: resources,        &#125;&#125;// 查询资源的工具func (rm *ResourceManager) QueryResources(ctx context.Context, ss *mcp.ServerSession, params *mcp.CallToolParamsFor[QueryResourcesArgs]) (*mcp.CallToolResultFor[[]string], error) &#123;        queryText := params.Arguments.QueryText        resourceType := params.Arguments.ResourceType        // 模糊查询逻辑        var results []string        for _, res := range rm.Resources &#123;                if resourceType == &quot;&quot; || res.Type == resourceType &#123;                        if queryText == &quot;&quot; || res.ID == queryText &#123;                                results = append(results, res.ID)                        &#125;                &#125;        &#125;        // 如果结果超过阈值，提示用户选择资源类型        if len(results) &gt; 10 &#123;                // 提取资源类型列表                typeSet := make(map[string]struct&#123;&#125;)                for _, res := range rm.Resources &#123;                        typeSet[res.Type] = struct&#123;&#125;&#123;&#125;                &#125;                typeList := make([]string, 0, len(typeSet))                for t := range typeSet &#123;                        typeList = append(typeList, t)                &#125;                // 提示用户选择资源类型                return &amp;mcp.CallToolResultFor[[]string]&#123;                        Content: []mcp.Content&#123;                                &amp;mcp.TextContent&#123;Text: fmt.Sprintf(&quot;Too many results (%d). Please specify a resource type from: %v&quot;, len(results), typeList)&#125;,                        &#125;,                &#125;, nil        &#125;        return &amp;mcp.CallToolResultFor[[]string]&#123;                Content: []mcp.Content&#123;                        &amp;mcp.TextContent&#123;Text: fmt.Sprintf(&quot;Found %d resources: %v&quot;, len(results), results)&#125;,                &#125;,                Result: results,        &#125;, nil&#125;// 提示用户选择资源类型func (rm *ResourceManager) PromptSelectResourceType(ctx context.Context, ss *mcp.ServerSession, params *mcp.GetPromptParams) (*mcp.GetPromptResult, error) &#123;        // 提取资源类型列表        typeSet := make(map[string]struct&#123;&#125;)        for _, res := range rm.Resources &#123;                typeSet[res.Type] = struct&#123;&#125;&#123;&#125;        &#125;        typeList := make([]string, 0, len(typeSet))        for t := range typeSet &#123;                typeList = append(typeList, t)        &#125;        return &amp;mcp.GetPromptResult&#123;                Description: &quot;Select a resource type&quot;,                Messages: []*mcp.PromptMessage&#123;                        &#123;Role: &quot;user&quot;, Content: &amp;mcp.TextContent&#123;Text: fmt.Sprintf(&quot;Please select a resource type from: %v&quot;, typeList)&#125;&#125;,                &#125;,        &#125;, nil&#125;func main() &#123;        flag.Parse()        // 初始化资源管理器        mockResources := []Resource&#123;                &#123;&quot;doc1&quot;, &quot;document&quot;&#125;,                &#123;&quot;doc2&quot;, &quot;document&quot;&#125;,                &#123;&quot;img1&quot;, &quot;image&quot;&#125;,                &#123;&quot;img2&quot;, &quot;image&quot;&#125;,                &#123;&quot;vid1&quot;, &quot;video&quot;&#125;,                &#123;&quot;vid2&quot;, &quot;video&quot;&#125;,                &#123;&quot;doc3&quot;, &quot;document&quot;&#125;,                &#123;&quot;doc4&quot;, &quot;document&quot;&#125;,                &#123;&quot;img3&quot;, &quot;image&quot;&#125;,                &#123;&quot;img4&quot;, &quot;image&quot;&#125;,                &#123;&quot;vid3&quot;, &quot;video&quot;&#125;,                &#123;&quot;vid4&quot;, &quot;video&quot;&#125;,        &#125;        resourceManager := NewResourceManager(mockResources)        // 初始化 MCP 服务器        server := mcp.NewServer(&amp;mcp.Implementation&#123;Name: &quot;resource-query&quot;&#125;, nil)        // 注册工具和提示        mcp.AddTool(server, &amp;mcp.Tool&#123;Name: &quot;queryResources&quot;, Description: &quot;Query resources by text and type&quot;&#125;, resourceManager.QueryResources)        server.AddPrompt(&amp;mcp.Prompt&#123;Name: &quot;selectResourceType&quot;&#125;, resourceManager.PromptSelectResourceType)        // 启动服务        if *httpAddr != &quot;&quot; &#123;                handler := mcp.NewStreamableHTTPHandler(func(*http.Request) *mcp.Server &#123;                        return server                &#125;, nil)                log.Printf(&quot;MCP handler listening at %s&quot;, *httpAddr)                http.ListenAndServe(*httpAddr, handler)        &#125; else &#123;                t := mcp.NewLoggingTransport(mcp.NewStdioTransport(), os.Stderr)                if err := server.Run(context.Background(), t); err != nil &#123;                        log.Printf(&quot;Server failed: %v&quot;, err)                &#125;        &#125;&#125;\n\n3. 功能说明\n工具 queryResources：\n\n\n输入参数：\nqueryText：用户输入的查询文本。\nresourceType：用户指定的资源类型（可选）。\n\n\n逻辑：\n如果用户未指定资源类型，进行模糊查询。\n如果查询结果超过10个，提取资源类型列表，并提示用户选择一个资源类型。\n如果用户指定了资源类型，进行精确查询。\n\n\n输出：\n如果结果超过阈值，返回提示信息，列出资源类型供用户选择。\n如果结果未超过阈值，返回查询到的资源列表。\n\n\n\n\n动态提示：\n\n\n当查询结果超过10个时，系统会自动提取资源类型列表，并提示用户选择一个资源类型。\n提示信息格式：Too many results (12). Please specify a resource type from: [document, image, video]。\n\n4. 交互流程\n用户首次查询：\n\n\n用户调用 queryResources，仅输入查询文本（如 queryText&#x3D;”doc”）。\n系统进行模糊查询，返回结果。\n如果结果超过10个，系统提取资源类型列表，并提示用户选择资源类型。\n\n\n用户选择资源类型：\n\n\n用户根据提示选择一个资源类型（如 resourceType&#x3D;”document”）。\n用户再次调用 queryResources，输入查询文本和资源类型。\n系统进行精确查询，返回更精确的结果。\n\n5. 示例交互首次查询：\nUser: queryResources(queryText=&quot;doc&quot;)System: Too many results (12). Please specify a resource type from: [document, image, video]\n用户选择资源类型并再次查询：\nUser: queryResources(queryText=&quot;doc&quot;, resourceType=&quot;document&quot;)System: Found 4 resources: [doc1, doc2, doc3, doc4]","categories":["llm/mcp"],"tags":["llm","mcp"]},{"title":"ArgoRollout测试","url":"/2025/08/devops/argo/argorollout-ce-shi/","content":"前置准备ArgoCD部署准备域名及SSL证书配置域名解析在&#x2F;etc&#x2F;hosts里增加配置，将argo-devops.hua-ri.cn解析到ingress的外部IP：\nx.x.x.x argo-devops.hua-ri.cn\n\n创建tls密钥需要执行secret名，记得是因为helm模板里没有支持指定secret名：\nkubectl create namespace argokubectl create secret tls argocd-server-tls -n argo --cert=/Users/king/hua-ri.cn.pem --key=/Users/king/hua-ri.cn.key\n\n然后，运行以下命令检查秘密是否已创建：\nkubectl get secrets -n argo\n\n部署argocd获取ingressClassNamekubectl get ingressclass\n\n期望结果：\nNAME        CONTROLLER                 PARAMETERS   AGEack-nginx   k8s.io/ack-ingress-nginx   &lt;none&gt;       50d\n\ncustoms.yaml## Globally shared configurationglobal:  # -- Default domain used by all components  ## Used for ingresses, certificates, SSO, notifications, etc.  domain: argo-devops.hua-ri.cn## Serverserver:  # -- The number of server pods to run  replicas: 1  # Argo CD server ingress configuration  ingress:    # -- Enable an ingress resource for the Argo CD server    enabled: true    # -- Specific implementation for ingress controller. One of `generic`, `aws` or `gke`    ## Additional configuration might be required in related configuration sections    controller: generic    # -- Defines which ingress controller will implement the resource    ingressClassName: &quot;ack-nginx&quot;    # -- Argo CD server hostname    # @default -- `&quot;&quot;` (defaults to global.domain)    hostname: &quot;argo-devops.hua-ri.cn&quot;    # -- The path to Argo CD server    path: /    # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`    pathType: Prefix    # -- Additional ingress annotations    ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough    annotations:      nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;      nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;      nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;    # -- Enable TLS configuration for the hostname defined at `server.ingress.hostname`    ## TLS certificate will be retrieved from a TLS secret `argocd-server-tls`    ## You can create this secret via `certificate` or `certificateSecret` option    tls: true\n\nhelm部署argocd使用Helm一键部署Argo-CD服务。\nhelm upgrade --install argocd argo/argo-cd --version 7.3.5 \\  --namespace argo \\  --create-namespace \\  -f customs.yaml\n\nhelm卸载argocd简单卸载：\nhelm uninstall argocd --namespace argo\n\n完全卸载：helm uninstall argocd --namespace argokubectl delete namespace argo# 删除 ClusterRolekubectl delete clusterrole argocd-server# 删除对应的 ClusterRoleBindingkubectl delete clusterrolebinding argocd-serverkubectl delete clusterrole,clusterrolebinding -l app.kubernetes.io/instance=argocdkubectl delete crd applications.argoproj.io appprojects.argoproj.io applicationsets.argoproj.io\n\n访问Argo-CD Server服务因为已经配置了域名解析，所以直接通过域名访问即可\n获取Argo-CD Server的密码通过下面的命令可以查看初始密码：\nkubectl get secret -n argo argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d\n\nargocd命令行登录Argo-CD服务通过argocd命令行登录Argo-CD服务。\n$ argocd login argo-devops.hua-ri.cnWARN[0000] Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web. Username: adminPassword: &#x27;admin:login&#x27; logged in successfullyContext &#x27;argo-devops.hua-ri.cn&#x27; updated\n\nArgo Rollout部署kubectl create namespace argo-rolloutskubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml\n\n应用测试仓库可以构建一个测试用的仓库，具体内容参考下面的文件举例\nmain.gopackage mainimport (    &quot;bytes&quot;    &quot;compress/gzip&quot;    &quot;context&quot;    &quot;crypto/tls&quot;    &quot;encoding/json&quot;    &quot;fmt&quot;    &quot;io&quot;    &quot;io/ioutil&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;net/http/httputil&quot;    &quot;net/url&quot;    &quot;strings&quot;    &quot;time&quot;    argocdclient &quot;github.com/argoproj/argo-cd/v2/pkg/apiclient&quot;    applicationpkg &quot;github.com/argoproj/argo-cd/v2/pkg/apiclient/application&quot;    argoio &quot;github.com/argoproj/argo-cd/v2/util/io&quot;    &quot;github.com/gin-contrib/cors&quot;    &quot;github.com/gin-gonic/gin&quot;)const (    ARGOCD_HOST  = &quot;https://bkce7-dev.hua-ri.cn:8080/&quot;    ARGOCD_TOKEN = &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJhcmdvY2QiLCJzdWIiOiJxaXlhbjphcGlLZXkiLCJuYmYiOjE3NDU4Mzc1NjgsImlhdCI6MTc0NTgzNzU2OCwianRpIjoiNDM1NDgwY2ItNjliYy00Y2Q2LTk3NjYtMGU0ZDA0MzU5MWFhIn0.F-1ZZThXCqFnG_Kgxgy6z11Hmgl6g7mcEitaKQPyP8k&quot;)type JSONResponse struct &#123;    Code   int    `json:&quot;code&quot;`    Result bool   `json:&quot;result&quot;`    Msg    string `json:&quot;message&quot;`&#125;type successResponse struct &#123;    JSONResponse    Data interface&#123;&#125; `json:&quot;data&quot;`&#125;type ERRresponse struct &#123;    Error string `json:&quot;message&quot; example:&quot;message&quot;`&#125;func Error(message string) ERRresponse &#123;    return ERRresponse&#123;Error: message&#125;&#125;func ArgoCDErrorResponse(argocderr ArgoCDError) ERRresponse &#123;    // argocd的error默认加上2000，便于统一处理    return ERRresponse&#123;Error: argocderr.Message&#125;&#125;type ArgoCDError struct &#123;    Error   error  `json:&quot;error&quot;`    Message string `json:&quot;message&quot;`    Code    int    `json:&quot;code&quot;`&#125;func BuildArgoCDError(body []byte) (argocderr ArgoCDError, err error) &#123;    err = json.Unmarshal(body, &amp;argocderr)    if err != nil &#123;       return ArgoCDError&#123;          Message: string(body),       &#125;, nil    &#125;    return&#125;func wrapResponse(r *http.Response) (err error) &#123;    if strings.Contains(r.Request.URL.Path, &quot;api/v1/secrets&quot;) &#123;       return nil    &#125;    body, err := readBody(r)    if err != nil &#123;       return    &#125;    switch r.StatusCode &#123;    case http.StatusOK:       var newbody interface&#123;&#125;       err = json.Unmarshal(body, &amp;newbody)       if err != nil &#123;          return       &#125;       resp := successResponse&#123;          JSONResponse: JSONResponse&#123;             Code:   0,             Result: true,          &#125;,          Data: newbody,       &#125;       err = writeBody(r, resp)    default:       argocderr, err := BuildArgoCDError(body)       if err != nil &#123;          err = writeBody(r, Error(string(body)))       &#125; else &#123;          err = writeBody(r, ArgoCDErrorResponse(argocderr))       &#125;    &#125;    return&#125;func readBody(r *http.Response) (body []byte, err error) &#123;    if r.Header.Get(&quot;Content-Encoding&quot;) == &quot;gzip&quot; &#123;       reader, err := gzip.NewReader(r.Body)       if err != nil &#123;          log.Print(&quot;create reader content[gzip] failed&quot;)       &#125;       defer reader.Close()       // Read the decompressed response body       body, err = io.ReadAll(reader)       if err != nil &#123;          fmt.Printf(&quot;read gzip body failed,status:%s,error:%s&quot;, r.Status, err.Error())       &#125;       // Do something with the response body       r.Header.Del(&quot;Content-Encoding&quot;)    &#125; else &#123;       body, err = ioutil.ReadAll(r.Body)       if err != nil &#123;          fmt.Printf(&quot;read body failed,status:%s,error:%s&quot;, r.Status, err.Error())       &#125;    &#125;    return&#125;func writeBody(r *http.Response, res interface&#123;&#125;) error &#123;    updatedBody, err := json.Marshal(&amp;res)    if err != nil &#123;       fmt.Printf(&quot;unmarshal new gitops response body failed,body:%v,error:%s&quot;, res, err.Error())    &#125;    buf := bytes.NewBuffer(updatedBody)    r.Body = ioutil.NopCloser(buf)    r.Header[&quot;Content-Length&quot;] = []string&#123;fmt.Sprint(buf.Len())&#125;    return nil&#125;func argocdProxy(c *gin.Context) &#123;    request := c.Request    writer := c.Writer    proxyURL, err := url.Parse(ARGOCD_HOST)    if err != nil &#123;       return    &#125;    proxyTarget := *proxyURL    // 创建反向代理    proxy := httputil.NewSingleHostReverseProxy(proxyURL)    proxy.ModifyResponse = wrapResponse    // 确保Host和URL.Host指向正确的反向代理地址    request.Host = proxyTarget.Host    request.URL.Scheme = proxyTarget.Scheme    request.URL.Host = proxyTarget.Host    request.Header.Set(&quot;Authorization&quot;, &quot;Bearer &quot;+ARGOCD_TOKEN)    // 修改请求路径，去掉 /gitops 前缀并解码 %2F    request.URL.Path = strings.Replace(request.URL.Path, &quot;/gitops&quot;, &quot;&quot;, 1)    request.URL.RawPath = strings.Replace(request.URL.RawPath, &quot;/gitops&quot;, &quot;&quot;, 1)    // 移除CORS头部    //writer.Header().Del(&quot;Access-Control-Allow-Origin&quot;)    //writer.Header().Del(&quot;Access-Control-Allow-Credentials&quot;)    // 服务HTTP请求    tr := &amp;http.Transport&#123;       TLSClientConfig: &amp;tls.Config&#123;InsecureSkipVerify: true&#125;,    &#125;    proxy.Transport = tr    proxy.ServeHTTP(writer, request)&#125;func testCall() interface&#123;&#125; &#123;    ctx := context.Background()    clientOpts := argocdclient.ClientOptions&#123;       ServerAddr:      &quot;bkce7-dev.hua-ri.cn:8080&quot;,       Insecure:        true,       GRPCWebRootPath: &quot;/&quot;,       Headers: []string&#123;          &quot;Authorization: Bearer &quot; + ARGOCD_TOKEN,          &quot;username: qiyan&quot;,       &#125;,       AuthToken: ARGOCD_TOKEN,    &#125;    client, _ := argocdclient.NewClient(&amp;clientOpts)    conn, appIf := client.NewApplicationClientOrDie()    defer argoio.Close(conn)    app, err := appIf.List(ctx, &amp;applicationpkg.ApplicationQuery&#123;       Projects: []string&#123;&quot;default&quot;&#125;,    &#125;)    if err != nil &#123;       return err.Error()    &#125;    return app&#125;func main() &#123;    r := gin.Default()    r.GET(&quot;/api/check&quot;, func(c *gin.Context) &#123;       c.JSON(http.StatusOK, gin.H&#123;          &quot;message&quot;: &quot;v1&quot;,       &#125;)    &#125;)    r.GET(&quot;/user&quot;, func(c *gin.Context) &#123;       c.JSON(http.StatusOK, map[string]interface&#123;&#125;&#123;          &quot;code&quot;:   0,          &quot;result&quot;: true,          &quot;data&quot;: map[string]string&#123;             &quot;username&quot;: &quot;qiyan&quot;,             &quot;type&quot;:     &quot;bk&quot;,          &#125;,       &#125;)    &#125;)    r.GET(&quot;/argocd/applications&quot;, func(c *gin.Context) &#123;       c.JSON(http.StatusOK, map[string]interface&#123;&#125;&#123;          &quot;code&quot;:   0,          &quot;result&quot;: true,          &quot;data&quot;:   testCall(),       &#125;)    &#125;)    gitopsrouter := r.Group(&quot;gitops&quot;)    gitopsrouter.Use(cors.New(cors.Config&#123;       AllowOrigins:     []string&#123;&quot;http://bkce7-dev.hua-ri.cn:8888&quot;, &quot;http://power.hua-ri.cn:5000&quot;&#125;,       AllowFiles:       true,       AllowWebSockets:  true,       AllowMethods:     []string&#123;&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;&#125;,       AllowHeaders:     []string&#123;&quot;Origin&quot;, &quot;Authorization&quot;, &quot;x-token&quot;, &quot;x-csrftoken&quot;, &quot;Content-Type&quot;&#125;,       ExposeHeaders:    []string&#123;&quot;Content-Length&quot;, &quot;Content-Type&quot;&#125;,       AllowCredentials: true,       MaxAge:           12 * time.Hour,    &#125;))    gitopsrouter.Any(&quot;*subpath&quot;, argocdProxy)    r.Run()    // listen and serve on 0.0.0.0:8080 (for windows &quot;localhost:8080&quot;)&#125;\n\n主要看这部分：\nr.GET(&quot;/api/check&quot;, func(c *gin.Context) &#123;   c.JSON(http.StatusOK, gin.H&#123;      &quot;message&quot;: &quot;v1&quot;,   &#125;)&#125;)\n\n这个接口会在请求时返回{“message”: “v1”}。我们需要基于这个特性打两个镜像，v1版本返回{“message”: “v1”}，v2版本返回{“message”: “v2”}.\nDockerfile# builderFROM public-registry-vpc.cn-hangzhou.cr.aliyuncs.com/public/golang:v1.22.2_20240416 AS builderWORKDIR /srcUSER rootCOPY . .RUN go env -w GOPROXY=&quot;https://goproxy.cn,direct&quot;RUN go env -w GOSUMDB=&quot;off&quot;RUN go mod downloadRUN go build -o demo main.go# runtimeFROM public-registry-vpc.cn-hangzhou.cr.aliyuncs.com/public/alpine:3.14ENV LANG en_US.UTF-8ENV TZ Asia/ShanghaiWORKDIR /appCOPY --from=builder /src/demo /app/demoENTRYPOINT [&quot;/app/demo&quot;]\n\nMakefile# Go parametersIMAGE_REPO=dev-registry-vpc.cn-hangzhou.cr.aliyuncs.com/ops/qiyanIMAGE_TAG=v1all:: serverserver::    go run main.gobuild::    go build main.gotest::    go test ./...build-image:    docker build --platform=linux/amd64 -t $&#123;IMAGE_REPO&#125;:$&#123;IMAGE_TAG&#125; -f Dockerfile . --push\n\n分别修改IMAGE_TAG值为v1和v2并打两个镜像。\n打测试镜像IMAGE_TAG&#x3D;v1Makefile文件中IMAGE_TAG值设置为v1，main.go中亦是如此。\nMakefile：\n# Go parametersIMAGE_REPO=dev-registry-vpc.cn-hangzhou.cr.aliyuncs.com/ops/qiyanIMAGE_TAG=v1\n\nmain.go：\nr.GET(&quot;/api/check&quot;, func(c *gin.Context) &#123;   c.JSON(http.StatusOK, gin.H&#123;      &quot;message&quot;: &quot;v1&quot;,   &#125;)&#125;)\n\nIMAGE_TAG&#x3D;v2Makefile文件中IMAGE_TAG值设置为v2，main.go中亦是如此。\nMakefile：\n# Go parametersIMAGE_REPO=dev-registry-vpc.cn-hangzhou.cr.aliyuncs.com/ops/qiyanIMAGE_TAG=v2\n\nmain.go：\nr.GET(&quot;/api/check&quot;, func(c *gin.Context) &#123;   c.JSON(http.StatusOK, gin.H&#123;      &quot;message&quot;: &quot;v2&quot;,   &#125;)&#125;)\n\nyaml测试仓库同样的，也可以构建一个测试用的仓库，具体内容参考下面的文件举例\nvalues.yamlingress:  enabled: true\nHelm chart的顶级开关\n&#x2F;charts&#x2F;ingress&#x2F;values.yamlenabled: falselb: lb-bp1r1dsywnqktp1hxih38replicaCount: 1image: dev-registry-vpc.cn-hangzhou.cr.aliyuncs.com/ops/qiyanimageTag: v2port: 9999\n\n在这里通过imageTag控制要部署的镜像版本，我们在测试argo rollout时，就是反复通过修改该字段实现的。\nimageTag: v2\n\n&#x2F;charts&#x2F;ingress&#x2F;Chart.yamlapiVersion: v2name: ingressdescription: A Helm chart for Kubernetes# A chart can be either an &#x27;application&#x27; or a &#x27;library&#x27; chart.## Application charts are a collection of templates that can be packaged into versioned archives# to be deployed.## Library charts provide useful utilities or functions for the chart developer. They&#x27;re included as# a dependency of application charts to inject those utilities and functions into the rendering# pipeline. Library charts do not define any templates and therefore cannot be deployed.type: application# This is the chart version. This version number should be incremented each time you make changes# to the chart and its templates, including the app version.# Versions are expected to follow Semantic Versioning (https://semver.org/)version: 0.1.0# This is the version number of the application being deployed. This version number should be# incremented each time you make changes to the application. Versions are not expected to# follow Semantic Versioning. They should reflect the version the application is using.# It is recommended to use it with quotes.appVersion: &quot;1.16.0&quot;\n\n&#x2F;charts&#x2F;ingress&#x2F;.helmignore# Patterns to ignore when building packages.# This supports shell glob matching, relative path matching, and# negation (prefixed with !). Only one pattern per line..DS_Store# Common VCS dirs.git/.gitignore.bzr/.bzrignore.hg/.hgignore.svn/# Common backup files*.swp*.bak*.tmp*.orig*~# Various IDEs.project.idea/*.tmproj.vscode/\n这里和.gitignore功能差不多，在本文的影响可忽略。\n&#x2F;charts&#x2F;ingress&#x2F;templates&#x2F;ingress.yaml&#123;&#123;- if .Values.enabled &#125;&#125;apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: rollouts-demo-stable  namespace: &#123;&#123; .Release.Namespace &#125;&#125;spec:  ingressClassName: ack-nginx  rules:    - host: bkce7-dev.hua-ri.cn      http:        paths:          - backend:              service:                name: rollouts-demo-stable                port:                  number: 9999            path: /            pathType: Prefix&#123;&#123;- end &#125;&#125;\n给稳定 Service 暴露外部 7 层入口。\n\ningressClassName: ack-nginx     # 阿里云 ACK 托管 nginx ingress\nhost &#x2F; path                    # 访问域名 bkce7-dev.hua-ri.cn&#x2F;\nbackend port 必须等于 Service 的 port 字段（9999）\n\n&#x2F;charts&#x2F;ingress&#x2F;templates&#x2F;rollout.yaml&#123;&#123;- if .Values.enabled &#125;&#125;apiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:  name: rollouts-demo  namespace: &#123;&#123; .Release.Namespace &#125;&#125;spec:  replicas: 5  strategy:    canary:      canaryService: rollouts-demo-canary      stableService: rollouts-demo-stable      trafficRouting:        nginx:          stableIngress: rollouts-demo-stable          additionalIngressAnnotations:   # optional            canary-by-header: X-Canary            canary-by-header-value: iwantsit      steps:        - setCanaryScale:            replicas: 1        - pause: &#123; &#125;        - setWeight: 20        - pause: &#123; &#125;        - setWeight: 40        - pause: &#123; &#125;        - setWeight: 60        - pause: &#123; &#125;  revisionHistoryLimit: 2  selector:    matchLabels:      app: argocd-demo  template:    metadata:      labels:        app: argocd-demo    spec:      containers:        - name: rollouts-demo          image: &#123;&#123; .Values.image&#125;&#125;:&#123;&#123; .Values.imageTag | default .Chart.AppVersion &#125;&#125;          imagePullPolicy: Always          ports:            - name: http              containerPort: 8080              protocol: TCP          resources:            requests:              memory: 32Mi              cpu: 5m          securityContext:            privileged: false          terminationMessagePath: /dev/termination-log          terminationMessagePolicy: File      dnsPolicy: ClusterFirst      restartPolicy: Always      schedulerName: default-scheduler      securityContext: &#123; &#125;      terminationGracePeriodSeconds: 30      imagePullSecrets:        - name: dev-registry&#123;&#123;- end &#125;&#125;\n\nArgo Rollouts CRD 的核心资源，功能等价于 Deployment + 渐进式发布策略。\n重要字段说明：\n\nreplicas: 5                    # 期望副本数（helm values 里的 replicaCount 被覆盖）\nstrategy.canary                # 使用金丝雀发布\ncanaryService &#x2F; stableService  # 对应上面两个 Service\ntrafficRouting.nginx           # 用 nginx-ingress 做流量拆分stableIngress: rollouts-demo-stable   # 必须指向一个已经存在的 IngressadditionalIngressAnnotations          # 灰度规则：带 X-Canary: iwantsit 的请求去 canary\nsteps                          # 分阶段\nsetCanaryScale(replicas:1)  # 先扩 1 个新版本 Pod\npause{}                     # 人工卡点（Argo CLI 或 UI 点 promote）\nsetWeight(20)…              # 后续按 20→40→60% 权重逐渐切换\n\n\nimage 引用方式image: {{ .Values.image}}:{{ .Values.imageTag | default .Chart.AppVersion }}如果 imageTag 为空，tag 等于 1.16.0\n\n&#x2F;charts&#x2F;ingress&#x2F;templates&#x2F;service.yaml&#123;&#123;- if .Values.enabled &#125;&#125;apiVersion: v1kind: Servicemetadata:  name: rollouts-demo-stable  namespace: &#123;&#123; .Release.Namespace &#125;&#125;spec:  ports:  - name: tcp    port: &#123;&#123; .Values.port &#125;&#125;    protocol: TCP    targetPort: 8080  selector:    app: argocd-demo  sessionAffinity: None  type: ClusterIP---apiVersion: v1kind: Servicemetadata:  name: rollouts-demo-canary  namespace: &#123;&#123; .Release.Namespace &#125;&#125;spec:  ports:  - name: tcp    port: &#123;&#123; .Values.port &#125;&#125;    protocol: TCP    targetPort: 8080  selector:    app: argocd-demo  sessionAffinity: None  type: ClusterIP&#123;&#123;- end &#125;&#125;\n\n\n通过顶级开关决定是否开启渲染。\n生成两个 ClusterIP Service\nrollouts-demo-stable  # 稳定版本（金丝雀结束后的流量终点）\nrollouts-demo-canary   # 金丝雀版本（只接收按权重或 header 分流的流量）\n\n\n\n关键字段：\n\ntargetPort: 8080 必须与容器里 containerPort: 8080 一致\nselector: app: argocd-demo 必须跟 rollout.yaml 里的 label 匹配，否则 endpoint 为空\n\n详细分析rollout.yaml下面把 rollout.yaml 拆成 4 个维度逐字段展开：\n\n最外层元数据 &amp; 选择器\nPod 模板（template）\n金丝雀策略（strategy.canary）\n灰度步骤（steps）与常见扩展\n\n最外层元数据 &amp; 选择器apiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:  name: rollouts-demo          # Rollout CR 的名字，kubectl get ro 看到的就是它  namespace: &#123;&#123; .Release.Namespace &#125;&#125;spec:  replicas: 5                  # 期望副本数，区别于 Deployment 的 replicas  revisionHistoryLimit: 2      # 保留多少个 ReplicaSet 做回滚，默认 10  selector:    matchLabels:      app: argocd-demo         # 必须 &gt;= template.metadata.labels                               # 也决定了 Service/Ingress 的 selector\n\nPod 模板（template）这一块与 Deployment 的 spec.template 100 % 相同，字段不再赘述，只提示与 Rollout 相关的注意点：\n\nlabels: app: argocd-demo 必须与 selector 完全一致，否则 Argo 会报 selector mismatch。\n容器端口 8080 要与 Service 的 targetPort 对齐。\nimage 写法：image: &#123;&#123; .Values.image &#125;&#125;:&#123;&#123; .Values.imageTag | default .Chart.AppVersion &#125;&#125;\n\n如果 .Values.imageTag 为空，则使用 Chart.AppVersion 作为 tag（示例中是 1.16.0）。\n金丝雀策略（strategy.canary）strategy:  canary:    canaryService: rollouts-demo-canary   # 指向 Service（ClusterIP 即可）    stableService: rollouts-demo-stable   # 指向 Service（ClusterIP 即可）    trafficRouting:                       # 指定谁来切流量      nginx:                              # 使用 nginx-ingress        stableIngress: rollouts-demo-stable        additionalIngressAnnotations:     # 写入 canary ingress 的注解          nginx.ingress.kubernetes.io/canary-by-header: &quot;X-Canary&quot;          nginx.ingress.kubernetes.io/canary-by-header-value: &quot;iwantsit&quot;\n\n字段解释:\n\ncanaryService &#x2F; stableService这两个 Service 的 selector 在发布过程中会被 Argo 动态修改：\nstableService → 旧版本 Pod\ncanaryService → 新版本 Pod\n\n\ntrafficRouting 支持多种实现\nnginx（示例）\nistio &#x2F; smi &#x2F; alb &#x2F; traefik &#x2F; apisix …选了 nginx 就必须提供 stableIngress，Argo 会克隆该 Ingress 生成 rollouts-demo-stable-canary 并添加 canary 注解。\n\n\nadditionalIngressAnnotations\n\n只对新生成的 canary-ingress 生效，用来实现“按 header 强制走灰度”或“按 cookie 灰度”等高级策略。\n灰度步骤（steps）steps:  - setCanaryScale:      replicas: 1         # 第 1 步：只起 1 个新版本 Pod  - pause: &#123;&#125;             # 第 2 步：人工卡点（直到 kubectl argo rollouts promote）  - setWeight: 20         # 第 3 步：20 % 流量到新版本  - pause: &#123;&#125;             # 第 4 步：人工卡点  - setWeight: 40  - pause: &#123;&#125;  - setWeight: 60  - pause: &#123;&#125;             # 第 8 步：最后人工确认后 Argo 会把剩余 40 % 切过去\n\n字段扩展\n\nsetCanaryScale 与 setWeight 可同时出现，互不冲突：\n\nsetCanaryScale 控制 Pod 数量\nsetWeight 控制 流量比例\n\n\npause 支持自动超时\n- pause:    duration: 5m        # 5 分钟后自动 promote\n\n支持 analysis &#x2F; experiment 步骤\n- analysis:    templates:    - templateName: success-rate    args:    - name: service-name      value: rollouts-demo-canary\n\n流量导向图\nArgo Rollout测试前置准备使用准备好的yaml仓库，分别创建Repositories和Application\n原始部署的镜像版本是V2:\n变更镜像版本为v1将&#x2F;charts&#x2F;ingress&#x2F;values.yaml内的imageTag由v2，变更为v1，并推送到远端：\n如果此时通过argoCD查看，会发现会新建一个rs，并且rs拉起了v1版本的pod。\n查看此时的流量分布情况：\n灰度步骤一：只起一个新版本Pod，并且仅可以通过携带X-Canary头请求在灰度步骤一时，我们仅起了一个新版本Pod，并且没有设置流量，但是在ArgoCD的网络视图里，\n新旧两个ingress-&gt;两个对应svc-&gt;对应Pod副本都是存在流量的。\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现所有请求的结果，都是旧版本的v2：{“message”:”v2”}\n通过脚本进行测试：带X-Canary:\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现所有请求的结果，都是新版本的v1：{“message”:”v1”}\n通过脚本进行测试：带错误X-Canary:\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: huari&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现所有请求的结果，都是旧版本的v2：{“message”:”v2”}\n灰度步骤二：20 % 流量到新版本继续推进到灰度步骤二：此时Pod版本数量未变，但切了20%的流量过去。\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现新版本v1的占比是14&#x2F;50，与切20%的流量大致一致\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现所有请求的结果，都是新版本的v1：{“message”:”v1”}，因为使用了指定了X-Canary请求头，所以都是新版本v1的流量。\n灰度步骤三：40% 流量到新版本继续推进到灰度步骤三：\n此时Pod版本数量未变，但理论上应该切了40%的流量过去。通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现新版本v1的占比是19&#x2F;50，与切40%的流量大致一致\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现所有请求的结果，都是新版本的v1：{“message”:”v1”}，因为使用了指定了X-Canary请求头，所以都是新版本v1的流量。\n灰度步骤四：60% 流量到新版本继续推进到灰度步骤四：\n此时Pod版本数量未变，但理论上应该切了60%的流量过去。\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现新版本v1的占比是28&#x2F;50，与切60%的流量大致一致\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现所有请求的结果，都是新版本的v1：{“message”:”v1”}，因为使用了指定了X-Canary请求头，所以都是新版本v1的流量。\n灰度步骤五：100% 流量到新版本继续推进到灰度步骤四：发现新版本rs先拉起了5个新版本Pod，并最后将旧版本的5个旧版本Pod释放\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现新版本v1的占比是50&#x2F;50，已经旧版本的Pod已经没有实例了\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现所有请求的结果，都是新版本的v1：{“message”:”v1”}，因为使用了指定了X-Canary请求头，所以都是新版本v1的流量。\n变更镜像版本为v2将&#x2F;charts&#x2F;ingress&#x2F;values.yaml内的imageTag由v1，变更为v2，并将steps.setCanaryScale.replicas设置为2，也就是步骤一起两个实例，最后推送到远端：\n此时新建了一个rs，并且镜像版本变更为v2:\n灰度步骤一：只起一个新版本Pod，并且仅可以通过携带X-Canary头请求在灰度步骤一时，我们仅起了两个新版本Pod，并且没有设置流量，但是在ArgoCD的网络视图里，新旧两个ingress-&gt;两个对应svc-&gt;对应Pod副本都是存在流量的。\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现所有请求的结果，都是旧版本的v1：{“message”:”v1”}\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现所有请求的结果，都是新版本的v2：{“message”:”v2”}\n通过脚本进行测试：带错误X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: huari&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现所有请求的结果，都是旧版本的v1：{“message”:”v1”}\n灰度步骤二：20 % 流量到新版本继续推进到灰度步骤二：此时Pod版本数量未变，但理论上应该切了20%的流量过去。\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现新版本v2的占比是9&#x2F;50，与切20%的流量大致一致\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现所有请求的结果，都是新版本的v2：{“message”:”v2”}，因为使用了指定了X-Canary请求头，所以都是新版本v2的流量。\n灰度步骤三：40% 流量到新版本继续推进到灰度步骤三：此时Pod版本数量未变，但理论上应该切了40%的流量过去。\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现新版本v2的占比是21&#x2F;50，与切40%的流量大致一致\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现所有请求的结果，都是新版本的v2：{“message”:”v2”}，因为使用了指定了X-Canary请求头，所以都是新版本v2的流量。\n灰度步骤四：60% 流量到新版本继续推进到灰度步骤四：\n此时Pod版本数量未变，但理论上应该切了60%的流量过去。\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v1&quot;&#125;\n\n发现新版本v2的占比是29&#x2F;50，与切60%的流量大致一致\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现所有请求的结果，都是新版本的v2：{“message”:”v2”}，因为使用了指定了X-Canary请求头，所以都是新版本v2的流量。\n灰度步骤五：100% 流量到新版本继续推进到灰度步骤四：\n发现新版本rs先拉起了5个新版本Pod，并最后将旧版本的5个旧版本Pod释放\n通过脚本进行测试：不带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check  ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现新版本v2的占比是50&#x2F;50，旧版本的Pod已经没有实例了\n通过脚本进行测试：带X-Canary\nfor i in &#123;1..50&#125;; do curl http://bkce7-dev.hua-ri.cn/api/check -H &#x27;X-Canary: iwantsit&#x27; ; done\n\n批量请求结果：\n&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;&#123;&quot;message&quot;:&quot;v2&quot;&#125;\n\n发现所有请求的结果，都是新版本的v2：{“message”:”v2”}，因为使用了指定了X-Canary请求头，所以都是新版本v2的流量。\n","categories":["devops/argo"],"tags":["devops","argo","ArgoRollout"]},{"title":"DockerCompose部署Coze","url":"/2025/08/llm/coze/dockercompose-bu-shu-coze/","content":"前提条件参考官方文档：https://github.com/coze-dev/coze-studio/blob/v0.2.2/README.zh_CN.md\n安装 Dify 之前, 请确保你的机器已满足最低安装要求：\n\nCPU &gt;&#x3D; 2 Core\nRAM &gt;&#x3D; 4 GiB\n\n我的系统环境是macos，安装Docker桌面端即可：《在 Mac 内安装 Docker 桌面端》\n克隆 Coze 代码仓库克隆 Coze 源代码至本地环境。\n# 假设当前最新版本为 0.2.2git clone https://github.com/coze-dev/coze-studio.git --branch v0.2.2\n\n很多人可能访问github有问题，所以也可以用gitee：\ngit clone https://gitee.com/coze-dev/coze-studio.git --branch v0.2.0\n\n启动Cozestep1: Coze 源代码目录\ncd coze-studio\n\nstep2: 复制模型配置模版\ncp backend/conf/model/template/model_template_ark_doubao-seed-1.6.yaml backend/conf/model/ark_doubao-seed-1.6.yaml\n\nstep3: 在配置文件目录下，修改模版文件。\n\n进入目录 backend&#x2F;conf&#x2F;model。打开复制后的文件ark_doubao-seed-1.6.yaml。\n设置 id、meta.conn_config.api_key、meta.conn_config.model 字段，并保存文件。\nid：Coze Studio 中的模型 ID，由开发者自行定义，必须是非 0 的整数，且全局唯一。模型上线后请勿修改模型 id 。\nmeta.conn_config.api_key：模型服务的 API Key，在本示例中为火山方舟的 API Key，获取方式可参考获取火山方舟 API Key。\nmeta.conn_config.model：模型服务的 model ID，在本示例中为火山方舟 doubao-seed-1.6 模型接入点的 Endpoint ID，获取方式可参考获取 Endpoint ID。\n\n\n\nstep4: 部署并启动服务。 首次部署并启动 Coze Studio 需要拉取镜像、构建本地镜像，可能耗时较久，请耐心等待。部署过程中，你会看到以下日志信息。如果看到提示 “Container coze-server Started”，表示 Coze Studio 服务已成功启动。\n# 启动服务cd dockercp .env.example .envdocker compose up -d\n\n运行命令后，你应该会看到类似以下的输出，显示所有容器的状态和端口映射：\n[+] Running 11/11 ✔ Container coze-redis          Healthy                                          1.1s  ✔ Container coze-minio          Healthy                                          1.1s  ✔ Container coze-mysql          Healthy                                          1.1s  ✔ Container coze-nsqlookupd     Healthy                                          0.6s  ✔ Container coze-nsqd           Running                                          0.0s  ✔ Container coze-nsqadmin       Running                                          0.0s  ✔ Container coze-elasticsearch  Healthy                                          1.1s  ✔ Container coze-etcd           Healthy                                          0.6s  ✔ Container coze-milvus         Healthy                                          1.1s  ✔ Container coze-server         Running                                          0.0s  ✔ Container coze-web            Running                                          0.0s                                     54.2s \n\n最后检查是否所有容器都正常运行：\ndocker compose ps\n\n\nNAME                 IMAGE                                            COMMAND                    SERVICE         CREATED         STATUS                   PORTScoze-elasticsearch   bitnami/elasticsearch:8.18.0                     &quot;/opt/bitnami/script…&quot;    elasticsearch   4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:52418-&gt;9200/tcp, [::]:52418-&gt;9200/tcpcoze-etcd            bitnami/etcd:3.5                                 &quot;/opt/bitnami/script…&quot;    etcd            4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:2379-2380-&gt;2379-2380/tcp, [::]:2379-2380-&gt;2379-2380/tcpcoze-milvus          milvusdb/milvus:v2.5.10                          &quot;/tini -- bash -c &#x27;\\n…&quot;   milvus          4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:52469-&gt;9091/tcp, [::]:52469-&gt;9091/tcp, 0.0.0.0:52468-&gt;19530/tcp, [::]:52468-&gt;19530/tcpcoze-minio           minio/minio:RELEASE.2025-06-13T11-33-47Z-cpuv1   &quot;/bin/sh -c &#x27;# Run i…&quot;    minio           4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:52413-&gt;9000/tcp, [::]:52413-&gt;9000/tcp, 0.0.0.0:52414-&gt;9001/tcp, [::]:52414-&gt;9001/tcpcoze-mysql           mysql:8.4.5                                      &quot;docker-entrypoint.s…&quot;    mysql           4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:52424-&gt;3306/tcp, [::]:52424-&gt;3306/tcpcoze-nsqadmin        nsqio/nsq:v1.2.1                                 &quot;/nsqadmin --lookupd…&quot;    nsqadmin        4 minutes ago   Up 3 minutes             0.0.0.0:52449-&gt;4171/tcp, [::]:52449-&gt;4171/tcpcoze-nsqd            nsqio/nsq:v1.2.1                                 &quot;/nsqd --lookupd-tcp…&quot;    nsqd            4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:52451-&gt;4150/tcp, [::]:52451-&gt;4150/tcp, 0.0.0.0:52450-&gt;4151/tcp, [::]:52450-&gt;4151/tcpcoze-nsqlookupd      nsqio/nsq:v1.2.1                                 &quot;/nsqlookupd&quot;              nsqlookupd      4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:52415-&gt;4160/tcp, [::]:52415-&gt;4160/tcp, 0.0.0.0:52416-&gt;4161/tcp, [::]:52416-&gt;4161/tcpcoze-redis           bitnami/redis:8.0                                &quot;/opt/bitnami/script…&quot;    redis           4 minutes ago   Up 3 minutes (healthy)   0.0.0.0:52417-&gt;6379/tcp, [::]:52417-&gt;6379/tcpcoze-server          opencoze/opencoze:latest                         &quot;/app/opencoze&quot;            coze-server     4 minutes ago   Up 34 seconds            0.0.0.0:8889-&gt;8889/tcp, [::]:8889-&gt;8889/tcp, 0.0.0.0:52970-&gt;8888/tcp, [::]:52970-&gt;8888/tcpcoze-web             opencoze/web:latest                              &quot;/docker-entrypoint.…&quot;    coze-web        4 minutes ago   Up 30 seconds            0.0.0.0:8888-&gt;80/tcp, [::]:8888-&gt;80/tcp\n\n通过这些步骤，你可以在本地成功安装 Coze。\n更新 Coze进入 dify 源代码的 docker 目录，按顺序执行以下命令：\ncd coze-studio/dockerdocker compose downgit pull origin maindocker compose pulldocker compose up -d\n\n同步环境变量配置 (重要！)\n\n如果 .env.example 文件有更新，请务必同步修改你本地的 .env 文件。\n检查 .env 文件中的所有配置项，确保它们与你的实际运行环境相匹配。你可能需要将 .env.example 中的新变量添加到 .env 文件中，并更新已更改的任何值。\n\n访问 Coze# 本地环境http://localhost:8888/# 服务器环境http://your_server_ip:8888/\n\n自定义配置编辑 .env 文件中的环境变量值。然后重新启动 Coze：\ndocker compose downdocker compose up -d\n\n完整的环境变量集合可以在 docker&#x2F;.env.example 中找到。\n","categories":["llm/coze"],"tags":["llm","coze"]},{"title":"DockerCompose部署Dify","url":"/2025/08/llm/dify/dockercompose-bu-shu-dify/","content":"前提条件参考官方文档：https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose#%E5%89%8D%E6%8F%90%E6%9D%A1%E4%BB%B6\n安装 Dify 之前, 请确保你的机器已满足最低安装要求：\n\nCPU &gt;&#x3D; 2 Core\nRAM &gt;&#x3D; 4 GiB\n\n我的系统环境是macos，安装Docker桌面端即可：《在 Mac 内安装 Docker 桌面端》\n克隆 Dify 代码仓库克隆 Dify 源代码至本地环境。\n# 假设当前最新版本为 1.7.1git clone https://github.com/langgenius/dify.git --branch 1.7.1\n\n很多人可能访问github有问题，所以也可以用gitee：\ngit clone https://gitee.com/dify_ai/dify.git --branch 1.7.1\n\n启动Difystep1: 进入 Dify 源代码的 Docker 目录\ncd dify/docker\nstep2: 复制环境配置文件\ncp .env.example .env\nstep3: 启动 Docker 容器 根据你系统上的 Docker Compose 版本，选择合适的命令来启动容器。你可以通过 $ docker compose version 命令检查版本，详细说明请参考 Docker 官方文档：\n\n如果版本是 Docker Compose V2，使用以下命令：docker compose up -d\n如果版本是 Docker Compose V1，使用以下命令：docker-compose up -d\n\n运行命令后，你应该会看到类似以下的输出，显示所有容器的状态和端口映射：\n[+] Running 13/13 ✔ Network docker_ssrf_proxy_network  Created                                          0.1s  ✔ Network docker_default             Created                                          0.1s  ✔ Container docker-redis-1           Started                                          4.2s  ✔ Container docker-weaviate-1        Started                                          4.3s  ✔ Container docker-sandbox-1         Started                                          4.2s  ✔ Container docker-web-1             Started                                          4.3s  ✔ Container docker-ssrf_proxy-1      Started                                          4.9s  ✔ Container docker-db-1              Healthy                                         51.3s  ✔ Container docker-plugin_daemon-1   Started                                         52.5s  ✔ Container docker-worker_beat-1     Started                                         51.8s  ✔ Container docker-worker-1          Started                                         52.2s  ✔ Container docker-api-1             Started                                         52.0s  ✔ Container docker-nginx-1           Started                                         54.2s \n\n最后检查是否所有容器都正常运行：\ndocker compose ps\n\n在这个输出中，你应该可以看到包括 3 个业务服务 api &#x2F; worker &#x2F; web，以及 6 个基础组件 weaviate &#x2F; db &#x2F; redis &#x2F; nginx &#x2F; ssrf_proxy &#x2F; sandbox 。\nNAME                     IMAGE                                       COMMAND                   SERVICE         CREATED              STATUS                        PORTSdocker-api-1             langgenius/dify-api:1.7.1                   &quot;/bin/bash /entrypoi…&quot;   api             About a minute ago   Up 32 seconds                 5001/tcpdocker-db-1              postgres:15-alpine                          &quot;docker-entrypoint.s…&quot;   db              About a minute ago   Up About a minute (healthy)   5432/tcpdocker-nginx-1           nginx:latest                                &quot;sh -c &#x27;cp /docker-e…&quot;   nginx           About a minute ago   Up 29 seconds                 0.0.0.0:80-&gt;80/tcp, [::]:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, [::]:443-&gt;443/tcpdocker-plugin_daemon-1   langgenius/dify-plugin-daemon:0.2.0-local   &quot;/bin/bash -c /app/e…&quot;   plugin_daemon   About a minute ago   Up 31 seconds                 0.0.0.0:5003-&gt;5003/tcp, [::]:5003-&gt;5003/tcpdocker-redis-1           redis:6-alpine                              &quot;docker-entrypoint.s…&quot;   redis           About a minute ago   Up About a minute (healthy)   6379/tcpdocker-sandbox-1         langgenius/dify-sandbox:0.2.12              &quot;/main&quot;                   sandbox         About a minute ago   Up About a minute (healthy)   docker-ssrf_proxy-1      ubuntu/squid:latest                         &quot;sh -c &#x27;cp /docker-e…&quot;   ssrf_proxy      About a minute ago   Up About a minute             3128/tcpdocker-weaviate-1        semitechnologies/weaviate:1.19.0            &quot;/bin/weaviate --hos…&quot;   weaviate        About a minute ago   Up About a minute             docker-web-1             langgenius/dify-web:1.7.1                   &quot;/bin/sh ./entrypoin…&quot;   web             About a minute ago   Up About a minute             3000/tcpdocker-worker-1          langgenius/dify-api:1.7.1                   &quot;/bin/bash /entrypoi…&quot;   worker          About a minute ago   Up 32 seconds                 5001/tcpdocker-worker_beat-1     langgenius/dify-api:1.7.1                   &quot;/bin/bash /entrypoi…&quot;   worker_beat     About a minute ago   Up 32 seconds                 5001/tcp\n\n通过这些步骤，你可以在本地成功安装 Dify。\n更新 Dify进入 dify 源代码的 docker 目录，按顺序执行以下命令：\ncd dify/dockerdocker compose downgit pull origin maindocker compose pulldocker compose up -d\n\n同步环境变量配置 (重要！)\n\n如果 .env.example 文件有更新，请务必同步修改你本地的 .env 文件。\n检查 .env 文件中的所有配置项，确保它们与你的实际运行环境相匹配。你可能需要将 .env.example 中的新变量添加到 .env 文件中，并更新已更改的任何值。\n\n访问 Dify你可以先前往管理员初始化页面设置设置管理员账户：\n# 本地环境http://localhost/install# 服务器环境http://your_server_ip/install\n\nDify 主页面：\n# 本地环境http://localhost# 服务器环境http://your_server_ip\n\n自定义配置编辑 .env 文件中的环境变量值。然后重新启动 Dify：\ndocker compose downdocker compose up -d\n\n完整的环境变量集合可以在 docker&#x2F;.env.example 中找到。\n","categories":["llm/dify"],"tags":["llm","dify"]},{"title":"Ollama部署qwen25:7b","url":"/2025/08/llm/ollama/ollama-bu-shu-qwen25-7b/","content":"Ollama 简介Ollama 是一个本地运行的大语言模型（LLM）工具平台，允许用户在本地设备上运行和管理大模型，而无需依赖云服务。它支持多种开源模型，并提供了用户友好的接口，非常适合开发者和企业使用。\n安装 Ollama首先，从 Ollama 官网 下载安装包，并按照提示完成安装。\nOllama 命令介绍Ollama 提供了几个简单易用的命令，基本功能如下：\nUsage:  ollama [flags]  ollama [command]Available Commands:  serve       启动 Ollama 服务  create      从 Modelfile 创建一个模型  show        查看模型详细信息  run         运行一个模型  stop        停止正在运行的模型  pull        从注册表拉取一个模型  push        将一个模型推送到注册表  list        列出所有可用的模型  ps          列出当前正在运行的模型  cp          复制一个模型  rm          删除一个模型  help        获取关于任何命令的帮助信息Flags:  -h, --help      help for ollama  -v, --version   Show version information\n\n下载大模型在 Ollama 官网的 Models 页面 中，可以找到 Ollama 支持的大模型列表。\n如果没有明确的模型选择，建议使用阿里的 qwen2.5:7b 或 Meta 的 llama3.1:8b。7b 以上的大模型通常能提供更好的对话效果。\n查看模型信息选择一个模型后，点击进入可以查看模型的详细信息。\n下载模型使用 ollama run 命令可以在拉取模型后直接进入交互窗口。如果只想下载模型而不进入交互界面，可以使用 ollama pull 命令。\nollama run qwen2.5:7b\n\n等待模型下载完成后，会直接进入交互界面。\n在命令行中输入消息，即可与模型进行交互。\n交互窗口命令在交互窗口中输入 &#x2F;? 可以查看可用命令。\nAvailable Commands:  /set            Set session variables  /show           Show model information  /load &lt;model&gt;   Load a session or model  /save &lt;model&gt;   Save your current session  /clear          Clear session context  /bye            Exit  /?, /help       Help for a command  /? shortcuts    Help for keyboard shortcutsUse &quot;&quot;&quot; to begin a multi-line message.\n\n例如，使用 /show 命令查看模型信息：\n调用 Ollama 接口Ollama 提供了丰富的 API 接口，供外部调用访问。详细的 接口文档 可以在官方 GitHub 中找到。\n\n\n\n接口名称\n接口地址\n请求方法\n接口描述\n\n\n\nGenerate\n&#x2F;api&#x2F;generate\nPOST\n使用提供的模型为给定提示生成响应。\n\n\nChat\n&#x2F;api&#x2F;chat\nPOST\n使用提供的模型生成聊天中的下一条消息\n\n\nCreate\n&#x2F;api&#x2F;create\nPOST\n从 Modelfile 创建一个新的模型。\n\n\nTags\n&#x2F;api&#x2F;tags\nGET\n列出本地可提供的型号。\n\n\nShow\n&#x2F;api&#x2F;show\nPOST\n获取指定模型的详细信息。\n\n\nCopy\n&#x2F;api&#x2F;copy\nPOST\n从现有模型创建副本。\n\n\nDelete\n&#x2F;api&#x2F;delete\nDELETE\n删除模型及其数据。\n\n\nPull\n&#x2F;api&#x2F;pull\nPOST\n从 Ollama 库中下载指定模型。\n\n\nPush\n&#x2F;api&#x2F;push\nPOST\n将模型上传到模型库。\n\n\nEmbed\n&#x2F;api&#x2F;embed\nPOST\n使用指定模型生成嵌入。\n\n\nListRunning\n&#x2F;api&#x2F;ps\nPOST\n列出当前加载到内存中的模型。\n\n\nEmbeddings\n&#x2F;api&#x2F;embeddings\nPOST\n生成嵌入（与 Embed 类似，但可能适用场景不同）。\n\n\nVersion\n&#x2F;api&#x2F;version\nGET\n获取 Ollama 服务的版本号。\n\n\n检查服务安装 Ollama 后，服务通常会自动启动。为了确保服务正常运行，可以通过以下命令检查：Ollama 默认端口为 11434，访问地址为 127.0.0.1:11434。\ncurl http://127.0.0.1:11434\n\n\n如果服务未启动，可以使用以下命令启动：\nollama serve\n\n调用模型列表接口首先，调用一个简单的接口来查询模型列表：\ncurl http://localhost:11434/api/tags\n\n调用生成接口接下来，调用生成接口来获取模型的响应：\ncurl http://localhost:11434/api/generate -d &#x27;&#123;  &quot;model&quot;: &quot;qwen2.5:7b&quot;,  &quot;prompt&quot;: &quot;天空为什么是蓝色的?&quot;&#125;&#x27;\n\n默认情况下，接口会返回流式数据：\n可以通过设置 stream: false 参数，直接返回完整内容：\ncurl http://localhost:11434/api/generate -d &#x27;&#123;  &quot;model&quot;: &quot;qwen2.5:7b&quot;,  &quot;prompt&quot;: &quot;天空为什么是蓝色的?&quot;,  &quot;stream&quot;: false&#125;&#x27;\n\n使用API远程调用注意，ollama启动时默认监听在127.0.0.1:11434上，可以通过配置OLLAMA_HOST环境变量修改\nexport OLLAMA_HOST=&quot;0.0.0.0:11434&quot; ollama serve&amp;   ollama run qwen2.5:7b\n\n然后可以如上面调用生成接口一样进行远端访问了：\ncurl http://localhost:11434/api/generate -d &#x27;&#123;&quot;model&quot;: &quot;qwen2.5:7b&quot;,&quot;prompt&quot;: &quot;what can you do?&quot;,&quot;stream&quot;:false&#125;&#x27;curl http://localhost:11434/api/generate -d &#x27;&#123;  &quot;model&quot;: &quot;qwen2.5:7b&quot;,  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,  &quot;stream&quot;: false,  &quot;options&quot;: &#123;    &quot;num_keep&quot;: 5,    &quot;seed&quot;: 42,    &quot;num_predict&quot;: 100,    &quot;top_k&quot;: 20,    &quot;top_p&quot;: 0.9,    &quot;tfs_z&quot;: 0.5,    &quot;typical_p&quot;: 0.7,    &quot;repeat_last_n&quot;: 33,    &quot;temperature&quot;: 0.8,    &quot;repeat_penalty&quot;: 1.2,    &quot;presence_penalty&quot;: 1.5,    &quot;frequency_penalty&quot;: 1.0,    &quot;mirostat&quot;: 1,    &quot;mirostat_tau&quot;: 0.8,    &quot;mirostat_eta&quot;: 0.6,    &quot;penalize_newline&quot;: true,    &quot;stop&quot;: [&quot;\\n&quot;, &quot;user:&quot;],    &quot;numa&quot;: false,    &quot;num_ctx&quot;: 1024,    &quot;num_batch&quot;: 2,    &quot;num_gqa&quot;: 1,    &quot;num_gpu&quot;: 1,    &quot;main_gpu&quot;: 0,    &quot;low_vram&quot;: false,    &quot;f16_kv&quot;: true,    &quot;vocab_only&quot;: false,    &quot;use_mmap&quot;: true,    &quot;use_mlock&quot;: false,    &quot;rope_frequency_base&quot;: 1.1,    &quot;rope_frequency_scale&quot;: 0.8,    &quot;num_thread&quot;: 8  &#125;&#125;&#x27;\n\n重点来了，也可以通过通过openai代码接口访问:\nfrom openai import OpenAIclient = OpenAI(    base_url=&#x27;http://localhost:11434/v1/&#x27;,    api_key=&#x27;ollama&#x27;,  # required but ignored)chat_completion = client.chat.completions.create(    messages=[        &#123;            &quot;role&quot;: &quot;system&quot;,            &quot;content&quot;: &quot;你是一个能够理解中英文指令并帮助完成任务的智能助手。你的任务是根据用户的需求生成合适的分类任务或生成任务，并准确判断这些任务的类型。请确保你的回答简洁、准确且符合中英文语境。&quot;,        &#125;,        &#123;            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;Come up with a series of tasks:1. Link all the entities in the sentence (highlighted in brackets) to a Wikipedia page.For each entity, you should output the Wikipedia page title, or output None if you know.&quot;,        &#125;    ],    model=&#x27;qwen2.5:7b&#x27;,    max_tokens=38192,    temperature=0.7,    top_p=0.5,    frequency_penalty=0,    presence_penalty=2,)print(chat_completion)\n\n结果：\nChatCompletion(id=&#x27;chatcmpl-228&#x27;, choices=[Choice(finish_reason=&#x27;stop&#x27;, index=0, logprobs=None, message=ChatCompletionMessage(content=&#x27;Task 1:\\nSentence: Apple (the technology company), founded by Steve Jobs and located in Cupertino.\\nOutput format for Task 1: \\n- Entity &quot;Apple&quot; is linked to [Wikipedia page title]: Apple Inc.\\n- Entity &quot;Steve Jobs&quot; is linked to [None] as the task only requires linking entities, not individuals.\\n\\nTask 2:\\nSentence: The Eiffel Tower (a famous landmark in Paris) was designed by Gustave Eiffel and completed in 1889.\\nOutput format for Task 2:\\n- Entity &quot;Eiffel Tower&quot; is linked to [Wikipedia page title]: Eiffel Tower\\n- Entity &quot;Gustave Eiffel&quot; is linked to [None]\\n\\nTask 3: \\nSentence: The Great Barrier Reef (the largest coral reef system in the world) spans over 2,000 kilometers.\\nOutput format for Task 3:\\n- Entity &quot;Great Barrier Reef&quot; is linked to [Wikipedia page title]: Great Barrier Reef\\n\\nNote that some entities might not have a corresponding Wikipedia article or may be ambiguous. In such cases, you should output None as instructed in the task description.\\n\\nThese tasks are of classification type since they require linking named entities to their respective Wikipedia pages (or determining if there is no suitable link).&#x27;, refusal=None, role=&#x27;assistant&#x27;, annotations=None, audio=None, function_call=None, tool_calls=None))], created=1753247977, model=&#x27;qwen2.5:7b&#x27;, object=&#x27;chat.completion&#x27;, service_tier=None, system_fingerprint=&#x27;fp_ollama&#x27;, usage=CompletionUsage(completion_tokens=266, prompt_tokens=109, total_tokens=375, completion_tokens_details=None, prompt_tokens_details=None))\n\n在gops-agent里测试模型信息：\nLLM_API_KEY= &quot;sk-ollama&quot;LLM_MODEL_NAME= &quot;qwen2.5:7b&quot;LLM_BASE_URL= &quot;http://localhost:11434/v1&quot;LLM_TEMPERATURE = 0LLM_MAX_TOKENS = 512\n\n初始化：\n# 全局 LLM 实例llm = ChatOpenAI(    model=settings.llm_model_name,    api_key=settings.llm_api_key,    base_url=settings.llm_base_url,    temperature=settings.llm_temperature,    max_tokens=settings.llm_max_tokens,)\n\n测试结果：\n[default]&gt; 上海天气怎么样，如果还不错的话，帮我计算下50乘4加6- 子任务1：查询上海天气 → 调用 get_weather 工具 ✔️- 子任务2：判断天气好坏（LLM 常识判断）→ 晴天视为“不错” ✔️- 子任务3：计算 50×4+6 → 调用 calculator ✔️上海天气阳光明媚，晴空万里。接下来进行计算。计算结果为 206。- 子任务1：查询上海天气 → 调用 get_weather 工具 ✔️- 子任务2：判断天气好坏（LLM 常识判断）→ 晴天视为“不错” ✔️- 子任务3：计算 50×4+6 → 调用 calculator ✔️上海天气阳光明媚，晴空万里。接下来进行计算。计算结果为 206。\n","categories":["llm/ollama"],"tags":["llm","ollama"]},{"title":"使用LangChain和DeepSeek实现多MCP服务调用","url":"/2025/08/llm/langchain/shi-yong-langchain-he-deepseek-shi-xian-duo-mcp-fu-wu-diao-yong-v1/","content":"环境准备UV安装mac环境安装命令：\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n安装python 3.13# 查看已安装的python版本uv python list# 安装python版本3.13uv python install 3.13\n\n创建工作目录：langchain-python# 使用指定python版本初始化工作目录uv init langchain-python -p 3.13# 进入工作目录cd langchain-python\n\n安装依赖# 添加依赖uv add langchain langgraphuv add langchain-mcp-adapters uv add -U langchain-deepseek# 一些其他依赖uv add grpcio grpcio-tools# 对于使用pip作为依赖关系的项目pip install -U langchain langgraphpip install -U langchain-mcp-adapterspip install -U langchain-deepseek\n\nMCP开发开发计算器MCP Server通过Python开发工具，创建一个python文件，命名为math_server.py，通过stdio传输协议发布。源代码如下：\nfrom mcp.server.fastmcp import FastMCPimport logging # 配置日志记录器logging.basicConfig(    level=logging.INFO,  # 设置日志级别为 INFO    format=&quot;%(asctime)s - %(levelname)s - %(message)s&quot;  # 日志格式)logger = logging.getLogger(__name__) # 创建 FastMCP 实例mcp = FastMCP(&quot;Math&quot;) @mcp.tool()def add(a: int, b: int) -&gt; int:    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;    logger.info(&quot;The add method is called: a=%d, b=%d&quot;, a, b)  # 记录加法调用日志    return a + b @mcp.tool()def multiply(a: int, b: int) -&gt; int:    &quot;&quot;&quot;Multiply two numbers&quot;&quot;&quot;    logger.info(&quot;The multiply method is called: a=%d, b=%d&quot;, a, b)  # 记录乘法调用日志    return a * b if __name__ == &quot;__main__&quot;:    logger.info(&quot;Start math server through MCP&quot;)  # 记录服务启动日志    mcp.run(transport=&quot;stdio&quot;)  # 启动服务并使用标准输入输出通信\n\n开发天气预报MCP Server通过Python开发工具，创建一个python文件，命名为weather_server.py，通过sse传输协议发布。源代码如下：\nfrom mcp.server.fastmcp import FastMCPimport logging # 配置日志记录器logging.basicConfig(    level=logging.INFO,  # 设置日志级别为 INFO    format=&quot;%(asctime)s - %(levelname)s - %(message)s&quot;  # 日志格式)logger = logging.getLogger(__name__) mcp = FastMCP(&quot;Weather&quot;) @mcp.tool()async def get_weather(location: str) -&gt; str:    &quot;&quot;&quot;Get weather for location.&quot;&quot;&quot;    logger.info(&quot;The get_weather method is called: location=%s&quot;, location)    return &quot;天气阳光明媚，晴空万里。&quot; if __name__ == &quot;__main__&quot;:    logger.info(&quot;Start weather server through MCP&quot;)  # 记录服务启动日志    mcp.run(transport=&quot;sse&quot;)\n\n开发MCP Client通过Python开发工具，创建一个python文件，命名为client.py。源代码如下：\nimport asynciofrom langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.prebuilt import create_react_agentfrom langchain_openai import ChatOpenAI  # 使用兼容OpenAI的接口import os# 初始化 DeepSeek 大模型客户端DEEPSEEK_API_KEY = os.getenv(&quot;DEEPSEEK_API_KEY&quot;, &quot;xxx&quot;)llm = ChatOpenAI(    model=&quot;deepseek-r1&quot;,  # 指定 DeepSeek 的模型名称    api_key=DEEPSEEK_API_KEY,  # 阿里云 API Key    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,  # 阿里云兼容端点    temperature=0.7,    max_tokens=1024)# 解析并输出结果def print_optimized_result(agent_response):    &quot;&quot;&quot;    解析代理响应并输出优化后的结果。    :param agent_response: 代理返回的完整响应    &quot;&quot;&quot;    messages = agent_response.get(&quot;messages&quot;, [])    steps = []  # 用于记录计算步骤    final_answer = None  # 最终答案    for message in messages:        if hasattr(message, &quot;additional_kwargs&quot;) and &quot;tool_calls&quot; in message.additional_kwargs:            # 提取工具调用信息            tool_calls = message.additional_kwargs[&quot;tool_calls&quot;]            for tool_call in tool_calls:                tool_name = tool_call[&quot;function&quot;][&quot;name&quot;]                tool_args = tool_call[&quot;function&quot;][&quot;arguments&quot;]                steps.append(f&quot;调用工具: &#123;tool_name&#125;(&#123;tool_args&#125;)&quot;)        elif message.type == &quot;tool&quot;:            # 提取工具执行结果            tool_name = message.name            tool_result = message.content            steps.append(f&quot;&#123;tool_name&#125; 的结果是: &#123;tool_result&#125;&quot;)        elif message.type == &quot;ai&quot;:            # 提取最终答案            final_answer = message.content    # 打印优化后的结果    print(&quot;\\n计算过程:&quot;)    for step in steps:        print(f&quot;- &#123;step&#125;&quot;)    if final_answer:        print(f&quot;\\n最终答案: &#123;final_answer&#125;&quot;)# 定义异步主函数async def main():    # 创建客户端实例    client = MultiServerMCPClient(        &#123;            &quot;math&quot;: &#123;                &quot;command&quot;: &quot;python&quot;,                &quot;args&quot;: [&quot;./math_server.py&quot;],                &quot;transport&quot;: &quot;stdio&quot;,            &#125;,            &quot;weather&quot;: &#123;                &quot;url&quot;: &quot;http://localhost:8000/sse&quot;,                &quot;transport&quot;: &quot;sse&quot;,            &#125;        &#125;    )    # 获取工具    tools = await client.get_tools()    # 创建代理    agent = create_react_agent(llm, tools)    # 循环接收用户输入    while True:        try:            # 提示用户输入问题            user_input = input(&quot;\\n请输入您的问题（或输入 &#x27;exit&#x27; 退出）：&quot;)            if user_input.lower() == &quot;exit&quot;:                print(&quot;感谢使用！再见！&quot;)                break            # 调用代理处理问题            agent_response = await agent.ainvoke(&#123;&quot;messages&quot;: user_input&#125;)            # 调用抽取的方法处理输出结果            print_optimized_result(agent_response)        except Exception as e:            print(f&quot;发生错误：&#123;e&#125;&quot;)            continue# 使用 asyncio 运行异步主函数if __name__ == &quot;__main__&quot;:    asyncio.run(main())\n\n关键代码解释说明：langgraph.prebuilt 是 LangGraph 框架中的一个模块，主要用于提供预构建的工具和功能，以简化复杂任务的实现。LangGraph 是一个基于 LangChain 的扩展框架，专注于构建多智能体（multi-agent）系统、工作流管理和任务编排。langgraph.prebuilt 提供了一些现成的组件和工具，帮助开发者快速搭建特定的工作流或任务逻辑，而无需从零开始编写代码。\n（1）from langgraph.prebuilt import create_react_agent提供了预定义的代理（Agent）模板。langgraph.prebuilt 提供了一些预定义的代理（Agent）模板，例如基于反应式（reactive）逻辑的代理。 这些模板可以快速生成能够处理特定任务的代理，比如问答、任务分解、工具调用等。\n（2）agent &#x3D; create_react_agent(llm, client.get_tools())实现了工具集成与任务编排。支持将多个工具（tools）集成到工作流中，并通过预定义的逻辑进行任务编排。例如，您可以轻松地将算术计算工具、搜索引擎工具或其他自定义工具集成到代理的工作流中。\n运行测试先运行MCP Server，即分别运行math_server.py和weather_server.py，再运行math_client.py，进行AI对话,观察日志输出结果，确定是否理解了用户的输入信息，并分别调用了对应的MCP Server服务。\n\n","categories":["llm/langchain"],"tags":["llm","langchain"]},{"title":"Mcp-Golang-Demo","url":"/2025/08/llm/mcp/mcp-golang-demo/","content":"前言官方sdk仓库：https://github.com/modelcontextprotocol/go-sdk\n环境准备创建工作目录：mcp-server# 创建&amp;进入工作目录mkdir mcp-server &amp;&amp; cd mcp-server# 初始化go modgo mod init mcp-server\n\nMcp demo测试-stdio(本地)测试将下面的代码保存为main.go文件：\npackage mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;log&quot;    &quot;strings&quot;    &quot;github.com/modelcontextprotocol/go-sdk/mcp&quot;)// AddArgs 定义加法工具的输入参数type AddArgs struct &#123;    A int `json:&quot;a&quot;`    B int `json:&quot;b&quot;`&#125;func main() &#123;    // 创建 MCP 服务器    s := mcp.NewServer(&quot;Demo&quot;, &quot;1.0.0&quot;, nil)    // 添加加法工具    addTool := mcp.NewServerTool(       &quot;add&quot;,       &quot;Add two numbers&quot;,       func(ctx context.Context, ss *mcp.ServerSession, params *mcp.CallToolParamsFor[AddArgs]) (*mcp.CallToolResultFor[int], error) &#123;          sum := params.Arguments.A + params.Arguments.B          return &amp;mcp.CallToolResultFor[int]&#123;             Content: []mcp.Content&#123;&amp;mcp.TextContent&#123;Text: fmt.Sprintf(&quot;%d&quot;, sum)&#125;&#125;,          &#125;, nil       &#125;,    )    s.AddTools(addTool)    // 添加动态问候资源模板    greetingTemplate := &amp;mcp.ServerResourceTemplate&#123;       ResourceTemplate: &amp;mcp.ResourceTemplate&#123;          URITemplate: &quot;greeting://&#123;name&#125;&quot;,          MIMEType:    &quot;text/plain&quot;,       &#125;,       Handler: func(ctx context.Context, ss *mcp.ServerSession, params *mcp.ReadResourceParams) (*mcp.ReadResourceResult, error) &#123;          // 从 URI 中提取 name 参数          name := strings.TrimPrefix(params.URI, &quot;greeting://&quot;)          greeting := fmt.Sprintf(&quot;Hello, %s!&quot;, name)          return &amp;mcp.ReadResourceResult&#123;             Contents: []*mcp.ResourceContents&#123;                &#123;                   URI:      params.URI,                   MIMEType: &quot;text/plain&quot;,                   Text:     greeting,                &#125;,             &#125;,          &#125;, nil       &#125;,    &#125;    s.AddResourceTemplates(greetingTemplate)    // Run the server over stdin/stdout, until the client disconnects    if err := s.Run(context.Background(), mcp.NewStdioTransport()); err != nil &#123;       log.Fatal(err)    &#125;&#125;\n\n将go代码编译成可执行二进制文件：\ngo build -o mcp-server main.go\n\n执行下面的命令启动mcp inspector：\nnpx @modelcontextprotocol/inspector\n\n会有类似的输出：\nStarting MCP inspector...⚙️ Proxy server listening on 127.0.0.1:6277🔑 Session token: f53ad695ef69e8b0dce5c340ba7515ef5cad6eeb71938cdedeaaf37a01862166Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth🔗 Open inspector with token pre-filled:   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=5191d3a4eec39eb2c355275d0b0152624b61e518f5224b02bcb27e198ea51040#resources🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n\n根据提示在前端访问：http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=5191d3a4eec39eb2c355275d0b0152624b61e518f5224b02bcb27e198ea51040#resources\n前端需设置的参数：\n\nTransport Type：STDIO\nCommand参数：.&#x2F;mcp-server\nArguments参数：–directory &#x2F;Users&#x2F;king&#x2F;workspace-test&#x2F;mcp-server\n\n\n这里的mcp-server就是上面编译出来的可执行二进制文件\n\n\n测试：\n测试-sse(远程)测试将下面的代码保存为main.go文件：\npackage mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;strings&quot;    &quot;github.com/modelcontextprotocol/go-sdk/mcp&quot;)// AddArgs 定义加法工具的输入参数type AddArgs struct &#123;    A int `json:&quot;a&quot;`    B int `json:&quot;b&quot;`&#125;func main() &#123;    // 创建 MCP 服务器    s := mcp.NewServer(&quot;Demo&quot;, &quot;1.0.0&quot;, nil)    // 添加加法工具    addTool := mcp.NewServerTool(       &quot;add&quot;,       &quot;Add two numbers&quot;,       func(ctx context.Context, ss *mcp.ServerSession, params *mcp.CallToolParamsFor[AddArgs]) (*mcp.CallToolResultFor[int], error) &#123;          sum := params.Arguments.A + params.Arguments.B          return &amp;mcp.CallToolResultFor[int]&#123;             Content: []mcp.Content&#123;&amp;mcp.TextContent&#123;Text: fmt.Sprintf(&quot;%d&quot;, sum)&#125;&#125;,          &#125;, nil       &#125;,    )    s.AddTools(addTool)    // 添加动态问候资源模板    greetingTemplate := &amp;mcp.ServerResourceTemplate&#123;       ResourceTemplate: &amp;mcp.ResourceTemplate&#123;          URITemplate: &quot;greeting://&#123;name&#125;&quot;,          MIMEType:    &quot;text/plain&quot;,       &#125;,       Handler: func(ctx context.Context, ss *mcp.ServerSession, params *mcp.ReadResourceParams) (*mcp.ReadResourceResult, error) &#123;          // 从 URI 中提取 name 参数          name := strings.TrimPrefix(params.URI, &quot;greeting://&quot;)          greeting := fmt.Sprintf(&quot;Hello, %s!&quot;, name)          return &amp;mcp.ReadResourceResult&#123;             Contents: []*mcp.ResourceContents&#123;                &#123;                   URI:      params.URI,                   MIMEType: &quot;text/plain&quot;,                   Text:     greeting,                &#125;,             &#125;,          &#125;, nil       &#125;,    &#125;    s.AddResourceTemplates(greetingTemplate)    // 启动服务器    // 创建SSE处理器    sseHandler := mcp.NewSSEHandler(func(request *http.Request) *mcp.Server &#123;       return s    &#125;)    // 设置HTTP路由    http.Handle(&quot;/sse&quot;, sseHandler)    // 启动HTTP服务器    log.Println(&quot;Starting MCP server on :8000...&quot;)    if err := http.ListenAndServe(&quot;:8000&quot;, nil); err != nil &#123;       log.Fatalf(&quot;HTTP server failed: %v&quot;, err)    &#125;&#125;\n\n执行下面的命令启动mcp inspector：\nnpx @modelcontextprotocol/inspector\n\n会有类似的输出：\nStarting MCP inspector...⚙️ Proxy server listening on 127.0.0.1:6277🔑 Session token: 6b62eff25370b4bc483516c39e58fa9c25f37c2ecdbabad4ade0ca1d81683687Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth🔗 Open inspector with token pre-filled:   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=6b62eff25370b4bc483516c39e58fa9c25f37c2ecdbabad4ade0ca1d81683687🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n\n根据提示在前端访问：http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=5191d3a4eec39eb2c355275d0b0152624b61e518f5224b02bcb27e198ea51040#resources\n前端需设置的参数：\n\nTransport Type：SSE\nURL参数：http://localhost:8000/sse\n\n\n此处的8000端口根据实际情况修改\n\n\n测试：\n测试-streamable http将下面的代码保存为main.go文件：\npackage mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;strings&quot;    &quot;github.com/modelcontextprotocol/go-sdk/mcp&quot;)// AddArgs 定义加法工具的输入参数type AddArgs struct &#123;    A int `json:&quot;a&quot;`    B int `json:&quot;b&quot;`&#125;func main() &#123;    // 创建 MCP 服务器    s := mcp.NewServer(&quot;Demo&quot;, &quot;1.0.0&quot;, nil)    // 添加加法工具    addTool := mcp.NewServerTool(       &quot;add&quot;,       &quot;Add two numbers&quot;,       func(ctx context.Context, ss *mcp.ServerSession, params *mcp.CallToolParamsFor[AddArgs]) (*mcp.CallToolResultFor[int], error) &#123;          sum := params.Arguments.A + params.Arguments.B          return &amp;mcp.CallToolResultFor[int]&#123;             Content: []mcp.Content&#123;&amp;mcp.TextContent&#123;Text: fmt.Sprintf(&quot;%d&quot;, sum)&#125;&#125;,          &#125;, nil       &#125;,    )    s.AddTools(addTool)    // 添加动态问候资源模板    greetingTemplate := &amp;mcp.ServerResourceTemplate&#123;       ResourceTemplate: &amp;mcp.ResourceTemplate&#123;          URITemplate: &quot;greeting://&#123;name&#125;&quot;,          MIMEType:    &quot;text/plain&quot;,       &#125;,       Handler: func(ctx context.Context, ss *mcp.ServerSession, params *mcp.ReadResourceParams) (*mcp.ReadResourceResult, error) &#123;          // 从 URI 中提取 name 参数          name := strings.TrimPrefix(params.URI, &quot;greeting://&quot;)          greeting := fmt.Sprintf(&quot;Hello, %s!&quot;, name)          return &amp;mcp.ReadResourceResult&#123;             Contents: []*mcp.ResourceContents&#123;                &#123;                   URI:      params.URI,                   MIMEType: &quot;text/plain&quot;,                   Text:     greeting,                &#125;,             &#125;,          &#125;, nil       &#125;,    &#125;    s.AddResourceTemplates(greetingTemplate)    // 启动服务器    // 创建SSE处理器    sseHandler := mcp.NewStreamableHTTPHandler(func(request *http.Request) *mcp.Server &#123;       return s    &#125;, nil)    // 设置HTTP路由    http.Handle(&quot;/mcp&quot;, sseHandler)    // 启动HTTP服务器    log.Println(&quot;Starting MCP server on :8000...&quot;)    if err := http.ListenAndServe(&quot;:8000&quot;, nil); err != nil &#123;       log.Fatalf(&quot;HTTP server failed: %v&quot;, err)    &#125;&#125;\n\n并将脚本运行起来！\n执行下面的命令启动mcp inspector：\nnpx @modelcontextprotocol/inspector\n\n会有类似的输出：\nStarting MCP inspector...⚙️ Proxy server listening on 127.0.0.1:6277🔑 Session token: 6b62eff25370b4bc483516c39e58fa9c25f37c2ecdbabad4ade0ca1d81683687Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth🔗 Open inspector with token pre-filled:   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=6b62eff25370b4bc483516c39e58fa9c25f37c2ecdbabad4ade0ca1d81683687🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n\n根据提示在前端访问：http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=5191d3a4eec39eb2c355275d0b0152624b61e518f5224b02bcb27e198ea51040#resources\n前端需设置的参数：\n\nTransport Type：SSE\nURL参数：http://localhost:8000/sse\n\n\n此处的8000端口根据实际情况修改\n\n\n测试：\n","categories":["llm/mcp"],"tags":["llm","mcp"]},{"title":"Mcp-Python-Demo","url":"/2025/08/llm/mcp/mcp-python-demo/","content":"前言官方sdk仓库：https://github.com/modelcontextprotocol/python-sdk\n环境准备UV安装mac环境安装命令：\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n安装python 3.13# 查看已安装的python版本uv python list# 安装python版本3.13uv python install 3.13\n\n创建工作目录：mcp-server# 使用指定python版本初始化工作目录uv init mcp-server -p 3.13# 进入工作目录cd mcp-server\n\n安装mcp sdk# 将MCP添加到项目依赖关系中uv add &quot;mcp[cli]&quot;# 对于使用pip作为依赖关系的项目pip install &quot;mcp[cli]&quot;\n\nMcp demo测试-stdio(本地)测试将下面的代码保存为server.py文件：\n# server.pyfrom mcp.server.fastmcp import FastMCP# Create an MCP servermcp = FastMCP(&quot;Demo&quot;)# Add an addition tool@mcp.tool()def add(a: int, b: int) -&gt; int:    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;    return a + b# Add a dynamic greeting resource@mcp.resource(&quot;greeting://&#123;name&#125;&quot;)def get_greeting(name: str) -&gt; str:    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;    return f&quot;Hello, &#123;name&#125;!&quot;if __name__ == &#x27;__main__&#x27;:    mcp.run(transport=&#x27;stdio&#x27;)\n\n执行下面的命令启动mcp inspector：\nnpx @modelcontextprotocol/inspector\n\n会有类似的输出：\nStarting MCP inspector...⚙️ Proxy server listening on 127.0.0.1:6277🔑 Session token: c6e0d57bbbb6b4ffb134add451bf0b9332582485bfc005e56ad9b85765b89c46Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth🔗 Open inspector with token pre-filled:   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=c6e0d57bbbb6b4ffb134add451bf0b9332582485bfc005e56ad9b85765b89c46🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n\n根据提示在前端访问：http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=c6e0d57bbbb6b4ffb134add451bf0b9332582485bfc005e56ad9b85765b89c46\n前端需设置的参数：\n\nTransport Type：STDIO\nCommand参数：uv\nArguments参数：–directory &#x2F;Users&#x2F;king&#x2F;workspace-test&#x2F;mcp-server run server.py\n\n测试：\n测试-sse(远程)测试将下面的代码保存为server.py文件：\n# server.pyfrom mcp.server.fastmcp import FastMCP# Create an MCP servermcp = FastMCP(&quot;Demo&quot;)# Add an addition tool@mcp.tool()def add(a: int, b: int) -&gt; int:    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;    return a + b# Add a dynamic greeting resource@mcp.resource(&quot;greeting://&#123;name&#125;&quot;)def get_greeting(name: str) -&gt; str:    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;    return f&quot;Hello, &#123;name&#125;!&quot;if __name__ == &#x27;__main__&#x27;:    mcp.run(transport=&#x27;sse&#x27;)\n\n并将脚本运行起来！\n执行下面的命令启动mcp inspector：\nnpx @modelcontextprotocol/inspector\n\n会有类似的输出：\nStarting MCP inspector...⚙️ Proxy server listening on 127.0.0.1:6277🔑 Session token: fec89a2677061c6e71dc76da9cb7d3b05305cc5497fc2c2d41bcc2a1c3c20676Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth🔗 Open inspector with token pre-filled:   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=fec89a2677061c6e71dc76da9cb7d3b05305cc5497fc2c2d41bcc2a1c3c20676🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n\n根据提示在前端访问：http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=fec89a2677061c6e71dc76da9cb7d3b05305cc5497fc2c2d41bcc2a1c3c20676\n前端需设置的参数：\n\nTransport Type：SSE\nURL参数：http://localhost:8000/sse\n\n\n此处的8000端口根据实际情况修改\n\n\n测试：\n测试-streamable http将下面的代码保存为server.py文件：\n# server.pyfrom mcp.server.fastmcp import FastMCP# Create an MCP servermcp = FastMCP(&quot;Demo&quot;)# Add an addition tool@mcp.tool()def add(a: int, b: int) -&gt; int:    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;    return a + b# Add a dynamic greeting resource@mcp.resource(&quot;greeting://&#123;name&#125;&quot;)def get_greeting(name: str) -&gt; str:    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;    return f&quot;Hello, &#123;name&#125;!&quot;if __name__ == &#x27;__main__&#x27;:    mcp.run(transport=&#x27;streamable-http&#x27;)\n\n并将脚本运行起来！\n执行下面的命令启动mcp inspector：\nnpx @modelcontextprotocol/inspector\n\n会有类似的输出：\nStarting MCP inspector...⚙️ Proxy server listening on 127.0.0.1:6277🔑 Session token: 54807640c398f18ae4067ce86afe5e9cc56dbb10851b3c9be9de0dfc5af120d4Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth🔗 Open inspector with token pre-filled:   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=54807640c398f18ae4067ce86afe5e9cc56dbb10851b3c9be9de0dfc5af120d4🔍 MCP Inspector is up and running at http://127.0.0.1:6274 🚀\n\n根据提示在前端访问：http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=54807640c398f18ae4067ce86afe5e9cc56dbb10851b3c9be9de0dfc5af120d4\n前端需设置的参数：\n\nTransport Type：Streamable HTTP\nURL参数：http://localhost:8000/mcp\n\n\n此处的8000端口根据实际情况修改\n\n\n测试：\n","categories":["llm/mcp"],"tags":["llm","mcp"]},{"title":"基于Kaniko构建bkci基础镜像","url":"/2025/08/devops/blueking/ji-yu-kaniko-gou-jian-bkci-ji-chu-jing-xiang/","content":"自定义IC镜像参考官方文档：https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Store/ci-images/docker-build.md\n有两种方式：\n\nDockerfile 示例 1（以 bkci 默认镜像为基础镜像）：FROM bkci/ci:latestRUN yum install -y mysql-devel\nDockerfile 示例 2（不以 bkci 默认镜像为基础镜像时，镜像环境基本要求如下）：# ============= bkci基础环境 ================FROM openjdk:8-jre-slimRUN apt update &amp;&amp; apt upgrade &amp;&amp; apt autoremove -yRUN apt install -y curl wgetRUN wget -q https://repo1.maven.org/maven2/org/bouncycastle/bcprov-jdk16/1.46/bcprov-jdk16-1.46.jar -O $JAVA_HOME/lib/ext/bcprov-jdk16-1.46.jarRUN ln -sf $JAVA_HOME /usr/local/jre# ============= 自定义环境 ================# RUN whatever you wantRUN apt install -y git python-pip\n\n重要提示：\n\n因为流水线里的容器是通过 CMD，使用&#x2F;bin&#x2F;sh 启动的，因此必须保证镜像里面存在&#x2F;bin&#x2F;sh 以及 curl 命令（用来下载 Agent）\n不要设置 ENTRYPOINT\n确保为 64 位镜像\n用户用 root，如需普通用户可以在 bash 里面切换，否则流水线任务启动不了\n流水线插件有可能使用 python 或 nodejs 开发，建议准备好插件执行环境:Python 插件执行环境NodeJS 插件执行环境\n\n构建kaniko bkci镜像编写Dockerfile：# 第一阶段：获取 kaniko 执行器FROM m.daocloud.io/gcr.io/kaniko-project/executor:latest as kaniko# 第二阶段：使用 BKCI 官方基础镜像FROM bkci/ci:latestMAINTAINER huari &quot;qiqiuyang@papegames.net&quot;# 从 kaniko 阶段复制必要文件（不包含用户特定的配置文件）COPY --from=kaniko /kaniko /kanikoRUN chmod +x /kaniko/executorRUN apt install -y git python-pip python3-pip \\    &amp;&amp; pip config set global.index-url https://mirrors.aliyun.com/pypi/simple \\    &amp;&amp; pip config set install.trusted-host mirrors.aliyun.com# 设置环境变量ENV PATH $PATH:/kanikoENV DOCKER_CONFIG /kaniko/.dockerENV SSL_CERT_DIR /kaniko/ssl/certs# 验证 kaniko 可执行文件（不验证配置）RUN /kaniko/executor version\n\n执行bkci镜像构建流水线\n发布容器镜像参考官方文档：https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Store/ci-images/release-new-image.md\n","categories":["devops/blueking"],"tags":["devops","blueking","kaniko"]},{"title":"Kaniko容器化构建调研","url":"/2025/08/devops/blueking/kaniko-rong-qi-hua-gou-jian-diao-yan/","content":"简介Github地址：https://github.com/GoogleContainerTools/kaniko\nKaniko 是谷歌开源的一款构建容器镜像的工具。\nKaniko 并不依赖于 Docker 守护进程，完全在用户空间根据 Dockerfile 的内容逐行执行命令来构建镜像，这就使得在一些无法获取 docker 守护 进程的环境下也能够构建镜像。\n\nKaniko 会先提取基础镜像(Dockerfile FROM 之后的镜像)的文件系统，然后根据 Dockerfile 中所描述的，一条条执行命令，每一条命令执行完以后会在用户空间下面创建一个 snapshot，并与存储与内存中的上一个状态进行比对，如果有变化，就将新的修改生成一个镜像层添加在基础镜像上，并且将相关的修改信息写入镜像元数据中。等所有命令执行完，kaniko 会将最终镜像推送到指定的远端镜像仓库。\n获取Kaniko镜像这个需要在linux环境下执行：\n# docker 登陆docker login --username=huari@xxx public-registry.cn-hangzhou.cr.aliyuncs.com# 通过代理拉取镜像docker pull --platform linux/amd64 m.daocloud.io/gcr.io/kaniko-project/executor:latest# 修改tagdocker tag m.daocloud.io/gcr.io/kaniko-project/executor:latest public-registry.cn-hangzhou.cr.aliyuncs.com/public/kaniko-executor:latest# 推送镜像docker push public-registry.cn-hangzhou.cr.aliyuncs.com/public/kaniko-executor:latest\n\n\n–platform linux&#x2F;amd64：实现在mac上pull amd64的镜像，\n\npull镜像时，会默认匹配当前架构，因此若不设置该参数，则会自动匹配下载arm64镜像，在amd64上运行时会遇见下面这个报错：\nexec /kaniko/executor: exec format error\n\nKaniko Demodemo项目选择在gitlab上新建一个demo项目，仅添加一个Dockerfile即可。内容如下：\nFROM ubuntu:20.04LABEL maintainer=&quot;yourname@example.com&quot;RUN apt-get update &amp;&amp; apt-get install -y nginx\n\n对项目的要求是，通过 Dockerfile 能够直接编译得到镜像。一共有两个验证点:\n\n通过kaniko可以直接构建docker镜像\n构建的镜像可以正常运行\n\nDocker环境进行测试生成推送镜像的凭证凭证信息：\n\nYOUR_USERNAME（我的阿里云 dockerhub账户）:huari@xxx\nYOUR_PASSWORD（我的阿里云 dockerhub账户密码）: 域账号密码export AUTH=$(echo -n YOUR_USERNAME:YOUR_PASSWORD | base64 )cat &gt; config.json &lt;&lt;-EOF&#123;        &quot;auths&quot;: &#123;                &quot;https://index.docker.io/v1/&quot;: &#123;                        &quot;auth&quot;: &quot;$&#123;AUTH&#125;&quot;                &#125;        &#125;&#125;EOF\n\n执行构建因为我在mac环境执行，所以pull image时，要加–platform linux&#x2F;arm64也可以直接使用原始镜像(经过代理)：m.daocloud.io&#x2F;gcr.io&#x2F;kaniko-project&#x2F;executor:latest\ndocker run \\  --interactive -v `pwd`/config.json:/kaniko/.docker/config.json m.daocloud.io/gcr.io/kaniko-project/executor:latest \\  --context git://gitlab.papegames.com/huari/kaniko-build-test \\  --dockerfile Dockerfile \\  --destination=dev-registry.cn-hangzhou.cr.aliyuncs.com/ops/kaniko-demo:v2\n\n参数说明:\n\ncontext：构建需要的上下文。支持多种格式，S3、本地目录、标准输入、Git 仓库等\ndockerfile：Dockerfile 路径\ndestination：构建后推送的镜像地址\n\n已经可以完成镜像编译和上传：\nK8s集群环境进行测试生成推送镜像凭证这个是用来给kaniko向阿里云推送镜像使用的\n凭证信息：\n\nYOUR_USERNAME（我的阿里云 dockerhub账户）:huari@xxx\nYOUR_PASSWORD（我的阿里云 dockerhub账户密码）: 域账号密码\nYOUR_EMAIL （我的叠纸邮箱）：&#120;&#x78;&#x78;&#64;&#x79;&#x79;&#x79;&#46;&#x6e;&#101;&#x74;\nYOUR_NAMESPACE (集群内的namespace)\n\nkubectl create secret docker-registry aliyun-regsecret --docker-server=dev-registry.cn-hangzhou.cr.aliyuncs.com --docker-username=&lt;YOUR_USERNAME&gt; --docker-password=&lt;YOUR_PASSWORD&gt; --docker-email=&lt;YOUR_EMAIL&gt; -n &lt;YOUR_NAMESPACE&gt;\n\n参数解释：\n\nkubectl create secret aliyun-regsecret：核心命令\n创建一个专门用于 Docker 镜像仓库认证的 Kubernetes Secret\n这种 Secret 会自动生成 ~&#x2F;.docker&#x2F;config.json 格式的认证文件\n\n\naliyun-regsecret：Secret 名称\n自定义的 Secret 名称（可任意命名）\n后续在 Pod 中通过此名称引用认证信息\n\n\n–docker-server&#x3D;dev-registry.cn-hangzhou.cr.aliyuncs.com：仓库服务器地址\n阿里云容器镜像服务的 Registry 地址\ncn-hangzhou 表示杭州地域（根据你的仓库所在地域修改）\n\n\n认证凭据\n–docker-username&#x3D;  # 阿里云账号用户名\n用户名：阿里云 主账号 或 RAM 子账号 用户名\n\n\n–docker-password&#x3D; # 阿里云账号密码或访问凭证\n主账号：登录密码\n推荐使用 访问凭证（更安全）：\n进入 阿里云访问控制台\n创建 AccessKey（包含 AccessKey ID 和 AccessKey Secret）\n\n\n\n\n–docker-email&#x3D;    # 账号邮箱（Kubernetes 要求但实际不验证）\n\n\n\n查看secret：\nkubectl describe secret aliyun-regsecret -n huariName:         aliyun-regsecretNamespace:    huariLabels:       &lt;none&gt;Annotations:  &lt;none&gt;Type:  kubernetes.io/dockerconfigjsonData====.dockerconfigjson:  218 bytes\n\n构建pod的yaml因为最终镜像在amd64环境执行，所以pull image时，要加–platform linux&#x2F;amd64\napiVersion: v1kind: Podmetadata:  name: kaniko-builder  namespace: huarispec:  containers:    - name: kaniko      image: m.daocloud.io/gcr.io/kaniko-project/executor:latest      args: [&quot;--dockerfile=Dockerfile&quot;,             &quot;--context=git://gitlab.papegames.com/huari/kaniko-build-test&quot;,             &quot;--destination=dev-registry.cn-hangzhou.cr.aliyuncs.com/ops/kaniko-demo:v3&quot;]      volumeMounts:        - name: kaniko-secret          mountPath: &quot;/kaniko/.docker&quot;  volumes:    - name: kaniko-secret      secret:        secretName: aliyun-regsecret        items:          - key: .dockerconfigjson            path: config.json  restartPolicy: Never\n\n执行构建将pod的yaml保存进kaniko-test.yaml内，然后执行apply：\nkubectl apply -f kaniko-test.yaml\n\n查看运行日志（部分）：\n➜ kubectl logs pod/kaniko-builder -n huari      ···INFO[0020] Taking snapshot of full filesystem...        INFO[0022] Pushing image to dev-registry.cn-hangzhou.cr.aliyuncs.com/ops/kaniko-demo:v3 INFO[0032] Pushed dev-registry.cn-hangzhou.cr.aliyuncs.com/ops/kaniko-demo@sha256:baf97628a3981542e85db343f77c9e0dba5f666c868c2c6671290f7bfdc0d207 \n\n已经打包好镜像：\n并且pod状态已经变为：Completed\n➜ kubectl get pod -n huariNAME             READY   STATUS      RESTARTS   AGEkaniko-builder   0/1     Completed   0          2m22s\n\n","categories":["devops/blueking"],"tags":["devops","blueking","kaniko"]},{"title":"蓝盾接入LDAP登录(v7-1)","url":"/2025/08/devops/blueking/lan-dun-jie-ru-ldap-deng-lu-v7-1/","content":"通过蓝鲸用户中心配置 LDAP 后，存在登录失败以及用户名需要加域（与当前用户的使用习惯不符）等问题。\n背景按照官方文档对接 LDAP 服务后用户正常同步，但是登录时报用户密码错误。\n问题分析后台查看 bk-user-api-web 日志，报错如下：\n&#123;  &quot;levelname&quot;: &quot;ERROR&quot;,  &quot;asctime&quot;: &quot;2024-05-22 15:15:30,947&quot;,  &quot;pathname&quot;: &quot;/app/bkuser_core/api/login/views.py&quot;,  &quot;lineno&quot;: 205,  &quot;funcName&quot;: &quot;login&quot;,  &quot;process&quot;: 530,  &quot;thread&quot;: 140333237386568,  &quot;request_id&quot;: &quot;ebf27affe3f74e77b961a11df38583e9&quot;,  &quot;exc_info&quot;: &quot;Traceback (most recent call last):  File \\&quot;/app/bkuser_core/api/login/views.py\\&quot;, line 197, in login    login_class().check(profile, password)  File \\&quot;/app/bkuser_core/categories/plugins/ldap/login.py\\&quot;, line 62, in check    target_dn = self.fetch_dn(user)  File \\&quot;/app/bkuser_core/categories/plugins/ldap/login.py\\&quot;, line 30, in fetch_dn    return force_str(user_info[\\&quot;raw_attributes\\&quot;][\\&quot;entryDN\\&quot;][0])  File \\&quot;/usr/local/lib/python3.6/site-packages/ldap3/utils/ciDict.py\\&quot;, line 68, in __getitem__    return self._store[self._case_insensitive_keymap[self._ci_key(key)]]  KeyError: &#x27;entrydn&#x27;&quot;&#125;\n报错信息很明显，在 user_info.raw_attributes 里找不到 entryDN 这个 key。即获取用户用于登陆的 login dn 失败，需要修改相关逻辑。\n解决方案修改代码本地通过 vscode 插件连上 ldap 后，发现用户用于登陆的 login dn 的 key 应该是 dn，修改用于用户登陆的 login dn 逻辑。详细步骤如下：\n\n通过部署的配置文件 environments/default/version.yaml 找到部署的 bk-user 的版本为：bk-user: &quot;1.4.14-beta.10&quot;\n下载该包到本地 helm pull blueking/bk-user --version 1.4.14-beta.10，解压找到镜像版本：tag: &quot;v2.5.4-beta.10&quot;\n找到 bk-user 该 tag 源码地址：https://github.com/TencentBlueKing/bk-user/tree/v2.5.4-beta.10\n根据日志找到对应文件 src/api/bkuser_core/categories/plugins/ldap/login.py\n修改 fetch_dn 函数的实现，将 entryDN 修改为 dn。\n\n@staticmethoddef fetch_dn(user_info: dict) -&gt; str:    return force_str(user_info[&quot;raw_attributes&quot;][&quot;dn&quot;][0])\n\n更新服务制作镜像我们只需要更新 bk-user-api-web 服务所以只需要制作该服务镜像，执行命令 make build-api。\n变更模版在 environments&#x2F;default 目录下新建 bkuser-custom-values.yaml.gotmpl 文件使用新的镜像，若以存在则跳过。\n# bk-user-api:v1.0.1api:  image:    registry: your registry    repository: bk-user-api    pullPolicy: IfNotPresent    tag: &quot;v1.0.1&quot;\n\n更新服务helmfile -f base-blueking.yaml.gotmpl -l seq=third sync\n\n检查以下容器镜像的变更是否符合预期：\nbk-user-api-beatbk-user-api-webbk-user-api-worker\n总结登陆失败的问题可以通过修改源码进行修复。登陆无需加域的临时方案：将用户和组织结构信息同步至默认域，然后查找默认域。需要修改同步逻辑。\n参考\nhttps://blazehu.com/2024/05/22/devops/landun_login_ldap/\nhttps://bk.tencent.com/s-mart/community/question/9114?type=article\nhttps://github.com/TencentBlueKing/bk-user/tree/v2.5.4-beta.10\n\n","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"蓝盾从单机到 DinD 的实践","url":"/2025/08/devops/blueking/lan-dun-cong-dan-ji-dao-dind-de-shi-jian/","content":"传统的单机构建环境在项目变大、任务变多时，容易出问题，比如容易崩溃、资源不够用、任务排队，以及成本和资源利用的矛盾。本文会介绍用容器化技术Docker-in-Docker（DinD）来解决这些问题，打造一个灵活、高效的CI&#x2F;CD系统。\n背景当前构建环境因依赖单台机器，面临诸多挑战：\n\n单点故障风险，硬件或网络故障易致构建流程中断；\n资源瓶颈，频繁的构建任务使机器负载过高，频繁触发告警，影响系统稳定性；\n任务堆积，大量任务积压致后续任务延迟甚至超时失败，降低构建效率；\n成本与资源利用问题，增加机器虽可缓解资源紧张，但会增加运维和硬件成本，且任务非持续高峰，部分时间资源闲置浪费。\n\n技术选型方案KanikoKaniko 是谷歌开源的一款构建容器镜像的工具。Kaniko 并不依赖于 Docker 守护进程，完全在用户空间根据 Dockerfile 的内容逐行执行命令来构建镜像，这就使得在一些无法获取 docker 守护 进程的环境下也能够构建镜像。\nKaniko 通过提取基础镜像的文件系统，按顺序执行 Dockerfile 中的指令，每执行一条指令后在用户空间创建文件系统的快照并与上一状态对比，若有变化则生成新镜像层并更新元数据，最终将构建好的镜像推送到镜像仓库。\n简单例子下面是一个使用kaniko的构建的简单例子\n创建密钥\nkubectl create secret generic -n blazehu kaniko-secret-common --from-file=config.json\n\n构建测试\napiVersion: v1kind: Podmetadata:  name: kanikospec:  containers:    - name: kaniko      image: m.daocloud.io/gcr.io/kaniko-project/executor:latest      args:        - &quot;--dockerfile=Dockerfile&quot;        - &quot;--context=git://user:password@github.com:blazehu/go-examples.git#master&quot;        - &quot;--destination=blazehu1122/example:latest&quot;      volumeMounts:        - name: kaniko-secret          mountPath: /kaniko/.docker/  restartPolicy: Never  volumes:    - name: kaniko-secret      secret:        secretName: kaniko-secret-common\n\n\n使用 kaniko-project&#x2F;executor:latest 镜像执行构建任务\n构建参数 –context: 上下文指定 Git Repository（仅支持 git:&#x2F;&#x2F;[repository url][#reference][#commit-id] 格式）\n构建参数 –destination: 指定配置的推送镜像的地址\n镜像推送挂载了 kaniko-secret 密钥\n\n构建新的CI镜像那我们如何结合蓝盾来实现Dind呢？我们需要重新制作一个新的蓝盾CI镜像，参考《构建并托管一个 CI 镜像 》，该CI镜像需要包括 kaniko 执行器。这里通过多阶段构建来制作新的CI镜像。\nFROM m.daocloud.io/gcr.io/kaniko-project/executor:latest as kanikoFROM bkci/ci:latest# 复制必要文件COPY --from=kaniko /kaniko /kanikoRUN chmod +x /kaniko/executorRUN apt install -y git python-pip python3-pip \\    &amp;&amp; pip config set global.index-url https://mirrors.aliyun.com/pypi/simple \\    &amp;&amp; pip config set install.trusted-host mirrors.aliyun.com# 设置环境变量ENV PATH $PATH:/kanikoENV DOCKER_CONFIG /kaniko/.dockerENV SSL_CERT_DIR /kaniko/ssl/certs# 验证 kaniko 可执行文件RUN /kaniko/executor version\n\n蓝盾流水线\n\n第一步使用蓝盾 Checkout 插件拉取代码\n第二步使用蓝盾 Shell Script 插件执行 kaniko 构建命令\n\nkaniko/executor --context=/data/devops/workspace --dockerfile=Dockerfile --destination=blazehu1122/example:latest --ignore-path=/ &quot;\n\nDind Unix Socket使用 DaemonSet 来启动 Dind Pod，将 Docker socket 文件 &#x2F;var&#x2F;run&#x2F;docker.sock 挂载到 Pod 中。在要使用Docker服务的 Pod 中都需要挂载 socket文件。\n简单例子apiVersion: apps/v1kind: DaemonSetmetadata:  name: dinp-daemonsetspec:  selector:    matchLabels:      name: dinp-daemonset  template:    metadata:      labels:        name: dinp-daemonset    spec:      containers:      - name: dind        image: docker:dind        securityContext:          privileged: true        volumeMounts:        - name: dockersock          mountPath: /var/run/docker.sock      volumes:      - name: dockersock        hostPath:          path: /var/run/docker.sock          type: Socket\n\n在这个配置中，&#x2F;var&#x2F;run&#x2F;docker.sock 被挂载到 Pod 中，允许 Pod 直接与宿主机上的 Docker 守护进程通信。这种方式不需要设置 DOCKER_HOST 环境变量，因为 Docker 客户端和守护进程直接通过 socket 文件通信。\nDind TCP定义一个 Deployment 和一个 Service，用于启动一个包含 Dind 的 Pod，并通过 Service 对外提供 Docker 服务。在要使用Docker服务的 Pod 中设置 DOCKER_HOST 环境变量，使得 Docker 客户端知道如何连接到 Docker 守护进程（比如在bkci的基础镜像中注入该环境变量）。\n简单例子apiVersion: apps/v1kind: Deploymentmetadata:  name: dinp-deployment  namespace: blueking  labels:    name: dinp-deploymentspec:  replicas: 1  selector:    matchLabels:      name: dinp-deployment  template:    metadata:      labels:        name: dinp-deployment    spec:      containers:      - name: dind        image: docker:dind        resources:          requests:            memory: &quot;4Gi&quot;            cpu: &quot;2&quot;          limits:            memory: &quot;8Gi&quot;            cpu: &quot;4&quot;        securityContext:          privileged: true        env:        - name: DOCKER_TLS_CERTDIR          value: &quot;&quot;        - name: DOCKER_HOST          value: tcp://localhost:2375        tty: true        volumeMounts:        - name: docker-storage          mountPath: /var/lib/docker        - name: docker-run          mountPath: /var/run        readinessProbe:          exec:            command: [&quot;docker&quot;, &quot;info&quot;]          initialDelaySeconds: 10          failureThreshold: 6        livenessProbe:          exec:            command: [&quot;docker&quot;, &quot;info&quot;]          initialDelaySeconds: 60          failureThreshold: 10      ## 污点配置      tolerations:      - key: &quot;svc&quot;        value: &quot;bk&quot;        operator: &quot;Equal&quot;        effect: &quot;NoSchedule&quot;      affinity:        podAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: &quot;app.kubernetes.io/name&quot;                operator: &quot;In&quot;                values:                - dockerhost            topologyKey: &quot;kubernetes.io/hostname&quot;      volumes:      - name: docker-storage        hostPath:          path: /var/lib/docker_in_pod      - name: docker-run        hostPath:          path: /blueking/run          type: DirectoryOrCreate---apiVersion: v1kind: Servicemetadata:  name: bk-ci-docker-dinp  namespace: bluekingspec:  selector:    name: dinp-deployment  ports:    - protocol: TCP      port: 2375      targetPort: 2375\n\n在需要使用 Docker 的 Pod 中设置 DOCKER_HOST 环境变量为 bk-ci-docker-dinp.blueking.svc.cluster.local，通过 Kubernetes Service 的域名解析和端口转发机制，使 Pod 内的 Docker 客户端能够连接到后端的 Docker 守护进程。\n蓝盾流水线\n第一步使用蓝盾 Checkout 插件拉取代码\n第二步使用蓝盾 Shell Script 插件执行 docker 构建命令\n\ndocker context create dind --docker &quot;host=tcp://bk-ci-docker-dinp.blueking.svc.cluster.local:2375,ca=/root/.docker/certs/ca.pem,cert=/root/.docker/certs/cert.pem,key=/root/.docker/certs/key.pem&quot; docker context use dinddocker build --platform=linux/amd64 -t $&#123;IMAGE_REPO&#125;:$&#123;IMAGE_TAG&#125; -f Dockerfile . --push\n\n技术选型对比\n\n\n特性&#x2F;方案\nKaniko\nDind Unix Socket\nDind TCP\n\n\n\n依赖环境\n不依赖 Docker 守护进程\n依赖宿主机 Docker Socket\n依赖宿主机 Docker 守护进程（TCP）\n\n\n部署复杂度\n简单，只需部署 Pod\n中等，需要配置 DaemonSet\n较复杂，需要配置 Deployment 和 Service\n\n\n资源消耗\n低\n中等\n较高\n\n\n安全性\n高\n中等\n中等\n\n\n适用场景\nKubernetes 环境\n单机或多节点集群\n跨节点或 Kubernetes 集群\n\n\n蓝盾集成难度\n中等\n低\n中等\n\n\n虽然我们最终选择了 Kaniko 方案，但在实际应用中发现，基于 m.daocloud.io&#x2F;gcr.io&#x2F;kaniko-project&#x2F;executor:latest 制作的蓝盾 CI 镜像存在一些兼容性问题。\n根据 Kaniko 的官方文档，这种做法并不被推荐，可能会导致一些不可预见的问题。\n后续将根据蓝盾的官方文档和 Kaniko 的最佳实践，建议重新制作 CI 镜像。\n参考\nhttps://blazehu.com/2025/06/27/devops/landun_dind/\nhttps://github.com/GoogleContainerTools/kaniko\nhttps://juejin.cn/post/7217665415710081081\nhttps://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/Services/Store/ci-images/docker-build.md\n\n","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"蓝盾流水线中的 Kubernetes 调度优化","url":"/2025/08/devops/blueking/lan-dun-liu-shui-xian-zhong-de-kubernetes-diao-du-you-hua/","content":"背景在前文蓝盾「Docker公共构建机」缓存清理中我们通过分析源码，知道拉起的构建 Pod 通过 hostPath 挂载工作目录做缓存。我们接下来进一步分析创建 Pod 的流程。\n部署配置dispatch-k8s-manager&#x2F;resources&#x2F;config.yaml\ndispatch:  # 调度需要使用到的label，确定构建机唯一性  label: bkci.dispatch.kubenetes/core  # 通过k8s watch来观察构建机状态  watch:    task:      label: bkci.dispatch.kubenetes/watch-task  builder:    # 将构建机调度到指定标签节点的配置，不填写则在集群内都可以调度，优先级小于专机和特殊机器    nodeSelector:      label:      value:    # 构建机曾经调度过的节点名称列表    nodesAnnotation: bkci.dispatch.kubenetes/builder-history-nodes    # 容器历史资源使用相关    realResource:      # 监控构建机容器资源使用的 prometheus api地址， 字段为空则不开启realResource优化      # 注：集群内为 集群内为 &lt;service&gt;.&lt;namespace&gt;.svc.cluster.local:&lt;port&gt;      prometheusUrl:       realResourceAnnotation: bkci.dispatch.kubenetes/builder-real-resources  # 一些具有特定属性的机器，例如独特的网络策略  specialMachine:    label: bkci.dispatch.kubenetes/special-builder  # 只给特定用户使用的专机  privateMachine:    label: bkci.dispatch.kubenetes/private-builder\n\n通过 dispatch-k8s-manager 模块的配置文件，我们发现可以通过 nodeSelector、 nodesAnnotation 、realResource 等配置来设置调度策略。\n源码分析亲和性和污点容忍dispatch-k8s-manager&#x2F;pkg&#x2F;apiserver&#x2F;service&#x2F;builder_start.go\nfunc CreateBuilder(builder *Builder) (taskId string, err error) &#123;  volumes, volumeMounts := getBuilderVolumeAndMount(builder.Name, builder.NFSs)  var replicas int32 = 1  tolers, nodeMatches := buildDedicatedBuilder(builder)    ...  annotations, err := getBuilderAnnotations(builder.Name)  if err != nil &#123;    return &quot;&quot;, err  &#125;  ...  go task.DoCreateBuilder(    taskId,    &amp;kubeclient.Deployment&#123;      Name:        builder.Name,      Labels:      labels,      MatchLabels: matchlabels,      Replicas:    &amp;replicas,      Pod: kubeclient.Pod&#123;        Labels:      labels,        Annotations: annotations,        Volumes:     volumes,        Containers: []kubeclient.Container&#123;          &#123;            Image:        builder.Image,            Resources:    *resources,            Env:          getEnvs(builder.Env),            Command:      builder.Command,            VolumeMounts: volumeMounts,          &#125;,        &#125;,        NodeMatches:     nodeMatches,        Tolerations:     tolers,        PullImageSecret: pullImageSecret,      &#125;,    &#125;,  )    return taskId, nil&#125;// buildDedicatedBuilder 获取污点和节点亲和度配置func buildDedicatedBuilder(builder *Builder) ([]corev1.Toleration, []kubeclient.NodeMatch) &#123;    // 优先读取专机配置    ...    // 读取具有特殊配置的机器    ...    // 如果配置中配置了节点选择器则使用节点选择器    ...    return nil, nil&#125;// getBuilderAnnotations 获取构建机注释配置func getBuilderAnnotations(builderName string) (map[string]string, error) &#123;  ...  // 获取节点记录，用来把构建机分配到已有的节点  ...  // 获取RealResource记录  ...  return result, nil&#125;\n\ndispatch-k8s-manager&#x2F;pkg&#x2F;kubeclient&#x2F;deployment.go\nfunc CreateDeployment(dep *Deployment) error &#123;  ...  // 将 NodeMatches 转为 nodeAffinity  var affinity *corev1.Affinity  if len(dep.Pod.NodeMatches) &gt; 0 &#123;    var matches []corev1.NodeSelectorRequirement    for _, mat := range dep.Pod.NodeMatches &#123;      matches = append(matches, corev1.NodeSelectorRequirement&#123;        Key:      mat.Key,        Operator: mat.Operator,        Values:   mat.Values,      &#125;)    &#125;    affinity = &amp;corev1.Affinity&#123;      NodeAffinity: &amp;corev1.NodeAffinity&#123;        RequiredDuringSchedulingIgnoredDuringExecution: &amp;corev1.NodeSelector&#123;          NodeSelectorTerms: []corev1.NodeSelectorTerm&#123;            &#123;              MatchExpressions: matches,            &#125;,          &#125;,        &#125;,      &#125;,    &#125;  &#125;  ...  return nil&#125;\n在 CreateBuilder 里，调度相关的两个核心参数 tolers 和 nodeMatches 都是通过 buildDedicatedBuilder(builder) 返回的，这两个参数会一起传递给 kubeclient 层，在 kubeclient 的 CreateDeployment 方法中：\n\nNodeMatches 会被转换为 affinity.nodeAffinity，用于节点亲和调度。\nTolerations 会直接下发到 Pod 的 spec.tolerations 字段，用于污点容忍。\n\n历史节点调度蓝盾源码里我们找到了有关亲和性以及污点容忍的实现，但是有关历史节点调度的实现只有通过 getBuilderAnnotations 给 Pod 设置注解。至于如何通过注解影响调度在蓝盾源码里并没有找到相关内容。\n我们进一步分析发现，历史节点调度需要通过蓝盾基于K8S调度插件实现。\napiVersion: v1kind: Podmetadata:  annotations:    bkci.dispatch.kubenetes/builder-history-nodes: &#x27;[&quot;10.x.x.1&quot;,&quot;10.x.x.2&quot;,&quot;10.x.x.3&quot;]&#x27;  labels:    bkci.dispatch.kubenetes/core: build1753761077695-ivcpmoxg    bkci.dispatch.kubenetes/watch-task: t-1753785688231121886-iInjpMUr-builder-start  name: build1753761077695-ivcpmoxg-c9d8fc6c9-mqhkk  ...\n\npackage bkdevopsschedulerpluginimport (    &quot;context&quot;    &quot;encoding/json&quot;    &quot;k8s.io/api/core/v1&quot;    &quot;k8s.io/kubernetes/pkg/scheduler/framework&quot;)const nodesAnnotation = &quot;bkci.dispatch.kubenetes/builder-history-nodes&quot;const readResourceAnnotation = &quot;bkci.dispatch.kubenetes/builder-real-resources&quot;type realResourceUsage struct &#123;    Cpu    string `json:&quot;cpu&quot;`    Memory string `json:&quot;memory&quot;`&#125;func (s *SchedulerPlugin) Score(_ context.Context, _ *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) &#123;    // 读取历史节点信息    var nodeHis []string    if nodesS, ok := pod.ObjectMeta.Annotations[nodesAnnotation]; ok &#123;        _ = json.Unmarshal([]byte(nodesS), &amp;nodeHis)    &#125;    // 读取资源信息    var realResources []realResourceUsage    if realS, ok := pod.ObjectMeta.Annotations[readResourceAnnotation]; ok &#123;        _ = json.Unmarshal([]byte(realS), &amp;realResources)    &#125;    // 计算历史节点分数    nodeScore := calculateNodeHisScore(nodeHis, nodeName)    // 计算资源分数    // ...省略资源分数计算逻辑...    realResourceScore := ... // 通过 realResources 和节点资源情况计算    // 返回总分    return nodeScore + realResourceScore, nil&#125;var nodeHisScores = map[int]int64&#123;0: 30, 1: 20, 2: 10&#125;// calculateNodeHisScore 计算历史节点分数，将3个历史节点从最近到最远依次打分 30 - 10分func calculateNodeHisScore(nodeHis []string, nodeName string) int64 &#123;  if len(nodeHis) == 0 &#123;    return framework.MinNodeScore  &#125;  for index, name := range nodeHis &#123;    if name != nodeName &#123;      continue    &#125;    score := framework.MinNodeScore    if indexS, ok := nodeHisScores[index]; ok &#123;      score = indexS    &#125;    return score  &#125;  return framework.MinNodeScore&#125;\n\n\n在插件的 Score 阶段，会读取 Pod 的 bkci.dispatch.kubenetes&#x2F;builder-history-nodes 注解内容，并将其反序列化为历史节点名称数组，即提供历史节点信息。\n插件通过 calculateNodeHisScore 方法，根据当前调度节点是否在历史节点列表中，以及其在列表中的顺序，给予不同的分数（最近的历史节点分数最高）。\n该分数会与资源分数（通过 bkci.dispatch.kubenetes&#x2F;builder-real-resources 注解和节点资源情况计算得出）相加，作为最终调度优先级，影响调度器选择节点的排序。\n\n总结在蓝盾流水线中，通过以下方式实现了 Kubernetes 的调度优化：\n\n历史节点调度：通过注解记录历史节点信息，调度插件优先选择这些节点，减少初始化时间。\n亲和性（Affinity）：根据配置文件中的 nodeSelector 和代码中的 NodeMatches 转换为 nodeAffinity，确保 Pod 调度到特定节点。\n污点容忍（Tolerations）：仅在配置文件中指定了专机（privateMachine）时，生成污点容忍配置，允许 Pod 调度到带特定污点的节点。\n\n这些机制协同提升了调度效率和资源利用率。\n参考\nhttps://github.com/TencentBlueKing/bk-ci/blob/v2.0.0/\nhttps://github.com/TencentBlueKing/ci-dispatch-k8s-manager-plugin\nhttps://blazehu.com/2025/07/30/devops/landun_dispatch_scheduler/\n\n","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"蓝盾「Docker公共构建机」全链路源码解析","url":"/2025/08/devops/blueking/lan-dun-docker-gong-gong-gou-jian-ji-quan-lian-lu-yuan-ma-jie-xi/","content":"背景本文以蓝盾社区版7.1为例，结合实际源码和配置，详细梳理从前端点击「执行」到最终在 Kubernetes 集群拉起 Deployment 的全链路调用过程，\n目录结构bk-ci/src├── gateway          # OpenResty 网关（Lua）├── frontend         # Vue 前端，模块级微前端├── backend          # Kotlin + SpringCloud 微服务│   ├── process      # 流水线引擎│   ├── dispatch     # 构建调度（Docker &amp; K8s）│   └── ...├── agent            # Go 语言 Agent└── pipeline-plugin  # Java 插件 SDK\n\n源码拆解前端触发页面地址：https://devops.bk.tencent.com/console/pipeline/{projectId}/{pipelineId}/preview\n\n\n\n事件&#x2F;方法\n对应后端接口\n作用\n\n\n\nrequestStartupInfo\nGET &#x2F;ms&#x2F;process&#x2F;api&#x2F;user&#x2F;builds&#x2F;{p}&#x2F;{pl}&#x2F;manualStartupInfo\n获取流水线启动所需参数\n\n\nexecutePipeline\nPOST &#x2F;ms&#x2F;process&#x2F;api&#x2F;user&#x2F;builds&#x2F;{p}&#x2F;{pl}\n真正触发流水线执行\n\n\n\n通过全局事件总线 bus 通信，以及具名视图（named views）机制实现页面拆分和组合。在 preview.vue 页面监听 executePipeline 事件，然后在 PreviewHeader.vue 中通过事件总线触发执行。\n\n网关转发&#x2F;ms&#x2F;process&#x2F;api&#x2F;user&#x2F;builds&#x2F;… 统一转发到 process 微服务。\nlocation /ms/process/ &#123;    proxy_pass http://process/;&#125;\n\nProcess 服务：流水线启动主链路主要方法调用链如下：\nUserBuildResource.manualStartup()                // 接收启动流水线的请求    ↓ServiceBuildResourceImpl.manualStartup()         // 具体实现，做参数校验、权限校验等    ↓PipelineBuildFacadeService.buildManualStartup()  // 负责组装启动参数、调用核心服务    ↓PipelineBuildService.startPipeline()             // 启动流水线主流程，负责流水线状态流转、记录等    ↓PipelineRuntimeService.startBuild()              // 流水线引擎，解析模型，调度 Stage/Job/Container，准备构建任务\n这一阶段主要负责接收前端的启动请求，经过参数校验、权限校验后，组装启动参数，最终进入流水线引擎。流水线引擎会解析流水线的模型（YAML&#x2F;DSL），为后续的调度和任务准备做铺垫。\n生成并下发构建任务主要方法调用链如下：\nPipelineContainerService.prepareBuildContainerTasks()      // 遍历流水线模型，为每个 Job/Container 生成任务，判断分发类型    ↓VmOperateTaskGenerator.makeStartVMContainerTask()          // 针对容器构建机，生成 VM 启动任务（taskAtom = &quot;dispatchVMStartupTaskAtom&quot;）    ↓pipelineEventDispatcher.dispatch(PipelineBuildStartEvent()) // 下发流水线启动事件\n\n此阶段会遍历流水线模型中的每个 Job&#x2F;Container，根据其类型（如容器构建机）生成对应的任务。对于容器构建机，会生成 VM 启动任务，并通过事件分发器下发流水线启动事件，为后续的事件驱动调度做准备。\n事件驱动：Stage&#x2F;Container&#x2F;Task 调度主要方法调用链如下：\nPipelineBuildStartListener.run(event)                      // 消费 PipelineBuildStartEvent，驱动流水线调度    ↓BuildStartControl.handle(event)    ↓PipelineBuildStartEvent.execute(watcher)    ↓buildModel()    ↓pipelineEventDispatcher.dispatch(PipelineBuildStageEvent()) // 下发 Stage 事件PipelineStageBuildListener.run(event)                      // 消费 PipelineBuildStageEvent    ↓StageControl.handle(event)    ↓PipelineBuildStageEvent.execute(watcher)    ↓pipelineContainerService.listContainers(...)               // 遍历当前 Stage 下所有 Container（Job）    ↓pipelineEventDispatcher.dispatch(PipelineBuildContainerEvent()) // 为每个 Job 下发事件PipelineContainerBuildListener.run(event)                  // 消费 PipelineBuildContainerEvent    ↓ContainerControl.handle(event)    ↓ContainerCmdChain.doCommand(context)                       // 命令链执行，关键命令 StartActionTaskContainerCmd    ↓pipelineEventDispatcher.dispatch(PipelineBuildAtomTaskEvent()) // 下发插件任务事件\n蓝盾采用事件驱动架构，每个阶段（Stage）、每个 Job（Container）、每个插件（Task）都通过事件进行调度。每个事件都有对应的 Listener 消费，逐步推进流水线的执行流程，保证了系统的高解耦和可扩展性。\n插件任务调度与 VM 启动主要方法调用链如下：\nPipelineAtomTaskBuildListener.run(event)                   // 消费 PipelineBuildAtomTaskEvent    ↓TaskControl.handle(event)    ↓taskAtomService.start(buildTask)    ↓SpringContextUtil.getBean(IAtomTask::class.java, task.taskAtom).execute(task, runVariables)    ↓DispatchVMStartupTaskAtom.execute()                        // 对于 VM 启动任务，加载并执行    ↓dispatch()    ↓getDispatchType()                                          // 返回 DockerDispatchType（社区版容器构建机默认）    ↓pipelineEventDispatcher.dispatch(PipelineAgentStartupEvent()) // 下发分发事件\n\n每个插件任务（Atom）都会被动态加载并执行。对于 VM 启动任务，会加载 DispatchVMStartupTaskAtom 插件，判断分发类型（如 Docker），并下发 PipelineAgentStartupEvent，为后续的构建机分发做准备。\ndispatch-docker 服务：分发到 k8s主要方法调用链如下：\nDockerVMListener.onStartup(dispatchMessage)                // 消费 PipelineAgentStartupEvent    ↓getDockerRoutingType(projectId)                            // 判断路由类型（如 configmap 配置为 &quot;KUBERNETES&quot;）    ↓startKubernetesDocker(...)                                 // 路由类型为 KUBERNETES 时，走 k8s 资源池    ↓DispatchBuildService.startUp()    ↓createAndStartNewBuilder()    ↓containerServiceFactory.load(projectId).createAndStartBuilder()    ↓KubernetesContainerService.createAndStartBuilder()    ↓kubernetesBuilderClient.createBuilder()                    // HTTP POST /api/builders 调用 dispatch-k8s-manager\ndispatch-docker 服务会根据项目的路由配置，决定是走本地 Docker 还是 k8s 资源池。若配置为 KUBERNETES，则会通过 HTTP 请求调用 dispatch-k8s-manager 服务，准备在 k8s 集群中拉起构建容器。\n\n查看 bk-ci-bk-ci-dispatch-docker 这个 configmap，可以发现配置文件里的 defaultDockerRoutingType 是 “KUBERNETES”。\n\ndispatch-k8s-manager 服务：拉起 k8s Deployment主要方法调用链如下：\nPOST /api/builders                                         // 路由    ↓createBuilder handler    ↓service.CreateBuilder    ↓task.DoCreateBuilder    ↓kubeclient.CreateDeployment(dep)                           // 通过 k8s API 创建 Deployment，拉起实际的构建容器\n\ndispatch-k8s-manager 服务负责与 Kubernetes API 交互，接收来自 dispatch-docker 的 HTTP 请求后，组装 Deployment 对象并调用 k8s API，最终在集群中拉起实际的构建容器，完成流水线的环境准备。\n总结时序图\n总结\n\n\n阶段\n关键技术点\n一句话描述\n\n\n\n前端\nVue + Event Bus\n点击按钮 → 事件总线 → 请求发出\n\n\n网关\nOpenResty 前缀转发\n统一入口，&#x2F;ms&#x2F;process&#x2F;** 直接透传至 process 服务。\n\n\n流程引擎\n自研事件-命令链框架\nPipelineBuildStart → Stage → Container → Task → AgentStartup，层层事件推进，高内聚低耦合。\n\n\n插件\nIAtomTask SPI 机制\n运行时动态加载 DispatchVMStartupTaskAtom，扩展即插即用。\n\n\n调度\ndispatch-docker → dispatch-kubernetes\n根据 defaultDockerRoutingType&#x3D;KUBERNETES 路由到对应资源池。\n\n\nK8s 交付\ndispatch-k8s-manager 与 kube-apiserver 交互\n一条 HTTP 请求即可在集群内拉起 Deployment，数秒完成环境就绪。\n\n\n参考\nhttps://github.com/TencentBlueKing/bk-ci/tree/v2.0.0\nhttps://blazehu.com/2025/07/18/devops/landun_dispatch/\n\n","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"蓝盾「Docker公共构建机」缓存清理","url":"/2025/08/devops/blueking/lan-dun-docker-gong-gong-gou-jian-ji-huan-cun-qing-li/","content":"背景在使用蓝盾「Docker公共构建机」一段时间后，我们发现构建镜像偶发性超时。排查后发现是由于集群的 Node 节点的磁盘满了，本文会介绍如何清理构建缓存。\n我们发现构建镜像偶发性超时，排查发现是上了 Docker-in-Docker 构建镜像之后发生的，而且发生频率越来越高，进一步排查发现是由于 Pod 会通过 hostPath 挂载工作目录和日志目录，由于构建任务过多导致 Node 节点磁盘打满。\n排查过程事件分析通过 Pod 事件可以发现是由于 Node 节点磁盘打满，导致 Pod 被驱逐，构建任务失败。\nEvents:  Type     Reason                 Age                 From     Message  ----     ------                 ----                ----     -------  Warning  Evicted                20m                 kubelet  The node was low on resource: ephemeral-storage. Container build1753761077695-ivcpmoxg was using 1580320Ki, which exceeds its request of 0.  Normal   NodeHasNoDiskPressure  3m (x32 over 6d5h)  kubelet  Node 10.10.32.2 status is now: NodeHasNoDiskPressure\n\npod yaml: \nvolumes:- hostPath:    path: /data/landun/workspace/build1753761077695-ivcpmoxg    type: &quot;&quot;  name: data-volume- hostPath:    path: /data/landun/logs/build1753761077695-ivcpmoxg    type: &quot;&quot;  name: logs-volume\n是由于 Pod 通过 hostPath 挂载工作目录和日志目录，通过 hostPath 挂载目录是为了做缓存，当同一流水线任务重复执行时能够加速。\ndispatch-k8s-manager 模块的配置文件: dispatch-k8s-manager&#x2F;resources&#x2F;config.yaml\ndispatch:  volume:    # 构建机脚本    builderConfigMap:      name: dispatch-kubernetes-builder      items:        # 初始化脚本        - key: initsh.properties          path: init.sh        # 登录调试需要的sleep脚本        - key: sleepsh.properties          path: sleep.sh    hostPath:      # 数据盘      dataHostDir: /data/landun/workspace      # 日志盘      logsHostDir: /data/landun/logs    # 应用数据使用cfs    cfs:      path: /data/cfs  volumeMount:    dataPath: /data/landun/workspace    logPath: /data/logs    builderConfigMapPath: /data/landun/config    cfs:      path: /data/bkdevops/apps      readOnly: true\n\n源码分析dispatch-k8s-manager&#x2F;pkg&#x2F;apiserver&#x2F;service&#x2F;builder_start.go\n// getBuilderVolumeAndMount 获取一些构建机的常规的被挂载到pod上的volume和mountfunc getBuilderVolumeAndMount(  workloadName string,  nFSs []types.NFS,) (volumes []corev1.Volume, volumeMounts []corev1.VolumeMount) &#123;  volumes = getBuilderPodVolume(workloadName)  volumeMounts = getBuilderPodVolumeMount()  ...  return volumes, volumeMounts&#125;// getBuilderPodVolume 获取一些构建机的常规的被挂载到pod上的volume，包括配置configmap和data目录hostpathfunc getBuilderPodVolume(workloadName string) []corev1.Volume &#123;  dataHostPath := filepath.Join(config.Config.Dispatch.Volume.HostPath.DataHostDir, workloadName)  logHostPath := filepath.Join(config.Config.Dispatch.Volume.HostPath.LogsHostDir, workloadName)  var items []corev1.KeyToPath  for _, v := range config.Config.Dispatch.Volume.BuilderConfigMap.Items &#123;    items = append(items, corev1.KeyToPath&#123;      Key:  v.Key,      Path: v.Path,    &#125;)  &#125;  return ...&#125;\n通过源码分析可以发现 hostPath 是通过 dispatch-k8s-manager&#x2F;resources&#x2F;config.yaml 加上 workloadName 拼接而成的，所以没办法通过配置文件控制不使用 hostPath，于是我们通过定时任务来清理该缓存。\n解决方案参考 bk-applog-bkapp-filebeat 的日志清理方案，通过 DaemonSet 实现蓝盾挂载工作目录实施定时清理操作。\nNAME                                   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGEbk-applog-bkapp-filebeat-ingress       18        18        18      18           18          &lt;none&gt;          424dbk-applog-bkapp-filebeat-json          18        18        18      18           18          &lt;none&gt;          424dbk-applog-bkapp-filebeat-log-cleaner   18        18        18      18           18          &lt;none&gt;          424dbk-applog-bkapp-filebeat-stdout        18        18        18      18           18          &lt;none&gt;          424dbk-ci-builder-cleaner                  18        18        18      18           18          &lt;none&gt;          13d\n\n编写 daemonSet.yaml\napiVersion: apps/v1kind: DaemonSetmetadata:  name: bk-ci-builder-cleaner  namespace: blueking  labels:    app: bk-ci-builderspec:  revisionHistoryLimit: 10  selector:    matchLabels:      app: bk-ci-builder  template:    metadata:      labels:        app: bk-ci-builder      name: bk-ci-builder-cleaner    spec:      hostPID: true      restartPolicy: Always      serviceAccountName: bk-applog-bkapp-filebeat      containers:      - name: batch-delete-files        image: xxx.xxx.com/bk-ci-builder-cleaner:v1        imagePullPolicy: IfNotPresent        command:        - bash        args:        - -c        - while true; do ./delete_files.sh; sleep 21600; done;        resources:          requests:            cpu: 25m            memory: 32Mi          limits:            cpu: 2560m            memory: 256Mi        volumeMounts:        - mountPath: /data/devops/workspace          name: data-volume        - mountPath: /data/devops/logs          name: logs-volume      volumes:      - name: data-volume        hostPath:          path: /data/landun/workspace          type: DirectoryOrCreate      - name: logs-volume        hostPath:          path: /data/landun/logs          type: DirectoryOrCreate\n\n缓存清理脚本 delete_files.sh\n#!/usr/bin/env bash# delete_files.sh  —— 正式删除版# 同时扫描 /data/devops/workspace 和 /data/devops/logsset -euo pipefail# --------- 可配置参数 ---------ROOT_DIRS=(&quot;/data/devops/workspace&quot; &quot;/data/devops/logs&quot;)RETENTION_DAYS=7LOG_FILE=&quot;/tmp/delete_build_dirs.log&quot;# -----------------------------log() &#123;  printf &#x27;%s [%s] %s\\n&#x27; &quot;$(date &#x27;+%F %T&#x27;)&quot; &quot;$1&quot; &quot;$2&quot; | tee -a &quot;$LOG_FILE&quot;&#125;cutoff_date=$(date -d &quot;$RETENTION_DAYS days ago&quot; +%F)log INFO &quot;==== 开始检查并删除 $RETENTION_DAYS 天未更新的 build* 目录 ====&quot;for root in &quot;$&#123;ROOT_DIRS[@]&#125;&quot;; do  [[ -d $root ]] || &#123; log WARN &quot;目录不存在: $root&quot;; continue; &#125;  for dir in &quot;$root&quot;/build*; do    [[ -d $dir ]] || continue    # 二次确认：目录内是否仍无任何 7 天内更新的文件    if ! find &quot;$dir&quot; -type f -newermt &quot;$cutoff_date&quot; -print -quit | grep -q .; then        log DELETE &quot;$dir&quot;        rm -rf &quot;$dir&quot;    else        log SKIP &quot;$dir&quot;    fi  donedonelog INFO &quot;==== 清理完成，日志: $LOG_FILE ====&quot;\n\n参考https://github.com/TencentBlueKing/bk-ci/blob/v2.0.0/https://blazehu.com/2025/07/17/devops/landun_dind_cleaner/\n","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"数据库迁移","url":"/2025/08/devops/blueking/shu-ju-ku-qian-yi/","content":"数据备份备份脚本准备#!/bin/bashMYSQL_USER=rootMYSQL_HOST=127.0.0.1MYSQL_PASSWD=ignoredblist=&#x27;information_schema|mysql|test|db_infobase|performance_schema|sys&#x27;dblist=&quot;$(mysql -h$MYSQL_HOST -u$MYSQL_USER -p$MYSQL_PASSWD -Nse&quot;show databases;&quot;|grep -Ewv &quot;$ignoredblist&quot; | xargs echo)&quot;mysqldump -h$MYSQL_HOST -u$MYSQL_USER -p$MYSQL_PASSWD --skip-opt --create-options --default-character-set=utf8mb4 -R  -E -q -e --single-transaction --no-autocommit --max-allowed-packet=1G  --hex-blob -B $dblist &gt; /tmp/bk_mysql_alldata.sql\n\n将其中的MYSQL_USER、MYSQL_HOST、MYSQL_PASSWORD更换成需要备份的数据库及用户名密码。存为 dbbackup_mysql.sh 文件。\n将脚本拷贝到容器内执行以自建的蓝盾mysql为例\n# 将上面的数据备份脚本拷贝至自建的蓝盾mysql的pod中kubectl cp -n blueking /data/dbbackup_mysql.sh bk-ci-mysql-0:/tmp/dbbackup_mysql.sh# 开始执行数据备份kubectl exec -it -n blueking bk-mysql-mysql-master-0 -- bash /tmp/dbbackup_mysql.sh# 将备份好的sql从pod拷贝到本机暂存kubectl cp -n blueking bk-ci-mysql-0:/tmp/bk_mysql_alldata.sql /data/bkmysql_bak/bk_mysql_alldata.sql\n\n最后一步数据拷贝可以不做，直接在这个pod里进行后续的数据导入操作。\n数据导入上一步的数据备份是用root用户进行操作，备份中涉及存储过程函数和赋权。\n\n注意：如果导入到阿里云mysql服务中需要处理导出的sql文件，因为阿里云提供的mysql服务，root作为保留字段，不能由用户自由创建，但允许创建拥有root权限的账户，所以需要修改备份数据中相关的内容，更换成实际使用的数据库用以替换成 superuser 为例执行如下命令：\n\n# 统计&quot;`root`@&quot;字符串一共有多少个grep -o &#x27;`root`@&#x27; bk_mysql_alldata.sql | wc -l# 将&quot;`root`@&quot; 修改为 &quot;`superuser`@&quot;sed -i &#x27;s/`root`@/`superuser`@/g&#x27; bk_mysql_alldata.sql# 二次确认grep -o &#x27;`superuser`@&#x27; bk_mysql_alldata.sql | wc -l\n\n导入mysql -h $NEW_MYSQL_HOST -usuperuser -p$YOUR_PASSWORD --force &lt; bk_mysql_alldata.sql\n\n检查mysql -h$MYSQL_HOST -usuperuser -p$MYSQL_PASSWD -Nse&quot;show databases like &#x27;devops_ci%&#x27;;&quot;\n\nHelmfile更新变更 bkci\\environments\\default\\bkci\\bkci-custom-values.yaml.gotmpl 文件后执行 helmfile 相关命令更新服务。\n","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"生产部署文档(v7-1)","url":"/2025/08/devops/blueking/sheng-chan-bu-shu-wen-dang-v7-1/","content":"1. 概述需要先准备一台中控机，在中控机安装 kubectl、helm、helmfile 等工具，以及蓝鲸安装脚本。然后部署基础套餐，最后再部署持续集成套餐（蓝盾）。\n简单来说就是三个步骤：\n\n1.准备环境 \n2.部署基础服务 \n3.部署蓝盾\n\n2. 准备中控机按照官方文档安装和配置即可。\n3. 部署基础服务需要按照官方文档一步步部署。\n3.1 下载安装文件请在 中控机 使用下载脚本下载蓝鲸 helmfile 包及公共证书。（ helmfile相关value文件在git上维护）\nbkdl-7.1-stable.sh -ur latest base demo\n\n这些文件默认放在了 ~&#x2F;bkce7.1-install&#x2F; 目录。\n3.2 配置 Helm Chart 仓库helm repo add blueking https://hub.bktencent.com/chartrepo/bluekinghelm repo updatehelm repo list\n\n3.3 配置全局 custom-values相关文件已经修改，在git上维护，配置访问域名。\nBK_DOMAIN=bk.blazehu.com  # 请修改为你分配给蓝鲸平台的主域名 cd ~/bkce7.1-install/blueking/  # 进入工作目录# 可使用如下命令添加域名。如果文件已存在，请手动编辑。custom=environments/default/custom.yamlcat &gt;&gt; &quot;$custom&quot; &lt;&lt;EOFimageRegistry: $&#123;REGISTRY:-hub.bktencent.com&#125;domain:  bkDomain: $BK_DOMAIN  bkMainSiteDomain: $BK_DOMAINEOF\n\n3.4 生成 values 文件还有一些 values 文件随着部署环境的不同而变化，所以我们提供了脚本快速生成。\n生成蓝鲸 app code 对应的 secret\n./scripts/generate_app_secret.sh ./environments/default/app_secret.yaml\n\n生成 apigw 所需的 keypair\n./scripts/generate_rsa_keypair.sh ./environments/default/bkapigateway_builtin_keypair.yaml\n\n生成 paas 所需的 clusterAdmin\n./scripts/create_k8s_cluster_admin_for_paas3.sh\n\n3.5 安装入口网关3.5.1 安装 ingress controller先检查你的环境是否已经部署了 ingress controller:\nkubectl get pods -A -l app.kubernetes.io/name=ingress-nginx\n\n如果没有，则使用如下命令创建：\nhelmfile -f 00-ingress-nginx.yaml.gotmpl synckubectl get pods -A -l app.kubernetes.io/name=ingress-nginx  查看创建的pod\n\npops集群相关标签如下：\nkubectl get pods -A -l app=ingress-nginx  # 查看创建的podIP1=$(kubectl get svc -A -l app=nginx-ingress-lb -o jsonpath=&#x27;&#123;.items[0].status.loadBalancer.ingress[0].ip&#125;&#x27;)# IP1=$(kubectl get svc -A -l app.kubernetes.io/name=ingress-nginx -o jsonpath=&#x27;&#123;.items[0].status.loadBalancer.ingress[0].ip&#125;&#x27;)\n\n3.5.2 配置 coredns在部署过程中，会在容器内访问这些域名，所以需要提前配置 coredns，将蓝鲸域名解析到 service IP。\n\n注意: 当 service 被删除，重建后 clusterIP 会变动，此时需刷新 hosts 文件。\n\n因此需要注入 hosts 配置项到 kube-system namespace 下的 coredns 系列 pod\ncd ~/bkce7.1-install/blueking/  # 进入工作目录BK_DOMAIN=$(yq e &#x27;.domain.bkDomain&#x27; environments/default/custom.yaml)  # 从自定义配置中提取, 也可自行赋值#IP1=$(kubectl get svc -A -l app.kubernetes.io/instance=ingress-nginx -o jsonpath=&#x27;&#123;.items[0].spec.clusterIP&#125;&#x27;)IP1=$(kubectl get svc -A -l app=nginx-ingress-lb -o jsonpath=&#x27;&#123;.items[0].status.loadBalancer.ingress[0].ip&#125;&#x27;)./scripts/control_coredns.sh update &quot;$IP1&quot; $BK_DOMAIN bkrepo.$BK_DOMAIN docker.$BK_DOMAIN bkapi.$BK_DOMAIN bkpaas.$BK_DOMAIN bkiam-api.$BK_DOMAIN bkiam.$BK_DOMAIN apps.$BK_DOMAIN bknodeman.$BK_DOMAIN job.$BK_DOMAIN jobapi.$BK_DOMAIN./scripts/control_coredns.sh update &quot;$IP1&quot; devops.$BK_DOMAIN./scripts/control_coredns.sh list  # 检查添加的记录。\n\n确认注入结果，执行如下命令：\ncd ~/bkce7.1-install/blueking/  进入工作目录./scripts/control_coredns.sh list\n\n参考输出如下：\n172.27.124.109 bkce.diezhi.net172.27.124.109 bkrepo.bkce.diezhi.net172.27.124.109 docker.bkce.diezhi.net172.27.124.109 bkapi.bkce.diezhi.net172.27.124.109 bkpaas.bkce.diezhi.net172.27.124.109 bkiam-api.bkce.diezhi.net172.27.124.109 bkiam.bkce.diezhi.net172.27.124.109 apps.bkce.diezhi.net172.27.124.109 bknodeman.bkce.diezhi.net172.27.124.109 job.bkce.diezhi.net172.27.124.109 jobapi.bkce.diezhi.net172.27.124.109 devops.bkce.diezhi.net\n\n3.6 部署或对接存储服务3.6.1 部署蓝鲸预置的存储服务参考官方文档安装，相关helm配置已经放在gitlab仓库上维护，可以直接简单执行以下命令：\nhelmfile -f base-storage.yaml.gotmpl sync\n\n3.6.2 对接已有的存储服务禁用蓝鲸内置服务，配置使用已有服务。helmfile 定义及 values 文件已经放在gitlab仓库上维护。\n此处可直接跳过。\n3.7 部署基础套餐通过helmfile安装 base-blueking.yaml.gotmpl ，按照顺序依次安装。具体每层安装的内容可以查看文件内容。参考官方文档安装，相关helm配置已经放在gitlab仓库上维护，可以直接简单执行以下命令：\nhelmfile -f base-blueking.yaml.gotmpl -l seq=first synchelmfile -f base-blueking.yaml.gotmpl -l seq=second synchelmfile -f base-blueking.yaml.gotmpl -l seq=third sync# helmfile -f base-blueking.yaml.gotmpl -l seq=fourth sync\n\n3.8 访问蓝鲸桌面在负载均衡器配置后端为 ingress-nginx pod 所在机器的内网 IP，端口为 80。详细信息参考文档。\n3.8.1 查找ingress nginx svc找到ingress nginx的svc\n# popskubectl get svc -n kube-system|grep ingress# pops-devkubectl get svc -n ingress-nginx|grep ingress\n\n结果：\n# popsingress-nginx-controller-admission        ClusterIP      172.26.10.30    &lt;none&gt;          443/TCP                        3y69dnginx-ingress-lb                          LoadBalancer   172.26.1.155    10.212.14.158   80:30725/TCP,443:31357/TCP     3y69d# pops-devingress-nginx-controller             NodePort    172.27.124.109   &lt;none&gt;        80:32080/TCP,443:32443/TCP   279dingress-nginx-controller-admission   ClusterIP   172.27.123.38    &lt;none&gt;        443/TCP                      279dingress-nginx-controller-metrics     ClusterIP   172.27.116.175   &lt;none&gt;        10254/TCP                    279d\n\n这里nginx-ingress-lb是目前pops集群的ingress nginx svc，而ingress-nginx-controller是目前pops-dev集群的ingress nginx svc。\n3.8.2 设置svc type为LoadBalancer查看svc type：\n# 查看pops集群ingress nginx svc命令kubectl get svc/nginx-ingress-lb -n kube-system -oyaml|grep type# 查看pops-dev集群ingress nginx svc命令kubectl get svc/ingress-nginx-controller -n ingress-nginx -oyaml|grep type\n\n若无type: LoadBalancer结果，则手动进行修改：\n# 修改pops集群ingress nginx svc命令kubectl edit svc/nginx-ingress-lb -n kube-system# 修改pops-dev集群ingress nginx svc命令kubectl edit svc/ingress-nginx-controller -n ingress-nginx\n\n3.8.3 负载均衡clb实例查看svc yaml是否包含两个重要annotation：\n# 查看pops集群ingress nginx svc命令kubectl get svc/nginx-ingress-lb -n kube-system -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-force-override-listenerskubectl get svc/nginx-ingress-lb -n kube-system -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id# 查看pops-dev集群ingress nginx svc命令kubectl get svc/ingress-nginx-controller -n ingress-nginx -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-force-override-listenerskubectl get svc/ingress-nginx-controller -n ingress-nginx -oyaml|grep service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id\n\n若这两个重要annotation缺失，则需进行设置。\n负载均衡控制台地址：https://slb.console.aliyun.com/overview\n找到pops-dev集群的clb：pops-k8s-dev-ingress\n找到pops集群的clb：k8s-pops-ingress-slb\n# 修改pops集群ingress nginx svc命令# pops集群clb名称：k8s-pops-ingress-slb# pops集群clb id：lb-bp1tt6vxctuzi38mqfkw0kubectl edit svc/nginx-ingress-lb -n kube-system# 修改pops-dev集群ingress nginx svc命令# pops-dev集群clb名称：pops-k8s-dev-ingress# pops-dev集群clb id：lb-bp1r1dsywnqktp1hxih38kubectl edit svc/ingress-nginx-controller -n ingress-nginx\n\n3.8.4 访问地址查看浏览器访问地址：\ncd ~/bkce7.1-install/blueking/  # 进入工作目录# 从自定义配置中提取BK_DOMAIN=$(yq e &#x27;.domain.bkDomain&#x27; environments/default/custom.yaml)  # 普通登录地址(接入统一登录，且登录成功后会跳转蓝盾)echo &quot;http://$BK_DOMAIN&quot;# admin登录地址(接入统一登录后，可通过此方法适用admin登录)echo &quot;http://$&#123;BK_DOMAIN&#125;/login/origin/&quot;# 测试环境一般结果为：http://bkce.diezhi.net/login/origin/# 生产环境一般结果为：https://bk.diezhi.net/login/origin/# 查看用户密码(预设原始密码)：kubectl get cm -n blueking bk-user-api-general-envs -o go-template=&#x27;user=&#123;&#123;.data.INITIAL_ADMIN_USERNAME&#125;&#125;&#123;&#123;&quot;\\n&quot;&#125;&#125;password=&#123;&#123; .data.INITIAL_ADMIN_PASSWORD &#125;&#125;&#123;&#123;&quot;\\n&quot;&#125;&#125;&#x27;\n\n3.9 对接Ldap服务在用户中心里配置Ldap相关配置，然后更新 bk-user-api-web 服务的镜像。\n4. 部署蓝盾参考官方文档部署，配置 custom values 的内容提前修改完成，执行类似部署基础服务的以下命令：\ncd ~/bkce7.1-install/blueking/  # 进入工作目录helmfile -f 03-bkci.yaml.gotmpl sync  # 部署helmfile -f 03-bkci.yaml.gotmpl apply # 更新\n剩下的步骤参考官方文档执行即可，主要步骤有以下三个，其他的步骤可以不做。\n4.1 [可选]注册默认构建镜像我们提供了 bkci&#x2F;ci 镜像用于提供构建环境。为了加速镜像下载过程，可以修改镜像地址为 hub.bktencent.com&#x2F;bkci&#x2F;ci，或者为你自己托管的内网 registry。\nkubectl exec -it -n blueking bk-ci-mysql-0 -- /bin/bash -c &#x27;MYSQL_PWD=&quot;$MYSQL_ROOT_PASSWORD&quot; mysql -u root -e &quot;USE devops_ci_store; SELECT IMAGE_NAME,IMAGE_CODE,IMAGE_REPO_NAME FROM T_IMAGE WHERE IMAGE_CODE = \\&quot;bkci\\&quot; ;&quot;&#x27;\n\n请根据结果进行操作：\n\n如果有显示镜像数据，可以修改镜像地址为蓝鲸国内仓库，也可改为你已经缓存在内网的镜像：\nkubectl exec -it -n blueking bk-ci-mysql-0 -- /bin/bash -c &#x27;MYSQL_PWD=&quot;$MYSQL_ROOT_PASSWORD&quot; mysql -u root -e &quot;USE devops_ci_store; UPDATE  T_IMAGE SET IMAGE_REPO_NAME=\\&quot;hub.bktencent.com/bkci/ci\\&quot; WHERE IMAGE_CODE = \\&quot;bkci\\&quot; ;&quot;&#x27;\n\n然后重新查询数据库，可以看到 IMAGE_REPO_NAME 列已经更新。\n\n如果没有镜像，可以新增：\nkubectl exec -n blueking deploy/bk-ci-bk-ci-store -- \\curl -vs http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/market/image/init -X POST \\-H &#x27;X-DEVOPS-UID: admin&#x27; -H &#x27;Content-type: application/json&#x27; -d &#x27;&#123;&quot;imageCode&quot;:&quot;bkci&quot;,&quot;imageName&quot;:&quot;bkci&quot;,&quot;imageRepo&quot;:&quot;hub.bktencent.com/bkci/ci&quot;,&quot;projectCode&quot;:&quot;demo&quot;,&quot;userId&quot;:&quot;admin&quot;&#125;&#x27; | jq .\n\n\n\n提示\n当你单独卸载蓝盾重装后，可能出现查询镜像为空，但是新增镜像时报错 { status: 400, message: “权限中心创建项目失败” } 的情况。这是因为权限中心存在蓝盾 demo 项目的数据所致，我们后续会优化蓝盾单独卸载的文档。请先手动新建项目，并修改上述代码中 projectCode 字段的值。\n\n\n4.1.1 解决权限中心创建项目失败在 中控机 执行：\ncd ~/bkce7.1-install/blueking/  # 进入工作目录./scripts/control_coredns.sh list  # 检查添加的记录。\n\n会得到类似的结果：\n172.27.124.109 devops.bkce.diezhi.net172.27.124.109 bkce.diezhi.net172.27.124.109 bkrepo.bkce.diezhi.net172.27.124.109 docker.bkce.diezhi.net172.27.124.109 bkapi.bkce.diezhi.net172.27.124.109 bkpaas.bkce.diezhi.net172.27.124.109 bkiam-api.bkce.diezhi.net172.27.124.109 bkiam.bkce.diezhi.net172.27.124.109 apps.bkce.diezhi.net172.27.124.109 bknodeman.bkce.diezhi.net172.27.124.109 job.bkce.diezhi.net172.27.124.109 jobapi.bkce.diezhi.net\n\n使用admin账号通过浏览器访问devops.bkce.diezhi.net，并创建项目：\n\n\n此时重新新增：\nkubectl exec -n blueking deploy/bk-ci-bk-ci-store -- \\curl -vs http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/market/image/init -X POST \\-H &#x27;X-DEVOPS-UID: admin&#x27; -H &#x27;Content-type: application/json&#x27; -d &#x27;&#123;&quot;imageCode&quot;:&quot;bkci&quot;,&quot;imageName&quot;:&quot;bkci&quot;,&quot;imageRepo&quot;:&quot;hub.bktencent.com/bkci/ci&quot;,&quot;projectCode&quot;:&quot;huari-demo&quot;,&quot;userId&quot;:&quot;admin&quot;&#125;&#x27; | jq .\n\n如新建项目遇见可授权人员范围加载不出来(白屏转圈圈)\n需要确定跳转从蓝鲸跳转蓝盾是否是走了https，本文档是使用http部署，所以使用https会出现问题\n4.2 [可跳过]对接制品库蓝盾依靠蓝鲸制品库来提供流水线仓库和自定义仓库，需要调整制品库的认证模式。\n当 bk-ci release 成功启动后，我们开始配置蓝鲸制品库，并注册到蓝盾中。\n4.2.1 修改 bk-repo custom values相关配置已经放在gitlab仓库上维护，可以直接跳过本步骤。\n[可跳过]请在 中控机 执行：\ncd ~/bkce7.1-install/blueking/  # 进入工作目录case $(yq e &#x27;.auth.config.realm&#x27; environments/default/bkrepo-custom-values.yaml.gotmpl 2&gt;/dev/null) in  null|&quot;&quot;)    tee -a environments/default/bkrepo-custom-values.yaml.gotmpl &lt;&lt;&lt; $&#x27;auth:\\n  config:\\n    realm: devops&#x27;  ;;  devops)    echo &quot;environments/default/bkrepo-custom-values.yaml.gotmpl 中配置了 .auth.config.realm=devops, 无需修改.&quot;  ;;  *)    echo &quot;environments/default/bkrepo-custom-values.yaml.gotmpl 中配置了 .auth.config.realm 为其他值, 请手动修改值为 devops.&quot;  ;;esac\n\n[可跳过]修改成功后，继续在工作目录执行如下命令使修改生效：\nhelmfile -f base-blueking.yaml.gotmpl -l name=bk-repo apply\n\n4.2.2 检查配置是否生效检查 release 生效的 values 和 configmap 是否重新渲染。\n[可跳过]请在 中控机 执行：\nhelm get values -n blueking bk-repo | yq e &#x27;.auth.config.realm&#x27;kubectl get cm -n blueking bk-repo-bkrepo-auth -o json | jq -r &#x27;.data.&quot;application.yml&quot;&#x27; | yq e &#x27;.auth.realm&#x27; -\n\n4.2.3 重启 bk-repo auth 微服务因为对接制品库的相关信息已经在gitlab仓库上维护了，所以此处不用进行重启。\n[可跳过]因为 deployment 没有变动，所以不会自动重启，此处需要单独重启：\nkubectl rollout restart deployment -n blueking bk-repo-bkrepo-auth\n\n4.2.4 在蓝盾中注册制品库[可跳过]请在 中控机 执行：\ncd ~/bkce7.1-install/blueking/  # 进入工作目录BK_DOMAIN=$(yq e &#x27;.domain.bkDomain&#x27; environments/default/custom.yaml)  # 从自定义配置中提取, 也可自行赋值# 向project微服务注册制品库kubectl exec -i -n blueking deploy/bk-ci-bk-ci-project -- curl -sS -X PUT -H &#x27;Content-Type: application/json&#x27; -H &#x27;Accept: application/json&#x27; -H &#x27;X-DEVOPS-UID: admin&#x27; -d &quot;&#123;\\&quot;showProjectList\\&quot;:true,\\&quot;showNav\\&quot;:true,\\&quot;status\\&quot;:\\&quot;ok\\&quot;,\\&quot;deleted\\&quot;:false,\\&quot;iframeUrl\\&quot;:\\&quot;//bkrepo.$BK_DOMAIN/ui/\\&quot;&#125;&quot; &quot;http://bk-ci-bk-ci-project.blueking.svc.cluster.local/api/op/services/update/Repo&quot;\n\n4.3 [可选]下载和上传插件4.3.1 下载插件请在 中控机 执行：\nbkdl-7.1-stable.sh -ur latest ci-plugins\n\n4.3.2 上传插件此操作只能新建插件，每个插件只能上传一次。\ncd ~/bkce7.1-install/blueking/  # 进入工作目录for f in ../ci-plugins/*.zip; do    atom=&quot;$&#123;f##*/&#125;&quot;    atom=$&#123;atom%.zip&#125;    echo &gt;&amp;2 &quot;upload $atom from $f&quot;    kubectl exec -i -n blueking deploy/bk-ci-bk-ci-store -- \\      curl -s \\      http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/pipeline/atom/deploy/&quot;?publisher=admin&quot; \\      -H &#x27;X-DEVOPS-UID: admin&#x27; -F atomCode=$atom -F file=@- &lt; &quot;$f&quot; | jq .      # 设置为默认插件，全部项目可见。    kubectl exec -n blueking deploy/bk-ci-bk-ci-store -- \\    curl -s http://bk-ci-bk-ci-store.blueking.svc.cluster.local/api/op/pipeline/atom/default/atomCodes/$atom \\-H &#x27;X-DEVOPS-UID: admin&#x27; -X POST | jq .done\n\n4.3.3 问题及解决方案这个问题已经在蓝盾的helm charts里面进行了优化\n上传插件时会碰到这个问题：\n蓝盾官方文档说法是：\n实际原因是：HTTP请求报了 413 Request Entity Too Large\n具体解决方式详见：Ingress 域名方式导致413 Request Entity Too Large-阿里云开发者社区\n造成这个问题的主要原因是nginx-ingress的默认配置中proxy-body-size的数值太小\n5. 支持https如果开始就准备好了相关证书，那么可以将该步骤提前，在部署基础服务和蓝盾之前就先修改好相关的yaml，将需要创建的Secret和要更新的Ingress配置都提前修改好，然后直接部署即可。\n5.1 购买相关证书涉及的域名：bk.blazehu.com、*.bk.blazehu.com（如devops.bk.blazehu.com）。需购买泛域名证书。\n5.2 创建相关Secret（用于存储TLS证书和私钥）# 创建SecretBK_DOMAIN=$(yq e &#x27;.domain.bkDomain&#x27; environments/default/custom.yaml)cd $HOME/$BK_DOMAINkubectl create secret tls $BK_DOMAIN -n blueking --cert=$HOME/$BK_DOMAIN/$BK_DOMAIN.pem --key=$HOME/$BK_DOMAIN/$BK_DOMAIN.key————————————————本文链接：https://blazehu.com/2024/05/24/devops/landun_install/版权声明： 本博客所有文章除特别声明外，均采用 CC BY 4.0 CN协议 许可协议。转载请注明出处！\n\n5.3 更新 Ingress TLS在证书及证书secret准备好之后，需要变更蓝鲸系列ingress开启tls的支持，执行对应的脚本\n#!/bin/bash# 配置变量NAMESPACE=&quot;blueking&quot;DOMAIN_FILE=&quot;environments/default/custom.yaml&quot;BK_DOMAIN=$(yq e &#x27;.domain.bkDomain&#x27; &quot;$DOMAIN_FILE&quot;)  # 从配置文件中读取域名TLS_HOST=&quot;*.$BK_DOMAIN&quot;  # 泛域名TLS_SECRET=&quot;$BK_DOMAIN&quot;  # Secret 名称与域名一致# 检查域名和 Secret 是否正确if [[ -z &quot;$BK_DOMAIN&quot; ]]; then  echo &quot;Error: BK_DOMAIN is not set in $DOMAIN_FILE.&quot;  exit 1fi# 获取命名空间中的所有 Ingress 资源ingresses=$(kubectl get ingress -n &quot;$NAMESPACE&quot; -o jsonpath=&#x27;&#123;.items[*].metadata.name&#125;&#x27;)# 遍历所有 Ingress 资源并更新 TLS 配置for ingress in $ingresses; do  echo &quot;Updating Ingress: $ingress in namespace: $NAMESPACE&quot;    # 检查 Ingress 是否已存在 TLS 配置  if kubectl get ingress &quot;$ingress&quot; -n &quot;$NAMESPACE&quot; -o jsonpath=&#x27;&#123;.spec.tls&#125;&#x27; | grep -q &quot;$TLS_HOST&quot;; then    echo &quot;TLS configuration for $TLS_HOST already exists in Ingress $ingress. Skipping.&quot;    continue  fi  # 更新 Ingress 的 TLS 配置  kubectl patch ingress &quot;$ingress&quot; -n &quot;$NAMESPACE&quot; --type=json -p=&#x27;[    &#123;      &quot;op&quot;: &quot;add&quot;,      &quot;path&quot;: &quot;/spec/tls&quot;,      &quot;value&quot;: [        &#123;          &quot;hosts&quot;: [&quot;&#x27;&quot;$TLS_HOST&quot;&#x27;&quot;],          &quot;secretName&quot;: &quot;&#x27;&quot;$TLS_SECRET&quot;&#x27;&quot;        &#125;      ]    &#125;  ]&#x27; || &#123; echo &quot;Failed to update Ingress $ingress&quot;; exit 1; &#125;  echo &quot;Updated Ingress $ingress with TLS configuration for $TLS_HOST.&quot;doneecho &quot;All Ingress resources in namespace $NAMESPACE have been updated with TLS configuration for $TLS_HOST.&quot;\n\n5.4 配置蓝鲸启用HTTPS在git仓库维护，主要有两个变更：\nenvironments&#x2F;default&#x2F;custom.yaml: .bkDomainScheme 值设置为 httpsenvironments&#x2F;default&#x2F;bkci&#x2F;bkci-custom-values.yaml.gotmpl: .config.bkHttpSchema 值设置为 https\nyq -i &#x27;.bkDomainScheme = &quot;https&quot;&#x27; environments/default/custom.yaml# 将bkHttpSchema: https替换为bkHttpSchema: httpsed -i &#x27;s|bkHttpSchema: http|bkHttpSchema: https|&#x27; environments/default/bkci/bkci-custom-values.yaml.gotmpl\n\n5.5 构建机Agent配置变更及重启# 停止agent服务./stop.shBK_DOMAIN=&quot;deveops.bk.blazehu.com&quot;# 修改.agent.properties文件，开启httpssed -i &#x27;&#x27; &#x27;s|http://$BK_DOMAIN|https://$BK_DOMAIN|g&#x27; .agent.properties# 修改telegraf.conf文件，开启httpssed -i &#x27;&#x27; &#x27;s|http://$BK_DOMAIN|https://$BK_DOMAIN|g&#x27; telegraf.conf# 启动agent./start.sh# 这里需要注意，仔细查看.agent.properties里devops.agent.user， 这里是哪个用户就用哪个用户启动agent","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"Kustomize与Helm对比","url":"/2025/07/kubernetes/operator-kai-fa/kustomize-yu-helm-dui-bi/","content":"0、前言K8s 是一个开源容器编排平台，可自动执行容器化应用程序的部署、扩展和管理。近年来，K8s 已成为采用云原生架构和容器化技术的组织的标准。\n但是由于K8s的复杂性，所以很多公司以及开源组织都在开发相关的工具来简化k8s的使用门槛，这其中就包括两个很优秀的开源工具，Kustomize（K8s 的配置管理器）和Helm （K8s 的包管理器）。\n本文将针对二者来进行对比。\n\n\n\n\nKustomize\nHelm\n\n\n\n操作方法\noverlays\ntemplating\n\n\n使用成本\n简单\n复杂\n\n\n是否支持封装\n简单\n是\n\n\nkubectl集成\n是\n否\n\n\nkubectl集成\n声明式\n命令式\n\n\n1、KustomizeKustomize 是 k8s集群的配置定制工具。它允许管理员使用非模板文件进行声明性更改，而不影响原始清单文件。\n来看一下kubebuilder生成项目的Kustomize配置，以&#x2F;config&#x2F;crd目录为例：\n├── crd│   ├── bases│   │   └── apps.kubenode.alibaba-inc.com_myapps.yaml│   ├── kustomization.yaml│   ├── kustomizeconfig.yaml│   └── patches│       ├── cainjection_in_myapps.yaml│       └── webhook_in_myapps.yaml\n\n其中config&#x2F;crd目录是执行kubebuilder create api后生成的，最原始的目录结构是：\n├── crd│   ├── kustomization.yaml│   └── kustomizeconfig.yaml\n\n所以bases子目录和patches子目录都是执行make manifests后生成的。\n执行kubebuilder create api可以参考：https://blog.csdn.net/qq_41004932/article/details/142702284\n执行make manifests可以参考：https://blog.csdn.net/qq_41004932/article/details/142703870\n在 Kubebuilder 生成的项目中，config&#x2F;crd 目录下的文件主要用于管理和配置自定义资源定义（Custom Resource Definitions, CRDs）。这些文件通过 Kustomize 工具进行管理，以便于在不同的环境中部署和管理 CRDs。下面是每个文件的作用解释：\n\nbases 目录bases&#x2F;apps.kubenode.alibaba-inc.com_myapps.yaml:这个文件包含了自定义资源定义（CRD）的 YAML 描述。它定义了你的自定义资源（Custom Resource, CR）的结构和元数据。例如，如果我们创建了一个名为 MyApp 的自定义资源，这个文件将描述 MyApp 的 API 版本、资源名称、字段等。\nkustomization.yaml:这个文件是 Kustomize 的配置文件，用于定义如何组合和修改 Kubernetes 资源文件。它指定了哪些资源文件（如 CRD 文件）需要被应用，并且可以包含补丁文件和其他配置选项。例如，它可以指定 bases 目录中的 CRD 文件，以及 patches 目录中的补丁文件。\nkustomizeconfig.yaml:这个文件通常用于配置 Kustomize 的一些高级选项，但在这个上下文中可能不是必需的。它可能会包含一些特定的配置项，用于定制 Kustomize 的行为。在大多数情况下，这个文件可能不需要手动编辑。\npatches&#x2F;cainjection_in_myapps.yaml:这个文件是一个补丁文件，用于在 CRD 上应用 Webhook 注入（例如，证书注入）。例如，如果你的 CRD 需要与 mutating 或 validating webhooks 一起工作，这个补丁文件会确保 Webhook 配置正确地注入到 CRD 中。\npatches&#x2F;webhook_in_myapps.yaml:这个文件也是一个补丁文件，用于在 CRD 上应用 Webhook 配置。例如，它可能会添加或修改 Webhook 的配置，以便在创建或更新自定义资源时触发特定的行为。\n\n总结\nbases 目录：包含 CRD 的基本定义文件。\nkustomization.yaml：Kustomize 的配置文件，用于定义如何组合和修改资源文件。\nkustomizeconfig.yaml：Kustomize 的高级配置文件（可选）。\npatches 目录：包含用于修改 CRD 的补丁文件，通常用于注入 Webhook 配置。\n如果我们像让这个crd根据不同的环境做针对性部署的，例如下面的目录结构:\n├── crd│   ├── bases│   │   └── apps.kubenode.alibaba-inc.com_myapps.yaml│   ├── development│   │   ├── kustomization.yaml│   │   └── patches│   │       └── webhook_in_myapps.yaml│   ├── testing│   │   ├── kustomization.yaml│   │   └── patches│   │       └── webhook_in_myapps.yaml│   ├── production│   │   ├── kustomization.yaml│   │   └── patches│   │       └── webhook_in_myapps.yaml│   ├── kustomization.yaml│   ├── kustomizeconfig.yaml│   └── patches│       ├── cainjection_in_myapps.yaml│       └── webhook_in_myapps.yaml\n\n所有自定义规范都包含在 kustomization.yaml 文件中，该文件将规范叠加在现有清单之上以生成资源的自定义版本。\n所以我们可以根据这一特性，针对不同的环境，对crd进行定制。\n2、HelmHelm 是一个能够在 K8s 上打包、部署和管理应用程序的工具，即使是最复杂的 K8s 应用程序它都可以帮助定义，安装和升级，同时Helm 也是 CNCF 的毕业项目。这里涉及到了以及关于helm的重要概念：\n\nHelm Charts：预先配置yaml的模板，在这里叫Chart，用于描述 K8s 应用程序的yaml和配置\nHelm Client：用于与 Helm 交互并管理这些Chart版本的命令行界面\nChart 仓库：管理Chart的仓库，跟Maven的Nexus一个意思，比如在公司环境构建上传，在客户的机房连接到这Chart 仓库下载Chart，并部署到k8s中。\n\n2.1 helm示例helm的示例需要用到kubectl、helm以及k8s集群，相应的安装参考：\n\nmac环境：https://blog.csdn.net/qq_41004932/article/details/142684319\nubuntu环境：https://blog.csdn.net/qq_41004932/article/details/142691490\nkind集群：https://blog.csdn.net/qq_41004932/article/details/142691490\n\nHelm Charts 是预先配置的 K8s 资源包。Helm Chart 包含部署特定应用程序或服务所需的所有信息，包括 K8s 清单、环境变量和其他配置\n目录名称是Chart的名称，如Helm 文档所示，我们通过helm create demo命令创建一个Chart，执行完以后，默认会生成一个 nginx 的Chart。\nhelm create demo\n\n结果：\n$ helm create demo                                                                                                                      [10:33:35]Creating demo\n\n查看目录结构：\n.├── Chart.yaml├── charts├── templates│   ├── NOTES.txt│   ├── _helpers.tpl│   ├── deployment.yaml│   ├── hpa.yaml│   ├── ingress.yaml│   ├── service.yaml│   ├── serviceaccount.yaml│   └── tests│       └── test-connection.yaml└── values.yaml3 directories, 10 files\n\n2.2 Chart.yaml定义了当前 chart版本，以及描述当前chart用途，其中 name 参数表示 chart 名称，后期上传下载都会用此名称\napiVersion: v2name: demodescription: A Helm chart for Kubernetes# A chart can be either an &#x27;application&#x27; or a &#x27;library&#x27; chart.## Application charts are a collection of templates that can be packaged into versioned archives# to be deployed.## Library charts provide useful utilities or functions for the chart developer. They&#x27;re included as# a dependency of application charts to inject those utilities and functions into the rendering# pipeline. Library charts do not define any templates and therefore cannot be deployed.type: application# This is the chart version. This version number should be incremented each time you make changes# to the chart and its templates, including the app version.# Versions are expected to follow Semantic Versioning (https://semver.org/)version: 0.1.0# This is the version number of the application being deployed. This version number should be# incremented each time you make changes to the application. Versions are not expected to# follow Semantic Versioning. They should reflect the version the application is using.# It is recommended to use it with quotes.appVersion: &quot;1.16.0&quot;\n\n2.3 values.yaml可变参数，都是在此文件中定义，在yaml模板中引用，比如：image.repository，而引用则通过.Values+变量的名进行引用\n# Default values for demo.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 1image:  repository: nginx  pullPolicy: IfNotPresent  # Overrides the image tag whose default is the chart appVersion.  tag: &quot;&quot;imagePullSecrets: []nameOverride: &quot;&quot;fullnameOverride: &quot;&quot;serviceAccount:  # Specifies whether a service account should be created  create: true  # Annotations to add to the service account  annotations: &#123;&#125;  # The name of the service account to use.  # If not set and create is true, a name is generated using the fullname template  name: &quot;&quot;podAnnotations: &#123;&#125;podSecurityContext: &#123;&#125;  # fsGroup: 2000securityContext: &#123;&#125;  # capabilities:  #   drop:  #   - ALL  # readOnlyRootFilesystem: true  # runAsNonRoot: true  # runAsUser: 1000service:  type: ClusterIP  port: 80ingress:  enabled: false  className: &quot;&quot;  annotations: &#123;&#125;    # kubernetes.io/ingress.class: nginx    # kubernetes.io/tls-acme: &quot;true&quot;  hosts:    - host: chart-example.local      paths:        - path: /          pathType: ImplementationSpecific  tls: []  #  - secretName: chart-example-tls  #    hosts:  #      - chart-example.localresources: &#123;&#125;  # We usually recommend not to specify default resources and to leave this as a conscious  # choice for the user. This also increases chances charts run on environments with little  # resources, such as Minikube. If you do want to specify resources, uncomment the following  # lines, adjust them as necessary, and remove the curly braces after &#x27;resources:&#x27;.  # limits:  #   cpu: 100m  #   memory: 128Mi  # requests:  #   cpu: 100m  #   memory: 128Miautoscaling:  enabled: false  minReplicas: 1  maxReplicas: 100  targetCPUUtilizationPercentage: 80  # targetMemoryUtilizationPercentage: 80nodeSelector: &#123;&#125;tolerations: []affinity: &#123;&#125;\n\n2.4 _helpers.tpl定义通用代码块，然后yaml 文件会通过 include 引用\n定义:\n&#123;&#123;- define &quot;demo.name&quot; -&#125;&#125;&#123;&#123;- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix &quot;-&quot; &#125;&#125;&#123;&#123;- end &#125;&#125;\n\n引用:\n&#123;&#123; include &quot;demo.fullname&quot; . &#125;&#125;\n2.5 templates此目录主要存放的是要部署的 yaml文件模板，同时也包含_helpers.tpl文件，模板会引用values.yaml、Chart.yaml定义的参数，以及_helpers.tpl定义的通用代码块\napiVersion: apps/v1kind: Deploymentmetadata:  name: &#123;&#123; include &quot;demo.fullname&quot; . &#125;&#125;  labels:    &#123;&#123;- include &quot;demo.labels&quot; . | nindent 4 &#125;&#125;spec:  &#123;&#123;- if not .Values.autoscaling.enabled &#125;&#125;  replicas: &#123;&#123; .Values.replicaCount &#125;&#125;  &#123;&#123;- end &#125;&#125;  selector:    matchLabels:      &#123;&#123;- include &quot;demo.selectorLabels&quot; . | nindent 6 &#125;&#125;  template:    metadata:      &#123;&#123;- with .Values.podAnnotations &#125;&#125;      annotations:        &#123;&#123;- toYaml . | nindent 8 &#125;&#125;      &#123;&#123;- end &#125;&#125;      labels:        &#123;&#123;- include &quot;demo.selectorLabels&quot; . | nindent 8 &#125;&#125;    spec:      &#123;&#123;- with .Values.imagePullSecrets &#125;&#125;      imagePullSecrets:        &#123;&#123;- toYaml . | nindent 8 &#125;&#125;      &#123;&#123;- end &#125;&#125;      serviceAccountName: &#123;&#123; include &quot;demo.serviceAccountName&quot; . &#125;&#125;      securityContext:        &#123;&#123;- toYaml .Values.podSecurityContext | nindent 8 &#125;&#125;      containers:        - name: &#123;&#123; .Chart.Name &#125;&#125;          securityContext:            &#123;&#123;- toYaml .Values.securityContext | nindent 12 &#125;&#125;          image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag | default .Chart.AppVersion &#125;&#125;&quot;          imagePullPolicy: &#123;&#123; .Values.image.pullPolicy &#125;&#125;          ports:            - name: http              containerPort: &#123;&#123; .Values.service.port &#125;&#125;              protocol: TCP          livenessProbe:            httpGet:              path: /              port: http          readinessProbe:            httpGet:              path: /              port: http          resources:            &#123;&#123;- toYaml .Values.resources | nindent 12 &#125;&#125;      &#123;&#123;- with .Values.nodeSelector &#125;&#125;      nodeSelector:        &#123;&#123;- toYaml . | nindent 8 &#125;&#125;      &#123;&#123;- end &#125;&#125;      &#123;&#123;- with .Values.affinity &#125;&#125;      affinity:        &#123;&#123;- toYaml . | nindent 8 &#125;&#125;      &#123;&#123;- end &#125;&#125;      &#123;&#123;- with .Values.tolerations &#125;&#125;      tolerations:        &#123;&#123;- toYaml . | nindent 8 &#125;&#125;      &#123;&#123;- end &#125;&#125;\n\nimage: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag | default .Chart.AppVersion &#125;&#125;&quot;\n\n2.5 部署如果想部署的话，可以使用下面命令进行\nhelm package helm-demo\n\n可以发现，这里helm是将要部署的资源当作一个整体来进行操作的，也就是讲一个资源的所有yaml配置作为一个整体的形式来进行操作。\n3、主要差异3.1 操作方法Kustomize 依赖特定于目录的kustomization.yaml文件来构建各个资源并对其进行更改。这些文件将补丁和覆盖应用到共享基文件夹中声明的资源，以提供自动化的多环境配置。\nHelm 通过引用value.yaml文件作为变量源，使用模板生成有效的 K8s 配置。模板目录托管 Helm Chart在部署期间用于创建资源的文件。\n3.2 便捷性从K8s 版本 1.14 开始，Kustomize 与 kubectl CLI 捆绑在一起，因此不需要掌握任何其他工具。Kustomize 支持声明式部署，并对每个文件使用纯 YAML，从而更容易使用。\nHelm 为K8s包管理任务添加了额外的抽象层，从而加快了希望简化集群配置和发布自动化的团队的学习曲线。Helm Chart 相对Kustomize复杂，不过功能更加强大。\n3.3 打包Kustomize 缺乏的打包功能，并且每个资源都必须在基本文件夹中声明，并在覆盖kustomization.yaml文件中单独声明变体。\n而Helm将所有必需的K8s资源都打包到一个文件夹中，该文件夹可以根据需要重复使用。Helm 还允许设置应用程序默认值，并且使用values.yaml文件修改参数，从而注入引用的 yaml 文件中。\n3.4 原生 kubectl 集成从 K8s 1.14 版开始，Kustomize 就预装了 kubectl，Helm 并未与 K8s 预先集成，因此必须手动安装 Helm。\n3.5 Kustomize 与 Helm - 何时使用3.5.1 何时使用 KustomizeKustomize允许在不改变原始文件的情况下进行精确更改。 因此可以有以下场景\n\n应用配置的变体管理：当你需要管理多个环境（例如开发、测试、生产）中应用的变体时，Kustomize 是一个很好的选择。它允许你为不同的环境创建不同的配置，并使用一套基础配置来定义通用部分。\n持续集成和持续部署（CI&#x2F;CD）流水线：Kustomize 可以与 CI&#x2F;CD 工具集成，帮助你实现自动化部署。通过在流水线中使用 Kustomize，你可以根据需要生成特定环境的配置，并将其应用到集群中。\n\n3.5.2 何时使用 HelmHelm 将所有 K8s 对象封装到一个包中，减少了与各个yaml 文件的交互。除此之外，大多数第三方供应商还提供预构建的 Helm 图表，以简化将其产品部署到 K8s 中的过程。因此，Helm 通常是安装现成解决方案（例如监控、数据库和消息中间件等）的首选\n","categories":["kubernetes/operator"],"tags":["kubernetes","operator"]},{"title":"operator部署验证","url":"/2025/07/kubernetes/operator-kai-fa/operator-bu-shu-yan-zheng/","content":"1、部署命令这个是很多博客教程都在使用的部署命令：\nmake manifestsmake installexport ENABLE_WEBHOOKS=falsemake run\n我们使用之前的demo来进行部署验证：Kubernetes-Operator篇-02-脚手架熟悉\n这里面涉及到的makefile的配置可以参考：Kubernetes-Operator篇-03-kubebuilder的Makefile文件熟悉\n下面让我来看看这些命令的含义，并且都是在干什么\n1.1 make manifestsmake manifests：\n.PHONY: manifestsmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases\n这里主要是为了生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象。\n在生成了WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象后，我们可以进行查看，但是不要手动去修改，因为后面的make install和make run等命令，都依赖该目标，并且因为都是使用的.PHONY关键字来申明的伪目标，所以每次执行make install和make run都会执行manifests依赖。\n所以单独执行make manifests的意义是什么呢，个人观点，在一个完整部署动作中，完成了代码变更后，是需要通过make manifests来进行配置的生成查看的。但是如果在本部署动作中，不关心生成配置的具体情况，只想走一遍流程，或者直接测试，那其实忽略不执行也ok，毕竟后面每一步都会重新生成且覆盖。\n1.2 make installmake install：\n.PHONY: installinstall: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.        $(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -\n\n先运行manifests和kustomize目标，然后使用kustomize 构建config&#x2F;crd目录中的资源，并使用kubectl将其应用到集群。\n这里manifests目标会生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象，并保存在config&#x2F;crd&#x2F;bases目录下。\n而kustomize目标是进行kustomize工具的下载，如果二进制文件不存在则下载，然后生成二进制文件的软链，二进制文件只在首次下载，后续都是仅更新软链。\n下面的命令是install实际执行的动作：\n$(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -\n\n1.3 make runmake run：\n.PHONY: runrun: manifests generate fmt vet ## Run a controller from your host.        go run ./cmd/main.go\n先运行manifests、generate、fmt和vet目标，然后使用go run命令运行cmd&#x2F;main.go文件\n这里manifests目标会生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象，并保存在config&#x2F;crd&#x2F;bases目录下。\n而generate 目标是使用controller-gen工具生成包含DeepCopy、DeepCopyInto, 和DeepCopyObject方法实现的代码。\n至于fmt和vet目标就是执行go的命令：fmt ./...和go vet ./...\n下面的命令是install实际执行的动作：\ngo run ./cmd/main.go\n\n1.4 环境变量ENABLE_WEBHOOKS环境变量ENABLE_WEBHOOKS控制着webhook的启用与否。\n这段代码是kubectl生成的operator项目的main函数中启用webhook逻辑的部分：\nif os.Getenv(&quot;ENABLE_WEBHOOKS&quot;) != &quot;false&quot; &#123;                if err = (&amp;appsv1.Myapp&#123;&#125;).SetupWebhookWithManager(mgr); err != nil &#123;                        setupLog.Error(err, &quot;unable to create webhook&quot;, &quot;webhook&quot;, &quot;Myapp&quot;)                        os.Exit(1)                &#125;        &#125;\n\n这里很简单粗暴的根据环境变量ENABLE_WEBHOOKS来决定是否启用webhook，os.Getenv的定义:\nfunc Getenv(key string) string &#123;        testlog.Getenv(key)        v, _ := syscall.Getenv(key)        return v&#125;func Getenv(key string) (value string, found bool) &#123;        envOnce.Do(copyenv)        if len(key) == 0 &#123;                return &quot;&quot;, false        &#125;        envLock.RLock()        defer envLock.RUnlock()        i, ok := env[key]        if !ok &#123;                return &quot;&quot;, false        &#125;        s := envs[i]        for i := 0; i &lt; len(s); i++ &#123;                if s[i] == &#x27;=&#x27; &#123;                        return s[i+1:], true                &#125;        &#125;        return &quot;&quot;, false&#125;\n这里返回的值，可能是空字符串，也可能是bool类型值的字符串，但是只要值不是false，就会开启webhook。\n至于这里为什么要临时关闭webhook，因为启用webhook是需要证书的，也就是我们需要在本地安装cert-manager，并且还需要配置，在本实验项目就不搞太复杂了。\n2、CRD 调试2.1 make manifestsmake manifests/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/basesls -lh config/crd/bases/总计 1.1M-rw-rw-r-- 1 king king 1.1M 10月  5 16:17 apps.kubenode.kingtest.com_myapps.yaml\n\n2.2 make install命令：\nmake install\n\n2.2.1 make install错误：dial tcp 127.0.0.1:8080: connect: connection refusedmake install/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases/home/king/workspace/king-devops/operator/myapp-operator/bin/kustomize build config/crd | kubectl apply -f -error: error validating &quot;STDIN&quot;: error validating data: failed to download openapi: Get &quot;http://localhost:8080/openapi/v2?timeout=32s&quot;: dial tcp 127.0.0.1:8080: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=falsemake: *** [Makefile:131：install] 错误 1\n\n这个错误很诡异： dial tcp 127.0.0.1:8080: connect: connection refused\n首先集群是存在的：\nsudo kind get clustersmyk8s-test\n\n其次已经使用下面的命令将集群信息，设置进去kubectl的上下文里：\nsudo kubectl cluster-info --context kind-myk8s-test\n\nsudo kubectl cluster-info –context kind-myk8s-test\nsudo kubectl get nodesNAME                       STATUS   ROLES           AGE   VERSIONmyk8s-test-control-plane   Ready    control-plane   43h   v1.31.0myk8s-test-worker          Ready    &lt;none&gt;          43h   v1.31.0myk8s-test-worker2         Ready    &lt;none&gt;          43h   v1.31.0\n\n进一步测试，分别执行kubectl version和kubectl cluster-info dump都遇见了类似的错误：\nkubectl version：\nkubectl versionClient Version: v1.31.1Kustomize Version: v5.4.2The connection to the server localhost:8080 was refused - did you specify the right host or port?\n\nkubectl cluster-info dump：\nkubectl cluster-info dumpThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\n但是很悲催的是，忘记将kind集群的kubeconfig导出到本地，我的理解是，虽然kubectl上下文是有集群kubeconfig的，但是本地并没有，简单来说，就是这个信息是kubectl自身保存的，不是直接使用的系统上存储的kubeconfig\n使用下面的命令，将kubeconfig信息保存在$HOME/.kube/config内：\nsudo kind export kubeconfig --name=myk8s-test --kubeconfig=$HOME/.kube/config\n\n再执行make install命令，本问题已经解决，但是新的问题出现了：\nmake install/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases/home/king/workspace/king-devops/operator/myapp-operator/bin/kustomize build config/crd | kubectl apply -f -The CustomResourceDefinition &quot;myapps.apps.kubenode.kingtest.com&quot; is invalid: metadata.annotations: Too long: must have at most 262144 bytesmake: *** [Makefile:131：install] 错误 1\n\n2.2.2 make install错误：metadata.annotations: Too long: must have at most 262144 byteskubebuilder的github issues中，修复过这个问题: https://github.com/kubernetes-sigs/kubebuilder/pull/2862/commits/2c5b9edf614444acd43ae5f65af1702a5ed63ed6\n修复方法：打开Makefile，在 manifests 命令处，修改 crd 为 crd:maxDescLen&#x3D;0\n原始的：\n.PHONY: manifestsmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases\n\n修复后的：\n.PHONY: manifestsmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.        $(CONTROLLER_GEN) rbac:roleName=manager-role crd:maxDescLen=0 webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases\n\n问题解决：\nmake install/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd:maxDescLen=0 webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases/home/king/workspace/king-devops/operator/myapp-operator/bin/kustomize build config/crd | kubectl apply -f -customresourcedefinition.apiextensions.k8s.io/my\n\n2.2.3 make install验证查看安装结果：\nkubectl get crdNAME                                CREATED ATmyapps.apps.kubenode.kingtest.com   2024-10-05T11:56:22Z\n已经可以看到我们定义的crd了。\n2.3 make run命令：\nexport ENABLE_WEBHOOKS=falsemake run\n\n执行结果：\nmake run/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen rbac:roleName=manager-role crd:maxDescLen=0 webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;go fmt ./...go vet ./...go run ./cmd/main.go2024-10-05T20:29:02+08:00       INFO    setup   starting manager2024-10-05T20:29:02+08:00       INFO    starting server &#123;&quot;name&quot;: &quot;health probe&quot;, &quot;addr&quot;: &quot;[::]:8081&quot;&#125;2024-10-05T20:29:02+08:00       INFO    Starting EventSource    &#123;&quot;controller&quot;: &quot;myapp&quot;, &quot;controllerGroup&quot;: &quot;apps.kubenode.kingtest.com&quot;, &quot;controllerKind&quot;: &quot;Myapp&quot;, &quot;source&quot;: &quot;kind source: *v1.Myapp&quot;&#125;2024-10-05T20:29:02+08:00       INFO    Starting Controller     &#123;&quot;controller&quot;: &quot;myapp&quot;, &quot;controllerGroup&quot;: &quot;apps.kubenode.kingtest.com&quot;, &quot;controllerKind&quot;: &quot;Myapp&quot;&#125;2024-10-05T20:29:02+08:00       INFO    Starting workers        &#123;&quot;controller&quot;: &quot;myapp&quot;, &quot;controllerGroup&quot;: &quot;apps.kubenode.kingtest.com&quot;, &quot;controllerKind&quot;: &quot;Myapp&quot;, &quot;worker count&quot;: 1&#125;\n\n已经成功启动！！！\n2.4 执行debug2.4.1 启动debug在上面执行了以下步骤：\nmake manifestsmake installexport ENABLE_WEBHOOKS=falsemake run\n我们先将make run运行的controller停止，然后打开operator项目，在项目的调谐函数内打上断点，然后直接以debug模式启动。\n打断点\n别忘了设置环境变量，关闭webhook\n以debug方式启动：\n执行结果\n接下来，我们来apply一个资源，测试一下。\n2.4.2 crd测试编写一个测试的yaml：\napiVersion: apps.kubenode.kingtest.com/v1kind: Myappmetadata:  name: myapp-samplespec:  foo: &quot;test value&quot;  replicas: 3  selector:    matchLabels:      app: myapp  template:    metadata:      labels:        app: myapp    spec:      containers:      - name: myapp-container        image: nginx:latest        ports:        - containerPort: 80\n\n将测试yaml通过kubectl apply进去集群:\nsudo kubectl apply -f myapptest.yamlmyapp.apps.kubenode.kingtest.com/myapp-sample created\n\n然后controller的调谐函数成功断住\n再向下执行一步\n在终端里已经成功打印出来代码里的日志：\n通过kubectl查看crd状态：\nsudo kubectl get Myapp -A -owideNAMESPACE   NAME           AGEdefault     myapp-sample   14m\n到此都是符合预期的。\n4 构建CRD镜像并部署进k8s上面我们是在本地运行的controller，那么在实际中该怎么办呢。\n答案是将controller打包成docker镜像，然后部署进集群。\n构建镜像并推送至你的镜像仓库\nmake docker-build docker-push IMG=&lt;some-registry&gt;/&lt;project-name&gt;:tag\n\n看下makefile的定义：\n.PHONY: docker-builddocker-build: ## Build docker image with the manager.        $(CONTAINER_TOOL) build -t $&#123;IMG&#125; ..PHONY: docker-pushdocker-push: ## Push docker image with the manager.        $(CONTAINER_TOOL) push $&#123;IMG&#125;\n\n这里就是使用CONTAINER_TOOL(默认为docker)构建并推送docker镜像，并使用IMG变量指定镜像名称和标签指定镜像将controller部署进你的集群\nmake deploy IMG=&lt;some-registry&gt;/&lt;project-name&gt;:tag\n\n查看makefile定义：\n.PHONY: deploydeploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.        cd config/manager &amp;&amp; $(KUSTOMIZE) edit set image controller=$&#123;IMG&#125;        $(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -\n\n\n先运行manifests和kustomize目标\n使用kustomize设置controller镜像为IMG\n使用kustomize构建config&#x2F;default目录中的资源，并使用kubectl将其应用到集群\n\n5 删除controller部署和卸载CRD删除controller部署：\nmake undeploy\n\n查看makefile定义：\n.PHONY: undeployundeploy: kustomize ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.        $(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n\n使用kustomize 构建config&#x2F;default目录中的资源，并使用kubectl将其从集群中删除。可以通过ignore-not-found&#x3D;true忽略资源未找到的错误\n从集群卸载CRD：\nmake uninstall\n\n查看makefile定义：\n.PHONY: uninstalluninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.        $(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n\n先运行manifests和kustomize目标\n使用kustomize 构建config&#x2F;crd目录中的资源，并使用kubectl将其从集群中删除，可以通过ignore-not-found&#x3D;true忽略资源未找到的错误\n\n6 其他这是在我们本地运行的controller，我在最初一直想不通是怎么连接的集群的，知道想起了最初的client-go的使用。\n在看本文的同学，肯定是听说过，甚至使用过client-go的，我们可以使用client-go去实现一些小工具，比如说某个namespace下pod的查询，甚至可以获取node的列表，pod的全部信息等等，只要是kubectl可以做的，都可以通过client-go来实现，并且可以做更复杂的场景。\n我们是可以在工具里指定kubeconfig的路径的，让client-go去读取，并渲染对应的client，然后和对应集群的apiserver做交互。\n知道了这点，那么问题来了，operator是怎么做的呢，operator其实是使用的controller-runtime库，他和client-go有什么区别呢，他是client-go以及其他库的更上一层封装。\n所以虽然没具体看，但是controller-runtime肯定是默认读取的$HOME&#x2F;.kube&#x2F;config文件的kubeconfig，至于可不可以指定其他的kubeconfig，理论上是可以的，因为就是指定下kubeconfig的路径，然后去读取渲染client，这里和自己直接使用client-go去实现是一样的逻辑。\n但是，controller-runtime究竟支不支持，还需要自己去找一下源码或者资料，因为本地运行controller这本身肯定是不建议的，所以不支持也是说得过去的。\n","categories":["kubernetes/operator"],"tags":["kubernetes","operator"]},{"title":"kubebuilder的makefile文件","url":"/2025/07/kubernetes/operator-kai-fa/kubebuilder-de-makefile-wen-jian/","content":"1、Makefile简介Makefile 是一种用于自动化构建软件项目的文件。它通常用于管理和执行编译、链接、测试等一系列任务，以提高开发效率。\n1.1 目标与依赖1.1.1目标（Targets）\n代表一个任务或一个文件的生成结果。例如，编译生成的可执行文件、库文件等都可以是目标。\n常见的目标有 “all”（表示构建整个项目）、“clean”（用于清理生成的文件）等。\n\n1.1.2 依赖（Dependencies）\n目标所依赖的其他文件或目标。只有当依赖发生变化时，才会重新执行生成目标的命令。\n例如，一个可执行文件可能依赖于多个源文件和库文件。\n\n1.2 规则与命令1.2.1 规则（Rules）\n描述了如何从依赖生成目标。通常由目标、依赖和命令三部分组成。\n格式一般为：target: dependencies，接着是命令部分，每行命令前面必须以制表符（Tab）开头。\n\n1.2.2 命令（Commands）\n用于执行生成目标的具体操作。可以是编译器命令、链接器命令、文件复制命令等。\n例如，编译 C 语言源文件的命令可能是gcc -o target source.c。\n\n1.3 变量与函数1.3.1 变量（Variables）\n可以定义和使用变量来存储常用的值，如编译器名称、编译选项、源文件列表等。\n变量定义方式为VARIABLE &#x3D; value，使用时用$(VARIABLE)。\n\n1.3.2 函数（Functions）\nMakefile 提供了一些内置函数，可以进行字符串处理、文件操作等。\n例如，$(wildcard *.c)可以获取当前目录下所有的 C 语言源文件。\n\n1.4 优势1.4.1 自动化构建\n可以自动执行一系列构建步骤，减少手动操作和错误。\n当源文件发生变化时，只重新构建受影响的部分，提高构建效率。\n\n1.4.2 可重复性确保在不同的环境中都能以相同的方式构建项目。\n1.4.3 易于维护项目的构建过程集中在一个文件中，便于修改和管理。\n2、kubebuilder makefile2.1 kubebuilder 项目makefile一个kubebuilder创建的operator项目的makefile文件内容：\n# Image URL to use all building/pushing image targetsIMG ?= controller:latest# ENVTEST_K8S_VERSION refers to the version of kubebuilder assets to be downloaded by envtest binary.ENVTEST_K8S_VERSION = 1.31.0# Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)ifeq (,$(shell go env GOBIN))GOBIN=$(shell go env GOPATH)/binelseGOBIN=$(shell go env GOBIN)endif# CONTAINER_TOOL defines the container tool to be used for building images.# Be aware that the target commands are only tested with Docker which is# scaffolded by default. However, you might want to replace it to use other# tools. (i.e. podman)CONTAINER_TOOL ?= docker# Setting SHELL to bash allows bash commands to be executed by recipes.# Options are set to exit when a recipe line exits non-zero or a piped command fails.SHELL = /usr/bin/env bash -o pipefail.SHELLFLAGS = -ec.PHONY: allall: build##@ General# The help target prints out all targets with their descriptions organized# beneath their categories. The categories are represented by &#x27;##@&#x27; and the# target descriptions by &#x27;##&#x27;. The awk command is responsible for reading the# entire set of makefiles included in this invocation, looking for lines of the# file as xyz: ## something, and then pretty-format the target and help. Then,# if there&#x27;s a line with ##@ something, that gets pretty-printed as a category.# More info on the usage of ANSI control characters for terminal formatting:# https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_parameters# More info on the awk command:# http://linuxcommand.org/lc3_adv_awk.php.PHONY: helphelp: ## Display this help.        @awk &#x27;BEGIN &#123;FS = &quot;:.*##&quot;; printf &quot;\\nUsage:\\n  make \\033[36m&lt;target&gt;\\033[0m\\n&quot;&#125; /^[a-zA-Z_0-9-]+:.*?##/ &#123; printf &quot;  \\033[36m%-15s\\033[0m %s\\n&quot;, $$1, $$2 &#125; /^##@/ &#123; printf &quot;\\n\\033[1m%s\\033[0m\\n&quot;, substr($$0, 5) &#125; &#x27; $(MAKEFILE_LIST)##@ Development.PHONY: manifestsmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases.PHONY: generategenerate: controller-gen ## Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.        $(CONTROLLER_GEN) object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;.PHONY: fmtfmt: ## Run go fmt against code.        go fmt ./....PHONY: vetvet: ## Run go vet against code.        go vet ./....PHONY: testtest: manifests generate fmt vet envtest ## Run tests.        KUBEBUILDER_ASSETS=&quot;$(shell $(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path)&quot; go test $$(go list ./... | grep -v /e2e) -coverprofile cover.out# Utilize Kind or modify the e2e tests to load the image locally, enabling compatibility with other vendors..PHONY: test-e2e  # Run the e2e tests against a Kind k8s instance that is spun up.test-e2e:        go test ./test/e2e/ -v -ginkgo.v.PHONY: lintlint: golangci-lint ## Run golangci-lint linter        $(GOLANGCI_LINT) run.PHONY: lint-fixlint-fix: golangci-lint ## Run golangci-lint linter and perform fixes        $(GOLANGCI_LINT) run --fix##@ Build.PHONY: buildbuild: manifests generate fmt vet ## Build manager binary.        go build -o bin/manager cmd/main.go.PHONY: runrun: manifests generate fmt vet ## Run a controller from your host.        go run ./cmd/main.go# If you wish to build the manager image targeting other platforms you can use the --platform flag.# (i.e. docker build --platform linux/arm64). However, you must enable docker buildKit for it.# More info: https://docs.docker.com/develop/develop-images/build_enhancements/.PHONY: docker-builddocker-build: ## Build docker image with the manager.        $(CONTAINER_TOOL) build -t $&#123;IMG&#125; ..PHONY: docker-pushdocker-push: ## Push docker image with the manager.        $(CONTAINER_TOOL) push $&#123;IMG&#125;# PLATFORMS defines the target platforms for the manager image be built to provide support to multiple# architectures. (i.e. make docker-buildx IMG=myregistry/mypoperator:0.0.1). To use this option you need to:# - be able to use docker buildx. More info: https://docs.docker.com/build/buildx/# - have enabled BuildKit. More info: https://docs.docker.com/develop/develop-images/build_enhancements/# - be able to push the image to your registry (i.e. if you do not set a valid value via IMG=&lt;myregistry/image:&lt;tag&gt;&gt; then the export will fail)# To adequately provide solutions that are compatible with multiple platforms, you should consider using this option.PLATFORMS ?= linux/arm64,linux/amd64,linux/s390x,linux/ppc64le.PHONY: docker-buildxdocker-buildx: ## Build and push docker image for the manager for cross-platform support        # copy existing Dockerfile and insert --platform=$&#123;BUILDPLATFORM&#125; into Dockerfile.cross, and preserve the original Dockerfile        sed -e &#x27;1 s/\\(^FROM\\)/FROM --platform=\\$$\\&#123;BUILDPLATFORM\\&#125;/; t&#x27; -e &#x27; 1,// s//FROM --platform=\\$$\\&#123;BUILDPLATFORM\\&#125;/&#x27; Dockerfile &gt; Dockerfile.cross        - $(CONTAINER_TOOL) buildx create --name myapp-operator-builder        $(CONTAINER_TOOL) buildx use myapp-operator-builder        - $(CONTAINER_TOOL) buildx build --push --platform=$(PLATFORMS) --tag $&#123;IMG&#125; -f Dockerfile.cross .        - $(CONTAINER_TOOL) buildx rm myapp-operator-builder        rm Dockerfile.cross.PHONY: build-installerbuild-installer: manifests generate kustomize ## Generate a consolidated YAML with CRDs and deployment.        mkdir -p dist        cd config/manager &amp;&amp; $(KUSTOMIZE) edit set image controller=$&#123;IMG&#125;        $(KUSTOMIZE) build config/default &gt; dist/install.yaml##@ Deploymentifndef ignore-not-found  ignore-not-found = falseendif.PHONY: installinstall: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.        $(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -.PHONY: uninstalluninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.        $(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -.PHONY: deploydeploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.        cd config/manager &amp;&amp; $(KUSTOMIZE) edit set image controller=$&#123;IMG&#125;        $(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -.PHONY: undeployundeploy: kustomize ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.        $(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -##@ Dependencies## Location to install dependencies toLOCALBIN ?= $(shell pwd)/bin$(LOCALBIN):        mkdir -p $(LOCALBIN)## Tool BinariesKUBECTL ?= kubectlKUSTOMIZE ?= $(LOCALBIN)/kustomizeCONTROLLER_GEN ?= $(LOCALBIN)/controller-genENVTEST ?= $(LOCALBIN)/setup-envtestGOLANGCI_LINT = $(LOCALBIN)/golangci-lint## Tool VersionsKUSTOMIZE_VERSION ?= v5.4.3CONTROLLER_TOOLS_VERSION ?= v0.16.1ENVTEST_VERSION ?= release-0.19GOLANGCI_LINT_VERSION ?= v1.59.1.PHONY: kustomizekustomize: $(KUSTOMIZE) ## Download kustomize locally if necessary.$(KUSTOMIZE): $(LOCALBIN)        $(call go-install-tool,$(KUSTOMIZE),sigs.k8s.io/kustomize/kustomize/v5,$(KUSTOMIZE_VERSION)).PHONY: controller-gencontroller-gen: $(CONTROLLER_GEN) ## Download controller-gen locally if necessary.$(CONTROLLER_GEN): $(LOCALBIN)        $(call go-install-tool,$(CONTROLLER_GEN),sigs.k8s.io/controller-tools/cmd/controller-gen,$(CONTROLLER_TOOLS_VERSION)).PHONY: envtestenvtest: $(ENVTEST) ## Download setup-envtest locally if necessary.$(ENVTEST): $(LOCALBIN)        $(call go-install-tool,$(ENVTEST),sigs.k8s.io/controller-runtime/tools/setup-envtest,$(ENVTEST_VERSION)).PHONY: golangci-lintgolangci-lint: $(GOLANGCI_LINT) ## Download golangci-lint locally if necessary.$(GOLANGCI_LINT): $(LOCALBIN)        $(call go-install-tool,$(GOLANGCI_LINT),github.com/golangci/golangci-lint/cmd/golangci-lint,$(GOLANGCI_LINT_VERSION))# go-install-tool will &#x27;go install&#x27; any package with custom target and name of binary, if it doesn&#x27;t exist# $1 - target path with name of binary# $2 - package url which can be installed# $3 - specific version of packagedefine go-install-tool@[ -f &quot;$(1)-$(3)&quot; ] || &#123; \\set -e; \\package=$(2)@$(3) ;\\echo &quot;Downloading $$&#123;package&#125;&quot; ;\\rm -f $(1) || true ;\\GOBIN=$(LOCALBIN) go install $$&#123;package&#125; ;\\mv $(1) $(1)-$(3) ;\\&#125; ;\\ln -sf $(1)-$(3) $(1)endef\n\n2.2 变量定义部分2.2.1 IMG（定义构建和推送Docker镜像的目标镜像URL）# Image URL to use all building/pushing image targetsIMG ?= controller:latest\n作用：定义构建和推送Docker镜像的目标镜像URL解释：如果没有在命令中指定IMG，则默认值为controller:latest\n2.2.2 ENVTEST_K8S_VERSION（定义envtest工具下载的Kubernetes资产版本）# ENVTEST_K8S_VERSION refers to the version of kubebuilder assets to be downloaded by envtest binary.ENVTEST_K8S_VERSION = 1.31.0\n作用：定义envtest工具下载的Kubernetes资产版本解释：envtest是一个用于测试的工具，这个变量制定了要下载的Kubernetes版本。\n2.2.3 GOBIN（当前使用的Go的安装路径）# Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)ifeq (,$(shell go env GOBIN))GOBIN=$(shell go env GOPATH)/binelseGOBIN=$(shell go env GOBIN)endif\n\n作用：获取当前使用的Go的安装路径解释：如果GONBIN环境变量未设置，则使用GOPATCH&#x2F;bin作为默认路径；否则使用GOBIN的值。\n2.2.4 CONTAINER_TOOL（用于构建镜像的容器工具）# CONTAINER_TOOL defines the container tool to be used for building images.# Be aware that the target commands are only tested with Docker which is# scaffolded by default. However, you might want to replace it to use other# tools. (i.e. podman)CONTAINER_TOOL ?= docker\n\n作用：定义用于构建镜像的容器工具解释：默认值为docker，但是可以替换为其他工具，例如podman\n2.2.5 SHELL 和 .SHELLFLAGS（设置Makefile使用的shell和shell选项）# Setting SHELL to bash allows bash commands to be executed by recipes.# Options are set to exit when a recipe line exits non-zero or a piped command fails.SHELL = /usr/bin/env bash -o pipefail.SHELLFLAGS = -ec\n\n作用：设置Makefile使用的shell和shell选项解释：\n\nSHELL设置为&#x2F;usr&#x2F;bin&#x2F;env bash -o pipefail\n.SHELLFLAGS 设置为-ec，表示shell应该立即退出并在任何命令失败时报告错误\n\n2.3 目标定义部分2.3.1 通用目标2.3.1.1 all（定义默认目标，调用build目标）.PHONY: allall: build\n作用：定义默认目标，调用build目标解释：当运行make时，如果没有指定目标。默认会执行build目标。\n2.3.1.2 help（显示帮助信息）.PHONY: helphelp: ## Display this help.        @awk &#x27;BEGIN &#123;FS = &quot;:.*##&quot;; printf &quot;\\nUsage:\\n  make \\033[36m&lt;target&gt;\\033[0m\\n&quot;&#125; /^[a-zA-Z_0-9-]+:.*?##/ &#123; printf &quot;  \\033[36m%-15s\\033[0m %s\\n&quot;, $$1, $$2 &#125; /^##@/ &#123; printf &quot;\\n\\033[1m%s\\033[0m\\n&quot;, substr($$0, 5) &#125; &#x27; $(MAKEFILE_LIST)\n作用：显示帮助信息解释：使用awk命令解析Makefile,提取每个目标的描述，并按类别组织显示。##@表示类别，##表示目标描述\n2.3.2 开发目标2.3.2.1 manifests（生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象）.PHONY: manifestsmanifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.        $(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases\n作用：生成WebhookConfiguration、ClusterRole和CustomResourceDefinition 对象解释：使用controller-gen工具生成必要的Kubernetes资源文件，并保存到config&#x2F;crd&#x2F;bases目录\n2.3.2.2 generate（生成包含DeepCopy、DeepCopyInto, 和DeepCopyObject方法实现的代码。）.PHONY: generategenerate: controller-gen ## Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.        $(CONTROLLER_GEN) object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;\n作用：生成包含DeepCopy、DeepCopyInto, 和DeepCopyObject方法实现的代码。解释：使用controller-gen工具生成GO代码，这些代码自动生成对象的深拷贝方法。\n2.3.2.3 fmt（格式化代码）.PHONY: fmtfmt: ## Run go fmt against code.        go fmt ./...\n作用：格式化代码解释：使用go fmt命令格式化项目中的所有Go代码\n2.3.2.4 vet（检查代码中的潜在问题）.PHONY: vetvet: ## Run go vet against code.        go vet ./...\n作用：检查代码中的潜在问题解释：使用go vet命令检查项目中的所有Go代码，查找可能的问题。\n2.3.2.5 test（运行测试）.PHONY: testtest: manifests generate fmt vet envtest ## Run tests.        KUBEBUILDER_ASSETS=&quot;$(shell $(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path)&quot; go test $$(go list ./... | grep -v /e2e) -coverprofile cover.out\n作用：运行测试解释：\n\n先运行manifests、generate、fmt和vet目标\n使用envtest工具设置环境变量KUBEBUILDER_ASSETS，然后运行项目中的所有测试(排除e2e测试)\n\n2.3.2.6 test-e2e（运行端到端测试）# Utilize Kind or modify the e2e tests to load the image locally, enabling compatibility with other vendors..PHONY: test-e2e  # Run the e2e tests against a Kind k8s instance that is spun up.test-e2e:        go test ./test/e2e/ -v -ginkgo.v\n作用：运行端到端测试解释：使用go test命令运行test&#x2F;e2e目录中的端到端测试，并启动详细输出。\n2.3.2.7 lint（运行代码风格检查）.PHONY: lintlint: golangci-lint ## Run golangci-lint linter        $(GOLANGCI_LINT) run\n作用：运行代码风格检查解释：使用golangci-lint工具运行代码风格检查\n2.3.2.8 lint-fix （运行代码风格检查并自动修复问题）.PHONY: lint-fixlint-fix: golangci-lint ## Run golangci-lint linter and perform fixes        $(GOLANGCI_LINT) run --fix\n作用：运行代码风格检查并自动修复问题解释：使用golangci-lint工具运行代码风格检查，并使用–fix选项自动修复发现的问题。\n2.3.3 构建目标2.3.3.1 build（构建controller的二进制文件）.PHONY: buildbuild: manifests generate fmt vet ## Build manager binary.        go build -o bin/manager cmd/main.go\n作用：构建controller的二进制文件解释：\n\n先运行manifests、generate、fmt和vet目标\n使用go build命令编译cmd&#x2F;main.go文件，并将生成的二进制文件保存到bin&#x2F;manager。\n\n2.3.3.2 run（在主机上运行controller）.PHONY: runrun: manifests generate fmt vet ## Run a controller from your host.        go run ./cmd/main.go\n作用：在主机上运行controller解释：\n\n先运行manifests、generate、fmt和vet目标\n使用go run命令运行cmd&#x2F;main.go文件\n\n2.3.3.3 docker-build（构建Docker镜像）# If you wish to build the manager image targeting other platforms you can use the --platform flag.# (i.e. docker build --platform linux/arm64). However, you must enable docker buildKit for it.# More info: https://docs.docker.com/develop/develop-images/build_enhancements/.PHONY: docker-builddocker-build: ## Build docker image with the manager.        $(CONTAINER_TOOL) build -t $&#123;IMG&#125; .\n作用：构建Docker镜像解释：使用CONTAINER_TOOL(默认为docker)构建docker镜像，并使用IMG变量指定镜像名称和标签\n2.3.3.4 docker-push（推送Docker镜像）.PHONY: docker-pushdocker-push: ## Push docker image with the manager.        $(CONTAINER_TOOL) push $&#123;IMG&#125;\n作用：推送Docker镜像解释：使用CONTAINER_TOOL(默认为docker)推送构建好的Docker镜像到指定的仓库\n2.3.3.5 docker-buildx（构建并推送多平台支持的Docker镜像）# PLATFORMS defines the target platforms for the manager image be built to provide support to multiple# architectures. (i.e. make docker-buildx IMG=myregistry/mypoperator:0.0.1). To use this option you need to:# - be able to use docker buildx. More info: https://docs.docker.com/build/buildx/# - have enabled BuildKit. More info: https://docs.docker.com/develop/develop-images/build_enhancements/# - be able to push the image to your registry (i.e. if you do not set a valid value via IMG=&lt;myregistry/image:&lt;tag&gt;&gt; then the export will fail)# To adequately provide solutions that are compatible with multiple platforms, you should consider using this option.PLATFORMS ?= linux/arm64,linux/amd64,linux/s390x,linux/ppc64le.PHONY: docker-buildxdocker-buildx: ## Build and push docker image for the manager for cross-platform support        # copy existing Dockerfile and insert --platform=$&#123;BUILDPLATFORM&#125; into Dockerfile.cross, and preserve the original Dockerfile        sed -e &#x27;1 s/\\(^FROM\\)/FROM --platform=\\$$\\&#123;BUILDPLATFORM\\&#125;/; t&#x27; -e &#x27; 1,// s//FROM --platform=\\$$\\&#123;BUILDPLATFORM\\&#125;/&#x27; Dockerfile &gt; Dockerfile.cross        - $(CONTAINER_TOOL) buildx create --name myapp-operator-builder        $(CONTAINER_TOOL) buildx use myapp-operator-builder        - $(CONTAINER_TOOL) buildx build --push --platform=$(PLATFORMS) --tag $&#123;IMG&#125; -f Dockerfile.cross .        - $(CONTAINER_TOOL) buildx rm myapp-operator-builder        rm Dockerfile.cross\n\n作用：构建并推送多平台支持的Docker镜像解释：\n\n使用sed命令复制现有的Dockerfile并插入–platform&#x3D;${BUILDPLATFORM}选项,生成Dockerfile.cross\n创建一个名为myapp-builder的构建器\n使用buildx构建并推送多平台镜像\n删除构建器和临时生成的Dockerfile.cross\n\n2.3.3.6 docker-installer（生成包含CRDs和部署的合并YAML文件）.PHONY: build-installerbuild-installer: manifests generate kustomize ## Generate a consolidated YAML with CRDs and deployment.        mkdir -p dist        cd config/manager &amp;&amp; $(KUSTOMIZE) edit set image controller=$&#123;IMG&#125;        $(KUSTOMIZE) build config/default &gt; dist/install.yaml\n作用：生成包含CRDs和部署的合并YAML文件解释：\n\n创建dist目录\n使用kustomize工具设置controller镜像为IMG\n使用kustomize构建config&#x2F;default目录中的资源，并保存到dist&#x2F;install.yaml\n\n2.3.4 部署目标2.3.4.1 install（安装CRDs到Kubernetes集群）.PHONY: installinstall: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.        $(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -\n作用：安装CRDs到Kubernetes集群解释：\n\n先运行manifests和kustomize目标\n使用kustomize 构建config&#x2F;crd目录中的资源，并使用kubectl将其应用到集群。\n\n2.3.4.2 uninstall（从Kubernetes集群卸载CRDs）.PHONY: uninstalluninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.        $(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n\n作用：从Kubernetes集群卸载CRDs解释：\n\n先运行manifests和kustomize目标\n使用kustomize 构建config&#x2F;crd目录中的资源，并使用kubectl将其从集群中删除，可以通过ignore-not-found&#x3D;true忽略资源未找到的错误\n\n2.3.4.3 deploy（部署controller到Kubernetes集群）.PHONY: deploydeploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.        cd config/manager &amp;&amp; $(KUSTOMIZE) edit set image controller=$&#123;IMG&#125;        $(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -\n作用：部署controller到Kubernetes集群解释：\n\n先运行manifests和kustomize目标\n使用kustomize设置controller镜像为IMG\n使用kustomize构建config&#x2F;default目录中的资源，并使用kubectl将其应用到集群\n\n2.3.4.4 undeploy（从Kubernetes集群中卸载Controller）.PHONY: undeployundeploy: kustomize ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.        $(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -\n作用：从Kubernetes集群中卸载Controller解释：使用kustomize 构建config&#x2F;default目录中的资源，并使用kubectl将其从集群中删除。可以通过ignore-not-found&#x3D;true忽略资源未找到的错误\n2.3.5 依赖目标2.3.5.1 ignore-not-found（定义ignore-not-found变量，默认值为false）ifndef ignore-not-found  ignore-not-found = falseendif\n作用：定义ignore-not-found变量，默认值为false解释：如果命令行中未指定ignore-not-found，则默认值为false\n2.3.5.2 LOCALBIN（定义本地二进制文件的安装目录）## Location to install dependencies toLOCALBIN ?= $(shell pwd)/bin$(LOCALBIN):        mkdir -p $(LOCALBIN)\n作用：定义本地二进制文件的安装目录，默认为当前目录下的bin目录解释：如果LOCALBIN未设置，则默认为$(shell pwd)&#x2F;bin。如果LOCALBIN目录不存在，则创建它。\n2.3.5.3 工具二进制文件（定义各种工具的路径和默认值）## Tool BinariesKUBECTL ?= kubectlKUSTOMIZE ?= $(LOCALBIN)/kustomizeCONTROLLER_GEN ?= $(LOCALBIN)/controller-genENVTEST ?= $(LOCALBIN)/setup-envtestGOLANGCI_LINT = $(LOCALBIN)/golangci-lint\n作用：定义各种工具的路径和默认值解释：\n\nKUBECTL：默认为kubectl\nKUSTOMIZE：默认为$(LOCALBIN)&#x2F;kustomize\nCONTROLLER_GEN：默认为$(LOCALBIN)&#x2F;controller-gen\nENVTEST：默认为$(LOCALBIN)&#x2F;setup-envtest\nGOLANGCI_LINT： 默认为$(LOCALBIN)&#x2F;golangci-lint\n\n2.3.5.4 工具版本（定义各个工具的版本）## Tool VersionsKUSTOMIZE_VERSION ?= v5.4.3CONTROLLER_TOOLS_VERSION ?= v0.16.1ENVTEST_VERSION ?= release-0.19GOLANGCI_LINT_VERSION ?= v1.59.1\n作用：定义各个工具的版本解释：\n\nKUSTOMIZE_VERSION：默认为v5.4.3\nCONTROLLER_TOOLS_VERSION：默认为v0.16.1\nENVTEST_VERSION：默认为release-0.19\nGOLANGCI_LINT_VERSION：默认为v1.59.1\n\n2.3.5.5 kustomize（下载kustomize工具）.PHONY: kustomizekustomize: $(KUSTOMIZE) ## Download kustomize locally if necessary.$(KUSTOMIZE): $(LOCALBIN)        $(call go-install-tool,$(KUSTOMIZE),sigs.k8s.io/kustomize/kustomize/v5,$(KUSTOMIZE_VERSION))\n作用：下载kustomize工具(如有必要)解释：如果kustomize文件不存在，则调用 go-install-tool函数下载kustomize工具\n2.3.5.6 controller-gen（下载controller-gen工具）.PHONY: controller-gencontroller-gen: $(CONTROLLER_GEN) ## Download controller-gen locally if necessary.$(CONTROLLER_GEN): $(LOCALBIN)        $(call go-install-tool,$(CONTROLLER_GEN),sigs.k8s.io/controller-tools/cmd/controller-gen,$(CONTROLLER_TOOLS_VERSION))\n作用：下载controller-gen工具(如有必要)解释：如果$(CONTROLLER_GEN)文件不存在，则调用 go-install-tool函数下载controller-gen工具\n2.3.5.7 envtest（下载setup-envtest工具）.PHONY: envtestenvtest: $(ENVTEST) ## Download setup-envtest locally if necessary.$(ENVTEST): $(LOCALBIN)        $(call go-install-tool,$(ENVTEST),sigs.k8s.io/controller-runtime/tools/setup-envtest,$(ENVTEST_VERSION))\n作用：下载setup-envtest工具(如有必要)解释：如果$(ENVTEST)文件不存在，则调用go-install-tool函数下载setup-envtes工具\n2.3.5.8 golangci-lint（下载golangci-lint工具）.PHONY: golangci-lintgolangci-lint: $(GOLANGCI_LINT) ## Download golangci-lint locally if necessary.$(GOLANGCI_LINT): $(LOCALBIN)        $(call go-install-tool,$(GOLANGCI_LINT),github.com/golangci/golangci-lint/cmd/golangci-lint,$(GOLANGCI_LINT_VERSION))\n作用：下载golangci-lint工具(如有必要)解释：如果$(GOLANGCI_LINT)文件不存在，则调用go-install-tool函数下载golangci-lint工具。\n2.4 自定义函数2.4.1 go-install-tool（下载并安装指定的Go包）# go-install-tool will &#x27;go install&#x27; any package with custom target and name of binary, if it doesn&#x27;t exist# $1 - target path with name of binary# $2 - package url which can be installed# $3 - specific version of packagedefine go-install-tool@[ -f &quot;$(1)-$(3)&quot; ] || &#123; \\set -e; \\package=$(2)@$(3) ;\\echo &quot;Downloading $$&#123;package&#125;&quot; ;\\rm -f $(1) || true ;\\GOBIN=$(LOCALBIN) go install $$&#123;package&#125; ;\\mv $(1) $(1)-$(3) ;\\&#125; ;\\ln -sf $(1)-$(3) $(1)endef\n作用：下载并安装指定的Go包解释：\n\n$1：目标路径和二进制文件名\n$2：包的URL\n$3:包的具体版本\n如果目标文件$(1)~$(3)不存在，则执行以下步骤\n设置set -e，确保任何命令执行失败立即退出\n设置package变量为包的URL和版本\n输出下载信息\n删除旧的二进制文件(如果存在)\n使用go install命令安装包，并将二进制文件保存在LOCALBIN目录\n将生成的二进制文件重命令为$(1)-$(3)\n最后创建一个符号链接，指向$(1)-$(3)\n\n","categories":["kubernetes/operator"],"tags":["kubernetes","operator"]},{"title":"operator开发脚手架","url":"/2025/07/kubernetes/operator-kai-fa/operator-kai-fa-jiao-shou-jia/","content":"1、脚手架工具Operator的实现方式主要包括OperatorSDK和KubeBuilder，目前KubeBuilder在阿里使用的比较多。\nKubeBuilder\nOperatorSDK\n我们这里主要是用KubeBuilder来进行，其中OperatorSDK其实也是应用了KubeBuilder。\n2、创建Operator工程创建脚手架工程：\nmkdir myapp-operatorcd myapp-operator/go env -w GO111MODULE=ongo env -w GOPROXY=https://goproxy.cn,directkubebuilder init --domain kubenode.kingtest.com\n\n创建脚手架结果：\nkubebuilder init --domain kubenode.kingtest.comINFO Writing kustomize manifests for you to edit... INFO Writing scaffold for you to edit...          INFO Get controller runtime:$ go get sigs.k8s.io/controller-runtime@v0.19.0 go: downloading sigs.k8s.io/controller-runtime v0.19.0go: downloading k8s.io/apimachinery v0.31.0go: downloading k8s.io/api v0.31.0go: downloading k8s.io/client-go v0.31.0go: downloading k8s.io/utils v0.0.0-20240711033017-18e509b52bc8go: downloading github.com/go-logr/logr v1.4.2go: downloading k8s.io/klog/v2 v2.130.1go: downloading golang.org/x/exp v0.0.0-20230515195305-f3d0a9c9a5ccgo: downloading github.com/evanphx/json-patch/v5 v5.9.0go: downloading github.com/prometheus/client_golang v1.19.1go: downloading gomodules.xyz/jsonpatch/v2 v2.4.0go: downloading github.com/gogo/protobuf v1.3.2go: downloading github.com/google/gofuzz v1.2.0go: downloading sigs.k8s.io/structured-merge-diff/v4 v4.4.1go: downloading k8s.io/apiextensions-apiserver v0.31.0go: downloading sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abdgo: downloading k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340go: downloading github.com/google/uuid v1.6.0go: downloading github.com/prometheus/client_model v0.6.1go: downloading github.com/prometheus/common v0.55.0go: downloading github.com/fsnotify/fsnotify v1.7.0go: downloading golang.org/x/net v0.26.0go: downloading gopkg.in/inf.v0 v0.9.1go: downloading github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9dago: downloading github.com/imdario/mergo v0.3.6go: downloading github.com/spf13/pflag v1.0.5go: downloading golang.org/x/term v0.21.0go: downloading github.com/golang/protobuf v1.5.4go: downloading github.com/google/gnostic-models v0.6.8go: downloading golang.org/x/time v0.3.0go: downloading sigs.k8s.io/yaml v1.4.0go: downloading github.com/beorn7/perks v1.0.1go: downloading github.com/cespare/xxhash/v2 v2.3.0go: downloading github.com/prometheus/procfs v0.15.1go: downloading golang.org/x/sys v0.21.0go: downloading google.golang.org/protobuf v1.34.2go: downloading golang.org/x/oauth2 v0.21.0go: downloading github.com/json-iterator/go v1.1.12go: downloading gopkg.in/yaml.v2 v2.4.0go: downloading github.com/fxamacker/cbor/v2 v2.7.0go: downloading github.com/google/go-cmp v0.6.0go: downloading gopkg.in/yaml.v3 v3.0.1go: downloading github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33ccgo: downloading github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822go: downloading github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1ddgo: downloading github.com/modern-go/reflect2 v1.0.2go: downloading github.com/go-openapi/jsonreference v0.20.2go: downloading github.com/go-openapi/swag v0.22.4go: downloading golang.org/x/text v0.16.0go: downloading github.com/go-openapi/jsonpointer v0.19.6go: downloading github.com/emicklei/go-restful/v3 v3.11.0go: downloading github.com/mailru/easyjson v0.7.7go: downloading github.com/josharian/intern v1.0.0go: downloading github.com/x448/float16 v0.8.4go: downloading github.com/pkg/errors v0.9.1INFO Update dependencies:$ go mod tidy           go: downloading github.com/onsi/ginkgo/v2 v2.19.0go: downloading github.com/stretchr/testify v1.9.0go: downloading github.com/onsi/gomega v1.33.1go: downloading github.com/go-logr/zapr v1.3.0go: downloading go.uber.org/zap v1.26.0go: downloading k8s.io/apiserver v0.31.0go: downloading go.uber.org/goleak v1.3.0go: downloading github.com/evanphx/json-patch v0.5.2go: downloading gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6cgo: downloading gopkg.in/evanphx/json-patch.v4 v4.12.0go: downloading github.com/kr/pretty v0.3.1go: downloading go.uber.org/multierr v1.11.0go: downloading github.com/go-task/slim-sprig/v3 v3.0.0go: downloading golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2dgo: downloading github.com/google/pprof v0.0.0-20240525223248-4bfdf5a9a2afgo: downloading github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2go: downloading github.com/rogpeppe/go-internal v1.12.0go: downloading github.com/kr/text v0.2.0go: downloading k8s.io/component-base v0.31.0go: downloading go.opentelemetry.io/otel v1.28.0go: downloading go.opentelemetry.io/otel/trace v1.28.0go: downloading google.golang.org/grpc v1.65.0go: downloading sigs.k8s.io/apiserver-network-proxy/konnectivity-client v0.30.3go: downloading golang.org/x/sync v0.7.0go: downloading github.com/google/cel-go v0.20.1go: downloading github.com/blang/semver/v4 v4.0.0go: downloading go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.53.0go: downloading go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.27.0go: downloading go.opentelemetry.io/otel/sdk v1.28.0go: downloading google.golang.org/genproto/googleapis/api v0.0.0-20240528184218-531527333157go: downloading github.com/felixge/httpsnoop v1.0.4go: downloading go.opentelemetry.io/otel/metric v1.28.0go: downloading github.com/asaskevich/govalidator v0.0.0-20190424111038-f61b66f89f4ago: downloading github.com/go-logr/stdr v1.2.2go: downloading google.golang.org/genproto/googleapis/rpc v0.0.0-20240701130421-f6361c86f094go: downloading github.com/antlr4-go/antlr/v4 v4.13.0go: downloading google.golang.org/genproto v0.0.0-20230822172742-b8732ec3820dgo: downloading github.com/spf13/cobra v1.8.1go: downloading github.com/stoewer/go-strcase v1.2.0go: downloading github.com/inconshreveable/mousetrap v1.1.0go: downloading go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.28.0go: downloading github.com/cenkalti/backoff/v4 v4.3.0go: downloading go.opentelemetry.io/proto/otlp v1.3.1go: downloading github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0go: downloading github.com/grpc-ecosystem/grpc-gateway v1.16.0Next: define a resource with:$ kubebuilder create api\n\n本步骤创建了 Go module 工程的模板文件，引入了必要的依赖。这里先设置了go的代理，不然会无法下载依赖。第一次新建工程时，会下载一些依赖(go: downloading 。。。)查看此时的项目结构:\n.├── cmd│   └── main.go├── config│   ├── default│   │   ├── kustomization.yaml│   │   ├── manager_metrics_patch.yaml│   │   └── metrics_service.yaml│   ├── manager│   │   ├── kustomization.yaml│   │   └── manager.yaml│   ├── network-policy│   │   ├── allow-metrics-traffic.yaml│   │   └── kustomization.yaml│   ├── prometheus│   │   ├── kustomization.yaml│   │   └── monitor.yaml│   └── rbac│       ├── kustomization.yaml│       ├── leader_election_role_binding.yaml│       ├── leader_election_role.yaml│       ├── metrics_auth_role_binding.yaml│       ├── metrics_auth_role.yaml│       ├── metrics_reader_role.yaml│       ├── role_binding.yaml│       ├── role.yaml│       └── service_account.yaml├── Dockerfile├── go.mod├── go.sum├── hack│   └── boilerplate.go.txt├── Makefile├── PROJECT├── README.md└── test    ├── e2e    │   ├── e2e_suite_test.go    │   └── e2e_test.go    └── utils        └── utils.go12 directories, 29 files\n\n创建API，生成CRD和Controller:\nkubebuilder create api --group apps --version v1 --kind Myapp\n\n参数含义：\n\ngroup参数表示组的概念\nversion定义版本\nkind定义自定义资源类型\n以上参数组成 自定义yaml 的 apiVersion和kind\n执行kubebuilder create api时直接带上–namespaced&#x3D;false可以将该对象设计为集群级别（类似node、pv）\n\n创建API结果：\nkubebuilder create api --group apps --version v1 --kind MyappINFO Create Resource [y/n]                        yINFO Create Controller [y/n]                      yINFO Writing kustomize manifests for you to edit... INFO Writing scaffold for you to edit...          INFO api/v1/myapp_types.go                        INFO api/v1/groupversion_info.go                  INFO internal/controller/suite_test.go            INFO internal/controller/myapp_controller.go      INFO internal/controller/myapp_controller_test.go INFO Update dependencies:$ go mod tidy           INFO Running make:$ make generate                mkdir -p /home/king/workspace/king-devops/operator/myapp-operator/binDownloading sigs.k8s.io/controller-tools/cmd/controller-gen@v0.16.1go: downloading sigs.k8s.io/controller-tools v0.16.1go: downloading github.com/fatih/color v1.17.0go: downloading golang.org/x/tools v0.24.0go: downloading github.com/gobuffalo/flect v1.0.2go: downloading golang.org/x/net v0.28.0go: downloading github.com/mattn/go-isatty v0.0.20go: downloading github.com/mattn/go-colorable v0.1.13go: downloading golang.org/x/sys v0.23.0go: downloading golang.org/x/sync v0.8.0go: downloading golang.org/x/mod v0.20.0go: downloading golang.org/x/text v0.17.0/home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;Next: implement your new API and generate the manifests (e.g. CRDs,CRs) with:$ make manifests\n\n此时的目录结构：\n.├── api│   └── v1│       ├── groupversion_info.go│       ├── myapp_types.go│       └── zz_generated.deepcopy.go├── bin│   ├── controller-gen -&gt; /home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen-v0.16.1│   └── controller-gen-v0.16.1├── cmd│   └── main.go├── config│   ├── crd│   │   ├── kustomization.yaml│   │   └── kustomizeconfig.yaml│   ├── default│   │   ├── kustomization.yaml│   │   ├── manager_metrics_patch.yaml│   │   └── metrics_service.yaml│   ├── manager│   │   ├── kustomization.yaml│   │   └── manager.yaml│   ├── network-policy│   │   ├── allow-metrics-traffic.yaml│   │   └── kustomization.yaml│   ├── prometheus│   │   ├── kustomization.yaml│   │   └── monitor.yaml│   ├── rbac│   │   ├── kustomization.yaml│   │   ├── leader_election_role_binding.yaml│   │   ├── leader_election_role.yaml│   │   ├── metrics_auth_role_binding.yaml│   │   ├── metrics_auth_role.yaml│   │   ├── metrics_reader_role.yaml│   │   ├── myapp_editor_role.yaml│   │   ├── myapp_viewer_role.yaml│   │   ├── role_binding.yaml│   │   ├── role.yaml│   │   └── service_account.yaml│   └── samples│       ├── apps_v1_myapp.yaml│       └── kustomization.yaml├── Dockerfile├── go.mod├── go.sum├── hack│   └── boilerplate.go.txt├── internal│   └── controller│       ├── myapp_controller.go│       ├── myapp_controller_test.go│       └── suite_test.go├── Makefile├── PROJECT├── README.md└── test    ├── e2e    │   ├── e2e_suite_test.go    │   └── e2e_test.go    └── utils        └── utils.go19 directories, 43 files\n\n如果需要在Nginx CRUD 时进行合法性检查， 可以生成webhook:\nkubebuilder create webhook --group apps --version v1 --kind Myapp --conversion --defaulting --programmatic-validation\n\n参数含义：\n\ngroup参数表示组的概念\nversion定义版本\nkind定义自定义资源类型\n以上参数组成 自定义yaml 的 apiVersion和kind\ndefaulting：默认值设置（Defaulting） 的目的是在CRD对象创建或更新时，如果某些字段没有被显式设置值，自动为其填充一个合理的默认值。这对于确保资源实例有一套统一的、预先定义的配置非常有用，可以减少用户的配置负担，并保证资源的一致性。例如，如果你定义了一个监控应用的CRD，可能希望默认开启日志记录功能，除非用户明确关闭它。\nprogrammatic-validation：程序化验证（Programmatic Validation） 则是在CRD对象创建或更新前，对提交的数据进行逻辑验证的机制。与CRD schema提供的静态验证相比，程序化验证可以实现更加复杂的业务逻辑验证。这意味着你可以在webhook服务中编写代码来检查CRD实例的数据是否满足特定的业务规则或约束条件。例如，你可以验证一个资源请求的内存配额是否超过了集群的最大允许值，或者确保引用的其他资源确实存在。\nconversion：自动数据迁移：conversion webhook可以在后台自动将CRD的一个版本转换为另一个版本，无需人工干预，从而简化了版本升级过程。\n兼容性保障：即使API发生变化，也能确保旧客户端和新客户端都能够正常工作，提高了系统的向后和向前兼容性。\n复杂逻辑处理：对于简单的字段添加或删除，Kubernetes的自动转换器可能足够用。但涉及到复杂的数据结构调整或逻辑变换时，就需要自定义conversion webhook来精确控制转换过程。\n\n\n\n执行结果：\nkubebuilder create webhook --group apps --version v1 --kind Myapp --conversion --defaulting --programmatic-validationINFO Writing kustomize manifests for you to edit... INFO Writing scaffold for you to edit...          INFO api/v1/myapp_webhook.go                      INFO api/v1/myapp_webhook_test.go                 INFO Webhook server has been set up for you.You need to implement the conversion.Hub and conversion.Convertible interfaces for your CRD types. INFO api/v1/webhook_suite_test.go                 INFO Update dependencies:$ go mod tidy           INFO Running make:$ make generate                /home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;Next: implement your new Webhook and generate the manifests with:$ make manifests\n\n此时目录结构：\n.├── api│   └── v1│       ├── groupversion_info.go│       ├── myapp_types.go│       ├── myapp_webhook.go│       ├── myapp_webhook_test.go│       ├── webhook_suite_test.go│       └── zz_generated.deepcopy.go├── bin│   ├── controller-gen -&gt; /home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen-v0.16.1│   └── controller-gen-v0.16.1├── cmd│   └── main.go├── config│   ├── certmanager│   │   ├── certificate.yaml│   │   ├── kustomization.yaml│   │   └── kustomizeconfig.yaml│   ├── crd│   │   ├── kustomization.yaml│   │   ├── kustomizeconfig.yaml│   │   └── patches│   │       ├── cainjection_in_myapps.yaml│   │       └── webhook_in_myapps.yaml│   ├── default│   │   ├── kustomization.yaml│   │   ├── manager_metrics_patch.yaml│   │   ├── manager_webhook_patch.yaml│   │   ├── metrics_service.yaml│   │   └── webhookcainjection_patch.yaml│   ├── manager│   │   ├── kustomization.yaml│   │   └── manager.yaml│   ├── network-policy│   │   ├── allow-metrics-traffic.yaml│   │   ├── allow-webhook-traffic.yaml│   │   └── kustomization.yaml│   ├── prometheus│   │   ├── kustomization.yaml│   │   └── monitor.yaml│   ├── rbac│   │   ├── kustomization.yaml│   │   ├── leader_election_role_binding.yaml│   │   ├── leader_election_role.yaml│   │   ├── metrics_auth_role_binding.yaml│   │   ├── metrics_auth_role.yaml│   │   ├── metrics_reader_role.yaml│   │   ├── myapp_editor_role.yaml│   │   ├── myapp_viewer_role.yaml│   │   ├── role_binding.yaml│   │   ├── role.yaml│   │   └── service_account.yaml│   ├── samples│   │   ├── apps_v1_myapp.yaml│   │   └── kustomization.yaml│   └── webhook│       ├── kustomization.yaml│       ├── kustomizeconfig.yaml│       └── service.yaml├── Dockerfile├── go.mod├── go.sum├── hack│   └── boilerplate.go.txt├── internal│   └── controller│       ├── myapp_controller.go│       ├── myapp_controller_test.go│       └── suite_test.go├── Makefile├── PROJECT├── README.md└── test    ├── e2e    │   ├── e2e_suite_test.go    │   └── e2e_test.go    └── utils        └── utils.go22 directories, 57 files\n\n最终的工程结构：\n.├── api│   └── v1│       ├── groupversion_info.go│       ├── myapp_types.go│       ├── myapp_webhook.go│       ├── myapp_webhook_test.go│       ├── webhook_suite_test.go│       └── zz_generated.deepcopy.go├── bin│   ├── controller-gen -&gt; /home/king/workspace/king-devops/operator/myapp-operator/bin/controller-gen-v0.16.1│   └── controller-gen-v0.16.1├── cmd│   └── main.go├── config│   ├── certmanager│   │   ├── certificate.yaml│   │   ├── kustomization.yaml│   │   └── kustomizeconfig.yaml│   ├── crd│   │   ├── kustomization.yaml│   │   ├── kustomizeconfig.yaml│   │   └── patches│   │       ├── cainjection_in_myapps.yaml│   │       └── webhook_in_myapps.yaml│   ├── default│   │   ├── kustomization.yaml│   │   ├── manager_metrics_patch.yaml│   │   ├── manager_webhook_patch.yaml│   │   ├── metrics_service.yaml│   │   └── webhookcainjection_patch.yaml│   ├── manager│   │   ├── kustomization.yaml│   │   └── manager.yaml│   ├── network-policy│   │   ├── allow-metrics-traffic.yaml│   │   ├── allow-webhook-traffic.yaml│   │   └── kustomization.yaml│   ├── prometheus│   │   ├── kustomization.yaml│   │   └── monitor.yaml│   ├── rbac│   │   ├── kustomization.yaml│   │   ├── leader_election_role_binding.yaml│   │   ├── leader_election_role.yaml│   │   ├── metrics_auth_role_binding.yaml│   │   ├── metrics_auth_role.yaml│   │   ├── metrics_reader_role.yaml│   │   ├── myapp_editor_role.yaml│   │   ├── myapp_viewer_role.yaml│   │   ├── role_binding.yaml│   │   ├── role.yaml│   │   └── service_account.yaml│   ├── samples│   │   ├── apps_v1_myapp.yaml│   │   └── kustomization.yaml│   └── webhook│       ├── kustomization.yaml│       ├── kustomizeconfig.yaml│       └── service.yaml├── Dockerfile├── go.mod├── go.sum├── hack│   └── boilerplate.go.txt├── internal│   └── controller│       ├── myapp_controller.go│       ├── myapp_controller_test.go│       └── suite_test.go├── Makefile├── PROJECT├── README.md└── test    ├── e2e    │   ├── e2e_suite_test.go    │   └── e2e_test.go    └── utils        └── utils.go22 directories, 57 files\n\n关键目录及其内容的概述：\n\nDockerfile: 定义了用于构建Operator容器镜像的Docker配置文件。\nMakefile: 包含了一系列预定义的Make任务，用于构建、测试、运行和部署Operator。\nPROJECT: 存储项目元数据的文件，Kubebuilder使用它来跟踪项目的状态和配置。\nREADME.md: 项目的主要说明文档，通常包含项目简介、安装指南、开发流程等信息。\napi&#x2F;v1: 定义了自定义资源定义(CRD)的目录，其中包含了资源的Go类型定义及DeepCopy生成文件。\nmyapp_types.go: 资源类型的定义(myapp_types.go)\nmyapp_webhook.go: webhook逻辑\nzz_generated.deepcopy.go: 深拷贝生成代码\ngroupversion_info.go: 定义了API组和版本信息。\n\n\nbin: 存放项目依赖的可执行文件，如controller-gen，用于CRD代码生成。\ncmd&#x2F;main.go: Operator的入口点，负责初始化和运行控制器。\nconfig: 配置目录，包含用于Kubernetes资源部署的各种Kustomize配置。\ncrd&#x2F;: 用于CRD资源的Kustomize配置。\ndefault&#x2F;: 应用默认的RBAC、服务等配置。\nmanager&#x2F;: Operator Manager的部署配置。\nnetwork-policy&#x2F;, prometheus&#x2F;, rbac&#x2F;, samples&#x2F;: 分别包含网络策略、Prometheus监控配置、RBAC规则和示例资源的配置。\n\n\ngo.mod, go.sum: Go模块管理文件，记录项目依赖。\nhack&#x2F;boilerplate.go.txt: 用于代码生成的模板文件，保持版权头部一致性。\ninternal&#x2F;controller: Operator控制器的核心逻辑所在，包含监控资源(Myapp)的控制器实现。\ntest: 测试相关目录。\ne2e&#x2F;: 端到端测试代码，验证整个Operator在Kubernetes集群上的行为。\nutils&#x2F;: 测试辅助工具和函数。\n\n\n\n3、编写代码下面主要以Deployment为例，核心逻辑是把自定义CR（Myapp）当做终态，把Deployment当做运行态，通过比对属性的不一致，编写相关的Reconcile逻辑。\n一张图解释各种资源和 Controller 的关系：\n3.1 定义 CRD在api&#x2F;v1&#x2F;myapp_type.go中定义 Spec 和 Status\n// MyappSpec defines the desired state of Myapptype MyappSpec struct &#123;        // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster        // Important: Run &quot;make&quot; to regenerate code after modifying this file        // Foo is an example field of Myapp. Edit myapp_types.go to remove/update        Foo                   string `json:&quot;foo,omitempty&quot;`        appsv1.DeploymentSpec `json:&quot;,inline&quot;`&#125;// MyappStatus defines the observed state of Myapptype MyappStatus struct &#123;        // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster        // Important: Run &quot;make&quot; to regenerate code after modifying this file        appsv1.DeploymentSpec `json:&quot;,inline&quot;`&#125;// +kubebuilder:object:root=true// +kubebuilder:subresource:status// Myapp is the Schema for the myapps APItype Myapp struct &#123;        metav1.TypeMeta   `json:&quot;,inline&quot;`        metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`        Spec   MyappSpec   `json:&quot;spec,omitempty&quot;`        Status MyappStatus `json:&quot;status,omitempty&quot;`&#125;\n\n注意+kubebuilder并非普通注释，不能随意删除。\n3.2 编写Reconcile逻辑在internal&#x2F;controller&#x2F;myapp_controller.go中实现 Reconcile 逻辑\nfunc (r *MyappReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) &#123;        logger := log.FromContext(ctx)        // TODO(user): your logic here        logger.Info(&quot;start Reconcile&quot; + req.Name)        return ctrl.Result&#123;&#125;, nil&#125;\n3.3 修改Webhook在api&#x2F;v1&#x2F;myapp_webhook.go中根据需要进行修改\nfunc (r *Myapp) ValidateCreate() error func (r *Myapp) ValidateUpdate(old runtime.Object) errorfunc (r *Myapp) Default()\n\n3.4 修改main入口以前的老版本kubebuilder生成的项目，需要在cmd&#x2F;main.go添加监听的namespace但是新版本没有这个信息，以下是示例：\nmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options&#123;                Scheme:                 scheme,                Metrics:                metricsServerOptions,                WebhookServer:          webhookServer,                HealthProbeBindAddress: probeAddr,                LeaderElection:         enableLeaderElection,                LeaderElectionID:       &quot;6a511ed4.kubenode.kingtest.com&quot;,                // LeaderElectionReleaseOnCancel defines if the leader should step down voluntarily                // when the Manager ends. This requires the binary to immediately end when the                // Manager is stopped, otherwise, this setting is unsafe. Setting this significantly                // speeds up voluntary leader transitions as the new leader don&#x27;t have to wait                // LeaseDuration time first.                //                // In the default scaffold provided, the program ends immediately after                // the manager stops, so would be fine to enable this option. However,                // if you are doing or is intended to do any operation such as perform cleanups                // after the manager stops then its usage might be unsafe.                // LeaderElectionReleaseOnCancel: true,\n\n如果想监听固定的namespace信息，可以在Reconcile内实现。\n","categories":["kubernetes/operator"],"tags":["kubernetes","operator"]},{"title":"operator简介","url":"/2025/07/kubernetes/operator-kai-fa/operator-jian-jie/","content":"1 kubernetes背景1.1 Controller模式controller通过根据被控制对象的属性和字段来实现编排，对于每一个build-in的资源类型，都有对应的controller。\n比如以下是一个简单的Deployment的yaml示例：\n其对应的Deployment Controller编排动作是确保app&#x3D;test的Pod数量为2，Pod的属性和字段由spec.template定义\napiVersion: apps/v1kind: Deploymentmetadata:  name: testspec:  selector:    matchLabels:      app: test  replicas: 2  template:    metadata:      labels:        app: test    spec:      containers:      - name: nginx        image: nginx:1.7.9        ports:        - containerPort: 80\n\n所有控制器的主要操作都是一个调谐循环（Reconcile loop）。主要有三步：\n\n观察期望的状态。\n观察所管理资源的当前状态。\n采取行动，使托管的资源处在期望的状态。for &#123;    actualState := GetResourceActualState(rsvc)    expectState := GetResourceExpectState(rsvc)    if actualState == expectState &#123;        // do nothing    &#125; else &#123;        Reconcile(rsvc)    &#125;&#125;\n\n1.2 声明式API声明式API：告诉K8S你要什么，而不是告诉它你怎么做。\n声明式API的操作体现在kubectl apply命令上，在对象创建和后续修改更新都使用apply命令，告诉k8s对象的终态即可，底层的实现是一个对原有API对象的PATCH操作实现的，可以一次性处理多个写操作，相对于命令时一个个的命令大大提高了操作效率。\n比如上面的deployment.yaml文件，在提交后会通过Group&#x2F;Version&#x2F;Resource的分级来找到deployment在GO语言中的结构体定义，从而将YAML描述转换成一个序列化的deployment对象，并通过etcd的API将其保存起来。\n1.3 “原生的资源-Controller”的问题\n原生资源不够用\nK8S内部的基础资源，如Pod、Statefulset、Service，能够覆盖大部分应用的工作模式。\n但对以下情况不友好：\n有些应用组件无法找到合适的基础资源作为模板，比如GPU资源、训练数据集、训练任务等；\n有些应用组件需要多个基础资源共同输出一项能力，会变得很复杂；\n\n\n\n\n细粒度操作繁琐\n需要对云原生K8S的细粒度资源的种类比较熟悉；\n应用在提供服务时，存在通过Label等标签信息对应的逻辑。\n\n\n缺乏定制化能力，如：\n一些简单的中间件集成\n对不同资源的操作时序的流水线管理。\n\n\n\n1.4 拓展自定义资源与控制器主要做两件事：\n\n编写自定义资源，并将其部署到K8S集群中: 通过编写符合K8S资源和结构属性的文件，使得K8S能够校验该资源并进行持久化。\n编写其对应的控制器，并将其部署到K8S集群中: 通过实现调谐逻辑，来完成资源编排的实际需求。\n\n2、Operator 介绍2.1 Operator借助 Kubernetes 的控制器模式，编写自定义的编排规则，完成对自定义资源的操作，比如增删改查等。\n一般来说，Operator&#x3D;CRD(自定义资源)+Controller(自定义控制器)+Webhook(Admission根据实际情况选择是否添加)\n2.2 自定义资源 CRDCRD（Custom Resource Definition）是用户的自定义资源类型，可以基于Kubernetes的API Server进行管理。其实例为CR（Custom Resource）。\napiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata:  name: test ## CRD名称spec:  conversion:    strategy: None  group: tq   # REST API: /apis/&lt;group&gt;/&lt;version&gt;  names:    kind: App    listKind: AppList    plural: apps    singular: app  scope: Namespaced  # 资源是否区分namespace  versions: [] # 资源的版本信息，例如：v1beta1、v1等\n\n2.3 自定义控制器 ControllerKubernetes控制器会监视资源的创建&#x2F;更新&#x2F;删除事件，并触发 Reconcile 函数作为响应。Reconcile 是一个使用 object（Resource 的实例）的命名空间和实例名来调用的函数，使object的实际状态与object的Spec中定义的状态保持一致。 调用完成后，Reconcile 会将object的状态更新为当前实际状态。\n在实际环境中，Controller会被打包成镜像，以Deployment拉起的Pod的形式应用在集群中，并用service中的ClusterIP方式进行服务暴露发现。\n2.4 名词解释\nCRD（Custom Resource Definition）: CRD允许用户在Kubernetes API中定义新的资源类型。这意味着你可以创建符合自己应用或服务特性的资源，这些资源能够被Kubernetes以类似于内置资源（如Pods、Services）的方式来管理和操作。CRD实质上是扩展了Kubernetes API的功能，允许你定义资源的名称、字段结构、验证规则等元数据。通过YAML或JSON文件定义CRD后，Kubernetes会将其注册到API服务器，使得用户可以通过Kubernetes API来创建、读取、更新和删除这些自定义资源。\nCR（Custom Resource）: CR是基于CRD定义的具体实例。一旦定义了CRD，用户就可以创建该类型的资源实例，这些实例就是CR。每个CR代表了特定的配置或状态，比如一个特定数据库实例的配置、一个复杂应用的部署配置等。CR的工作原理类似于Kubernetes中的其他对象（如Deployment、Service），但它们是用户自定义的，能够更好地适应特定应用的需求。开发者或运维人员通过创建CR来声明他们希望在集群中部署或管理的资源状态，而与之关联的Operator会根据这些声明来创建、维护和更新实际的Kubernetes对象。\nGVK（Group, Version, Kind）: GVK代表了资源的组（Group）、版本（Version）和种类（Kind）。它是从资源的API层面描述资源的一种方式。\nGroup：指资源所属的API组，用于区分不同功能领域的API。例如，apps&#x2F;v1中的apps就是组名，它通常对应着Kubernetes的一个特定功能领域，如应用程序管理。\nVersion：资源的API版本，表明资源定义遵循的特定API版本规范。随着Kubernetes的发展，资源的API可能会有变化，版本号帮助区分这些不同的规范。\nKind：资源的类型，如Pod、Service、Deployment等，它定义了一类资源的基本结构和用途。\n\n\nGVR (Group, Version, Resource): GVR与GVK类似，但用Resource替换了Kind，它从资源的访问路径角度描述资源。\nGroup：指资源所属的API组，用于区分不同功能领域的API。例如，apps&#x2F;v1中的apps就是组名，它通常对应着Kubernetes的一个特定功能领域，如应用程序管理。\nVersion：资源的API版本，表明资源定义遵循的特定API版本规范。随着Kubernetes的发展，资源的API可能会有变化，版本号帮助区分这些不同的规范。\nResource: 指的是资源在API路径中的名称，它直接关联到API端点，是用于与API服务器交互时的URL路径组成部分。Resource通常与Kind相似或相同，但在某些情况下可能有细微差别，特别是在使用自定义资源定义（CRDs）时。\n\n\nScheme: 负责管理和注册API类型（如各种资源对象）以及它们之间的转换关系。简单来说，Scheme是理解和操作Kubernetes对象的基础框架，它确保了对象的序列化、反序列化以及类型转换能够正确进行\n类型注册：Scheme允许你注册自定义或内置的API类型。这意味着你可以告诉Scheme如何识别和处理特定的Go结构体类型，比如Pod、Service、或者自定义资源定义（CRDs）等。\n对象转换：它提供了类型转换功能，允许在不同的API版本之间转换对象。这对于处理API的版本兼容性和升级非常重要。\n默认化：Scheme支持为对象设置默认值。当创建或更新资源时，如果没有明确指定某些字段，Scheme可以根据预设规则自动填充默认值\n序列化与反序列化：Scheme知道如何将Go结构体转换为JSON或YAML格式的数据（序列化），以及如何将这些数据反序列化回Go结构体。这对于与Kubernetes API服务器通信至关重要。\n元数据处理：它还负责处理对象的元数据，如API版本信息和Kind信息，确保这些信息在序列化和反序列化过程中得到妥善处理。\n\n\nManager：Manager整合了一系列功能，使得开发者能够更容易地创建和管理自定义资源（CRDs）及其对应的控制器逻辑。以下是Manager的一些关键职责和功能\n自定义资源管理：Manager负责监听和管理自定义资源定义（CRDs）的变化，当CR实例被创建、更新或删除时，它能确保相应的控制器逻辑得到执行。\n控制器注册：它提供了一个中心点来注册和管理不同的控制器（Controllers）。控制器是执行具体业务逻辑的组件，负责维护期望状态与实际集群状态的一致性。\n依赖注入：Manager框架通常支持依赖注入，使得开发者可以轻松地复用和共享服务，比如客户端集（ClientSets）用于与Kubernetes API交互、日志记录器、事件记录器等。\n领导者选举：在分布式环境中，Manager可以实现领导者选举逻辑，确保在一个集群中只有一个活跃的实例在执行操作，避免冲突和重复处理。\nWebhook注册：它还可能支持注册和管理自定义的Admission Webhooks，允许在资源创建或修改时插入自定义验证或修改逻辑。\n生命周期管理：Manager负责启动、停止控制器，以及处理它们的生命周期事件，确保资源的有效管理和清理。\n\n\nController：Kubebuilder为我们生成的脚手架文件，我们只需要实现Reconcile方法即可。\nInformers：在Kubernetes中，Informers是客户端库（client-go）提供的一个核心组件，用于高效地监听和同步Kubernetes API资源的变化。它是实现控制器模式的关键技术之一，尤其是对于那些需要根据资源状态变化做出反应的组件，如自定义控制器、Operator等\n资源监听：Informers持续监听Kubernetes API服务器中指定资源类型（如Pods、Deployments等）的增删改事件，通过watch API机制实现近乎实时的资源变化感知。\n缓存同步：它在本地维护一个资源对象的缓存副本，这个缓存会随着API服务器的事件更新而自动保持最新状态。这样，控制器或其他组件可以直接查询本地缓存获取资源列表或详情，而不需要频繁地直接查询API服务器，大大提高了效率。\n事件处理：Informers提供回调机制，允许用户注册处理函数（如EventHandler或Informer的事件处理器），当资源发生变化时自动触发这些函数，执行相应的业务逻辑。\n列表和详细信息的统一管理：通过Lister接口，Informers不仅能够提供资源列表，还能高效地获取单个资源的详细信息，进一步简化了资源的管理和查询过程。\n\n\nIndex：由于Controller经常要对Cache进行查询，Kubebuilder提供Index utility给Cache加索引提升查询效率。\nFinalizer：在Kubernetes中，Finalizer 是一种高级特性，用于确保资源在其被删除之前能完成必要的清理工作。它作为一种保障机制，可以让控制器或操作者有机会在对象被永久删除前执行一些清理、备份或其他必要的操作，确保资源的优雅删除和资源使用的完整性。\n阻止删除：删除操作会被暂停，直到所有列出的finalizer都被处理完毕。\n通知处理者：Kubernetes会通知或等待负责该finalizer的控制器（或外部系统）完成相应的清理操作。\n清除Finalizer：一旦处理者完成了其任务，它会通过API更新该对象，从metadata.finalizers字段中移除相应的finalizer项。\n继续删除流程：当所有finalizer都被移除后，Kubernetes才会最终从API服务器中删除该对象。\n配置Finalizer：Finalizer通常是通过自定义控制器（如Operator）动态添加和管理的。控制器可以在创建或更新资源时向对象的metadata.finalizers字段添加自定义的finalizer名称，同样，在完成清理工作后，需要通过API调用来移除这个finalizer，以允许资源被彻底删除。\n\n\nOwnerReference工作原理：在Kubernetes中，OwnerReference 是一种机制，用于建立资源对象之间的所有权关系。这种关系定义了资源的生命周期依赖性，即“拥有者”资源（owner）控制着“被拥有”资源（owned resource）的生命周期。当一个资源作为另一个资源的Owner时，它会对被拥有资源的创建、更新和删除产生影响，确保资源之间的一致性和自动化管理。\n自动清理（Garbage Collection）：当一个资源对象被删除，并且它拥有其他资源时，Kubernetes的垃圾收集机制会自动删除这些被拥有的资源，确保资源依赖关系被正确清理，避免孤儿资源的产生。\n生命周期耦合：被拥有资源的生命周期与拥有者资源紧密相连。如果拥有者资源被修改或删除，Kubernetes会相应地更新或清理被拥有资源。\n防止循环依赖：Kubernetes禁止创建会导致循环OwnerReference关系的资源，以防止资源管理混乱和潜在的死锁情况。\n\n\nOwnerReference结构定义：OwnerReference是以API对象的一个字段形式存在的，通常位于被拥有资源的metadata.ownerReferences字段中。它包含几个关键属性：\nAPIVersion：拥有者的API版本。\nKind：拥有者的资源类型。\nName：拥有者的名称。\nUID：拥有者的唯一标识符（UID），这是区分不同资源实例的关键。\nController（可选）：布尔值，表示是否由一个控制器（如Deployment或StatefulSet）管理这个关系。如果是控制器，Kubernetes会在适当的时候自动管理被拥有资源。\n\n\n\n2.5 CRD命名规范CRD的全名必须是符合如下的命名规范：${Kind}.${Group}.${Organization}.kubenode.alibaba-inc.com.\n\n${Organization}：一般为仓库的git group，即团队英文简称\n${Group}：必须是一种功能类别，如ops、apps、auth等，尽量用精简的单个英文单词的方式传达你的CRD属于的”类别”。组成的字母必须小写\n${Kind}：即为CRD真正的短名字，用精简的单个或多个英文单词的拼接来明明真正的CRD短名字。如AdvanceDeployment，NetBook等。使用大驼峰命名法(首字母也是答谢，即UpperCamelCase)。\nalipay.com：根据自己的公司名称进行确定，即Company Name Domain\n目前对于CRD的版本转换不太友好，一般统一使用v1.\n\n2.6 Spec，Status 规范\n用命令在apis包下生成CRD Types后，请不要随意修改apis里的结构体、命名规则、以及注释。\n只能，也只修改${Kind}_types.go文件里的Spec和StatusSpec结构体里的内容。\nSpec和StatusSpec里的字段都必须是Public的，也就是字段名首字母是大写。\n每个字段，都应该写上JSON Tag，JSON Tag必须使用小驼峰命名法，即LowerCamelCase。\n如果字段允许为空，JSON Tag记得带上omitempty。StatusSpec的字段一般都是允许为空的。例子：\n\ntype DemoSpecs struct &#123;        // FiledA 允许为空        FieldA string `json&quot;fieldA,omitempty&quot;`                // FieldB 不允许为空        FieldB string `json&quot;fieldB&quot;`&#125;\n\n3 Operator 工作流程\n3.1 核心组件\nInformer：一个依赖 Kubernetes List&#x2F;Watch API 、可监听事件并触发回调函数的二级缓存工具包。\n在启动时Reflector调用List API获得crd的全部Object实例，并缓存在Local Store中。\n然后Reflector调用Watch API来获取和监听对象实例的变化，维护缓存的变化。\n每当收到add&#x2F;delete等实例请求时，Reflector会将变更的事件+对象推送到deltaFIFO的队列中。\n根据delttaFIFO中的内容，先到Local Store中更新对象的信息与状态，之后经过eventHandler进入WorkQueue，进而触发controller的reconcilet调谐逻辑。\n\n\nWorkQueue：需要处理的事件队列，供controller来进行真正的业务处理。\nControl Loop：实际的控制器角色，通过实现调谐逻辑，确保期望和实际运行状态是一致的。通过Lister向Local Store中读Object实例，通过Client向API Server写具体的操作逻辑。\n\n3.2 实现细节\n消息可靠性：List短链接查询当前全量的资源及状态；Watch长链接接受增量的资源变更事件并做相应处理。两者互相帮助，达到消息的可靠性和数据的一致性。\n共享Informer：对同一类型的资源只建立一个链接，为多个controller提供共享cache的能力。\n若请求为添加操作，Indexer把API对象保存到本地缓存中，并为它创建索引；若为删除操作，则在本地缓存中删除该对象。\nLocalStore 会周期性地把所有资源的信息重新放到 DeltaFIFO 中，确保二级缓存间的同步，让失败的事件得到重新处理。若在入队前发现DeltaFIFO中已经有新版本的Object实例，则不入队。\nReconcilers是核心处理逻辑，但其只获取资源的名称和命名空间，并不知道资源的操作(增删改)是什么，也不知道资源的其他信息。目的就是在收到资源变更时，根据object的期望状态直接调整资源的状态。\n\n3.3 以Pod为对象的流程示例\n\nInformer 在初始化时，Reflector 会先通过 List API 向 API Server 获得所有的 Pod 实例。\nReflector 拿到全部 Pod 实例后，会将全部 Pod 实例放到 Local Store 中\n如果后面 Controller 调用 Lister 的 List&#x2F;Get 方法获取 Pod 实例时， 那么 Lister 会直接从 Local Store 中拿数据。\nInformer 初始化完成之后，Reflector开始通过Watch API监听Pod相关的所有事件；如果此时 pod_1 被删除，那么 Reflector 会监听到这个事件。\nReflector 将 pod_1 被删除的这个事件发送到 DeltaFIFO\nDeltaFIFO 首先会将这个事件（一般为key : value的形式）存储在自己的WorkQueue，然后会直接操作 Local Store 中的数据，删除 Local Store 中的 pod_1。\nWorkQueue 会 Pop 这个事件到 Controller 中。\nController 收到这个事件，通过 Lister 从本地 Local Store 中获取真正的对象实例，执行真正的业务逻辑。\n若有关联资源（ownerReference为该对象的资源），则删除关联资源。\n若无关联资源，则等待读取下一个事件。\n\n\n\n4 原生实现\nCRD实现： 首先利用CRD的定义文件types.go，通过官方的code-generator生成CRD的相关代码，包括标准client，deepcopy，informer和lister。\ncontroller实现： 然后基于官方的sample-controller的实践示例，进行自定义的编排逻辑编写，包括对于不同add &#x2F; update &#x2F;delete 等操作的响应等细节操作。\nYAML文件编写： 手动编写CRD定义和Deployment定义YAML文件，通过kubectl apply完成k8s资源的扩展和控制器的部署。\n其他开发工作： 比如对Controller进行二进制编译、镜像打包上传、以Deployment中的Pod的形式部署到K8s中等。\n\n等后面来看下，这些工作中，脚手架能帮忙实现多少。\n","categories":["kubernetes/operator"],"tags":["kubernetes","operator"]},{"title":"集群接入dashboard","url":"/2025/07/kubernetes/ji-qun-jie-ru-dashboard/","content":"1 Kubernets Dashboard安装下载kubernetes-dashboard的yaml:\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml -O dashboard.yaml\n\n在官方yaml里，我们需要将镜像修改一下，不然会遇见ImagePullBackOff的错误：\n\nkubernetesui&#x2F;dashboard:v2.7.0\nkubernetesui&#x2F;metrics-scraper:v1.0.8\n备注：版本可能不同，认准镜像名\n\n将以上镜像修改为可用的：\n\nm.daocloud.io&#x2F;docker.io&#x2F;kubernetesui&#x2F;dashboard:v2.7.0\nm.daocloud.io&#x2F;docker.io&#x2F;kubernetesui&#x2F;metrics-scraper:v1.0.8\n\n安装dashboard：\nkubectl apply -f ./dashboard.yaml\n\n验证操作界面已经部署并且正在运行:\nsudo kubectl get pod -n kubernetes-dashboardNAME                                         READY   STATUS    RESTARTS   AGEdashboard-metrics-scraper-864c58f57b-fjlfs   1/1     Running   0          98skubernetes-dashboard-58db7bd7d4-pdh76        1/1     Running   0          98s\n\n创建 ServiceAccount 和 ClusterRoleBinding 以提供对新创建的集群的管理权限访问:\nkubectl create serviceaccount -n kubernetes-dashboard admin-userkubectl create clusterrolebinding -n kubernetes-dashboard admin-user --clusterrole cluster-admin --serviceaccount=kubernetes-dashboard:admin-user\n\n需要用 Bearer Token 来登录到操作界面。使用以下命令将 token 保存到变量:\nsudo kubectl -n kubernetes-dashboard create token admin-usereyJhbGciOiJSUzI1NiIsImtpZCI6InF3b1ZJN0ZVSWUyRkF4blgxVG42d2hVMm0wTGtoSTg3VkVoai1yRTdMN3MifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzI3OTc0NzYyLCJpYXQiOjE3Mjc5NzExNjIsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiZTI2YjE1NjctYjk5YS00ZGRlLTlmMWUtYTIwYmUzZDAwZGJiIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiNWMxYjMyMmUtNDFmNS00ODcxLTkxNjQtZTYzOTk2NzkxZDM4In19LCJuYmYiOjE3Mjc5NzExNjIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.d-b12OQrq_9BnmWz5g-_2nvRS-ktEhg813N8zb-kWBh5GScUHhuiAej2v1p1kt54Xom1H6DaeyvlmL3G8ub7aKgZwJjOyJBFDnt0B04Ysz-KSj788jR_Yg2d1FhTbgk8-pBdV9qSweBVT6GRyQ53NIsTIc5ArDsvfOg66nEiW9rp5-3XLitKpoSLtp_Dpib1VpOR_1XAV8wRNVc9psxOp3vtALs1_jI0Izo_4qOX17OZ9FnxgkeeKglRFynlgGiQ0g2KG74oYQn0b_sUROvb52cdDJ2RDhk4yao2vjMyg19f_x1gK-xM8O7kgfYkA8gXEzguRMl0OEbWP_UgH0RQqA\n\n使用 kubectl 命令行工具运行以下命令以访问操作界面：\nkubectl proxy\n\n进入dashboard：\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n\n\n这里是用kubectl proxy起了一个代理，实现在集群外访问集群内的dashboard\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"加速代理","url":"/2025/07/kubernetes/jia-su-dai-li/","content":"1 友情链接\n镜像加速：https://github.com/DaoCloud/public-image-mirror\n二进制文件加速：https://github.com/DaoCloud/public-binary-files-mirror\nHelm 加速：https://github.com/DaoCloud/public-helm-charts-mirror\n\n2 二进制文件加速2.1 使用方法在原始 URL 上面加入 files.m.daocloud.io 的 前缀 就可以使用。比如：\n# Helm 下载原始URLwget https://get.helm.sh/helm-v3.9.1-linux-amd64.tar.gz# 加速后的 URLwget https://files.m.daocloud.io/get.helm.sh/helm-v3.9.1-linux-amd64.tar.gz\n\n2.2 安装 Helmcd /tmpexport HELM_VERSION=&quot;v3.9.3&quot;wget &quot;https://files.m.daocloud.io/get.helm.sh/helm-$&#123;HELM_VERSION&#125;-linux-amd64.tar.gz&quot;tar -zxvf helm-$&#123;HELM_VERSION&#125;-linux-amd64.tar.gzmv linux-amd64/helm /usr/local/bin/helmhelm version\n\n2.3 安装 KINDcd /tmpexport KIND_VERSION=&quot;v0.22.0&quot;curl -Lo ./kind https://files.m.daocloud.io/github.com/kubernetes-sigs/kind/releases/download/$&#123;KIND_VERSION&#125;/kind-linux-amd64chmod +x ./kindmv ./kind /usr/bin/kindkind version\n\n2.4 安装 istiocd /tmpexport ISTIO_VERSION=&quot;1.14.3&quot;wget &quot;https://files.m.daocloud.io/github.com/istio/istio/releases/download/$&#123;ISTIO_VERSION&#125;/istio-$&#123;ISTIO_VERSION&#125;-linux-amd64.tar.gz&quot;tar -zxvf istio-$&#123;ISTIO_VERSION&#125;-linux-amd64.tar.gz# Do follow the istio docs to install istio\n\n2.5 安装 nerdctl （代替 docker 工具）export NERDCTL_VERSION=&quot;1.7.6&quot;mkdir -p nerdctl ;cd nerdctlwget https://files.m.daocloud.io/github.com/containerd/nerdctl/releases/download/v$&#123;NERDCTL_VERSION&#125;/nerdctl-full-$&#123;NERDCTL_VERSION&#125;-linux-amd64.tar.gztar -zvxf nerdctl-full-$&#123;NERDCTL_VERSION&#125;-linux-amd64.tar.gzmkdir -p /opt/cni/bin ;cp -f libexec/cni/* /opt/cni/bin/ ;cp bin/* /usr/local/bin/ ;cp lib/systemd/system/*.service /usr/lib/systemd/system/systemctl enable containerd ;systemctl start containerd --nowsystemctl enable buildkit;systemctl start buildkit --now\n\n3 docker镜像加速3.1 使用方法增加前缀 (推荐方式)。比如：\n              docker.io/library/busybox                 |                 Vm.daocloud.io/docker.io/library/busybox\n\n或者 支持的镜像仓库 的 前缀替换 就可以使用。比如：\n           docker.io/library/busybox             |             Vdocker.m.daocloud.io/library/busybox\n\n3.2 通过 加速 安装 kubeadmkubeadm config images pull --image-repository k8s-gcr.m.daocloud.io\n\n3.3 通过 加速 安装 kindkind create cluster --name kind --image m.daocloud.io/docker.io/kindest/node:v1.22.1\n\n3.4 通过 加速 部署 应用(这里以 Ingress 为例)wget -O image-filter.sh https://github.com/DaoCloud/public-image-mirror/raw/main/hack/image-filter.sh &amp;&amp; chmod +x image-filter.shwget -O deploy.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/baremetal/deploy.yamlcat ./deploy.yaml | ./image-filter.sh | kubectl apply -f -\n\n3.5 Docker 加速添加到 &#x2F;etc&#x2F;docker&#x2F;daemon.json\n&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.m.daocloud.io&quot;  ]&#125;\n\n4 helm 加速helm repo add community https://release.daocloud.io/chartrepo/community \n\n\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"kubectl常用命令","url":"/2025/07/kubernetes/kubectl-chang-yong-ming-ling/","content":"登录命令根据机器ip使用kubectl登录机器(field-selector):\n#!/bin/bashexport targetIp=&quot;6.0.90.240&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;podinfo=`kubectl get pod --all-namespaces --field-selector=status.podIP=&quot;$targetIp&quot; -o wide | grep -v NAME | head -n 1 `ns=`echo $&#123;podinfo&#125; | awk &#x27;&#123;print $1&#125;&#x27;`pod=`echo $&#123;podinfo&#125; | awk &#x27;&#123;print $2&#125;&#x27;`echo &quot;$kubectl exec -it -n $&#123;ns&#125; $&#123;pod&#125; -- su - root&quot;kubectl exec -it -n $&#123;ns&#125; $&#123;pod&#125; -- su - root\n\n根据机器ip使用kubectl登录机器(label):\n#!/bin/bashexport targetIp=&quot;6.3.144.241&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;podinfo=`kubectl get pod --all-namespaces -l sigma.ali/ip=&quot;$targetIp&quot; -o wide | grep -v NAMESPACE`ns=`echo $&#123;podinfo&#125; | awk &#x27;&#123;print $1&#125;&#x27;`pod=`echo $&#123;podinfo&#125; | awk &#x27;&#123;print $2&#125;&#x27;`echo &quot;$kubectl exec -it -n $&#123;ns&#125; $&#123;pod&#125; -- su - root&quot;kubectl exec -it -n $&#123;ns&#125; $&#123;pod&#125; -- su - root\n\n更智能版本的kubectl登录命令：\n\n查看KUBECONFIG_DIR目录下有哪些kubeconfig可以用\n校验目标登录的ip格式\n查询并解析pod信息\n查询该pod有哪些容器并展示\n查询选定的容器有哪些用户(与user_array做交集)，支持自定义输入用户\n根据以上信息登录目标ip对应pod的选定容器#!/bin/bash# 添加特定用户user_array=(&quot;root&quot; &quot;admin&quot; &quot;log&quot;)# 从指定目录获取所有 kubeconfig 文件KUBECONFIG_DIR=&quot;/Users/king/.kube&quot;# 初始化 kubectl 命令前缀KUBECTL_CMD=&quot;kubectl&quot;# 确保将信息打印到终端里，即使在函数之间$(...)调用的场景function print_to_console() &#123;    printf &quot;%s\\n&quot; &quot;$1&quot; &gt;&amp;2&#125;# 检查IP地址是否符合正确的格式function is_valid_ip() &#123;    local ip=$1    local valid_regex=&#x27;^([0-9]&#123;1,3&#125;\\.)&#123;3&#125;[0-9]&#123;1,3&#125;$&#x27;    if [[ $ip =~ $valid_regex ]]; then        # 确保每个数字部分小于等于255        IFS=&#x27;.&#x27; read -r -a octets &lt;&lt;&lt; &quot;$ip&quot;        for octet in &quot;$&#123;octets[@]&#125;&quot;; do            if ((octet &gt; 255)); then                return 1            fi        done        return 0    fi    return 1&#125;KUBECONFIGS=($(find $KUBECONFIG_DIR -maxdepth 1 -name &quot;*.config&quot; -print))# 检查是否找到 kubeconfig 文件if [ $&#123;#KUBECONFIGS[@]&#125; -eq 0 ]; then    print_to_console &quot;没有找到任何 kubeconfig 文件在目录: $KUBECONFIG_DIR&quot;else    # 提供给用户选择的菜单，动态生成选项范围提示    cat &lt;&lt; EOF    ----------------------------------------------    |*******Please Enter Your Choice:[1-$&#123;#KUBECONFIGS[@]&#125;]*******|    ----------------------------------------------EOF    # 输出可供选择的配置文件选项    for i in &quot;$&#123;!KUBECONFIGS[@]&#125;&quot;; do        print_to_console &quot;*     $(($i + 1)) $&#123;KUBECONFIGS[$i]&#125;&quot;    done    # 捕获用户输入并确保在合法范围内    while true; do        read -p &quot;please input your choice [1-$&#123;#KUBECONFIGS[@]&#125;] (or press Enter to skip): &quot; num        if [[ -z &quot;$num&quot; ]]; then            break        elif [[ &quot;$num&quot; =~ ^[0-9]+$ ]] &amp;&amp; [ &quot;$num&quot; -ge 1 ] &amp;&amp; [ &quot;$num&quot; -le $&#123;#KUBECONFIGS[@]&#125; ]; then            selected_config=&quot;$&#123;KUBECONFIGS[$((num - 1))]&#125;&quot;            KUBECTL_CMD=&quot;kubectl --kubeconfig=$selected_config&quot;            print_to_console &quot;Using configuration file: $selected_config&quot;            break        else            print_to_console &quot;Invalid choice. Please try again.&quot;        fi    donefi# 输出用户选择的配置文件selected_config=&quot;$&#123;KUBECONFIGS[$((num - 1))]&#125;&quot;print_to_console &quot;You selected: $selected_config&quot;# 捕获用户输入的targetIPwhile true; do    read -p &quot;please input your target ip: &quot; targetIP    # 检查输入是否为空和格式有效性    if [[ -z &quot;$targetIP&quot; ]]; then        print_to_console &quot;IP 地址不能为空，请重新输入。&quot;    elif ! is_valid_ip &quot;$targetIP&quot;; then        print_to_console &quot;无效的IP格式，请输入有效的IP地址。&quot;    else        print_to_console &quot;您输入的IP地址是: $targetIP&quot;        break    fidone# 获取 Pod 信息podinfo=$($KUBECTL_CMD get pod --all-namespaces --field-selector=status.podIP=&quot;$targetIP&quot; -o wide | grep -v NAME | head -n 1)# 检查 podinfo 是否为空if [[ -z &quot;$podinfo&quot; ]]; then    print_to_console &quot;未能获取到对应 IP 的 Pod 信息，退出脚本。&quot;    exit 1fi# 提取命名空间和 Pod 名称ns=$(echo &quot;$&#123;podinfo&#125;&quot; | awk &#x27;&#123;print $1&#125;&#x27;)pod=$(echo &quot;$&#123;podinfo&#125;&quot; | awk &#x27;&#123;print $2&#125;&#x27;)# 检查 ns 和 pod 是否为空if [[ -z &quot;$ns&quot; || -z &quot;$pod&quot; ]]; then    print_to_console &quot;未能提取到命名空间或 Pod 名称，退出脚本。&quot;    exit 1fiprint_to_console &quot;Namespace: $ns, Pod: $pod&quot;# 获取容器列表containers=($($KUBECTL_CMD get pod $pod -n $ns -o jsonpath=&#x27;&#123;.spec.containers[*].name&#125;&#x27;))selected_container=&quot;&quot;if [ $&#123;#containers[@]&#125; -gt 0 ]; then    print_to_console &quot;请选择一个容器 (或直接按 Enter 跳过使用默认容器):&quot;    for i in &quot;$&#123;!containers[@]&#125;&quot;; do        print_to_console &quot;*      $(($i + 1)) $&#123;containers[$i]&#125;&quot;    done    while true; do        read -p &quot;please input your choice [1-$&#123;#containers[@]&#125;] (or press Enter to skip): &quot; container_num        if [[ -z &quot;$container_num&quot; ]]; then            break        elif [[ &quot;$container_num&quot; =~ ^[0-9]+$ ]] &amp;&amp; [ &quot;$container_num&quot; -ge 1 ] &amp;&amp; [ &quot;$container_num&quot; -le $&#123;#containers[@]&#125; ]; then            selected_container=&quot;$&#123;containers[$((container_num - 1))]&#125;&quot;            print_to_console &quot;Selected container: $selected_container&quot;            break        else            print_to_console &quot;Invalid choice. Please try again.&quot;        fi    donefi# 解析容器中的用户并添加到用户数组中while IFS=: read -r username _ uid _; do    if [[ $uid -ge 1000 &amp;&amp; $username != &quot;nobody&quot; ]]; then        user_array+=(&quot;$username&quot;)    fidone &lt;&lt;&lt; &quot;$user_list&quot;# 显示可供选择的用户列表print_to_console &quot;请选择一个用户 (或自定义输入):&quot;for i in &quot;$&#123;!user_array[@]&#125;&quot;; do    print_to_console &quot;*      $(($i + 1)) $&#123;user_array[$i]&#125;&quot;doneprint_to_console &quot;*      $(( $&#123;#user_array[@]&#125; + 1 )) 自定义输入 &quot;# 捕获用户选择的用户while true; do    read -p &quot;please input your choice [1-$(( $&#123;#user_array[@]&#125; + 1 ))]: &quot; user_num    if [[ &quot;$user_num&quot; =~ ^[0-9]+$ ]] &amp;&amp; [ &quot;$user_num&quot; -ge 1 ] &amp;&amp; [ &quot;$user_num&quot; -le $(( $&#123;#user_array[@]&#125; + 1 )) ]; then        if [ &quot;$user_num&quot; -eq $(( $&#123;#user_array[@]&#125; + 1 )) ]; then            read -p &quot;请输入自定义用户名: &quot; targetUser        else            targetUser=&quot;$&#123;user_array[$((user_num - 1))]&#125;&quot;        fi        print_to_console &quot;Selected user: $targetUser&quot;        break    else        print_to_console &quot;Invalid choice. Please try again.&quot;    fidone# 执行命令if [[ -n &quot;$selected_container&quot; ]]; then    print_to_console &quot;$KUBECTL_CMD exec -it -n $&#123;ns&#125; $&#123;pod&#125; -c $&#123;selected_container&#125; -- su - $targetUser&quot;    $KUBECTL_CMD exec -it -n $&#123;ns&#125; $&#123;pod&#125; -c $&#123;selected_container&#125; -- su - $targetUserelse    print_to_console &quot;$KUBECTL_CMD exec -it -n $&#123;ns&#125; $&#123;pod&#125; -- su - $targetUser&quot;    $KUBECTL_CMD exec -it -n $&#123;ns&#125; $&#123;pod&#125; -- su - $targetUserfi\n\n查询命令根据机器ip(field-selector)查询pod:\n#!/bin/bashexport fieldKEY=&quot;status.podIP&quot;export fieldVALUE=&quot;6.0.90.240&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;kubectl get pod --all-namespaces --field-selector=$fieldKEY=$fieldVALUE -o wide\n\n根据label查询pod:\n#!/bin/bashexport labelKEY=&quot;sigma.ali/ip&quot;export labelVALUE=&quot;6.0.90.240&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;kubectl get pod --all-namespaces -l $labelKEY=$labelVALUE -o wide\n\n导出yaml根据机器ip使用kubectl导出机器yaml:\n#!/bin/bashlocal podName=&quot;&quot;local namespace=&quot;&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;kubectl get pod/$podName -n $&#123;namespace&#125; -oyaml\n\ndescribe根据namespace和podName进行describe\nlocal namespace=&quot;longtermbase&quot;local podName=&quot;inplaceset-antcodebuild-tn1oimjfl-gz00b-0&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;kubectl describe pod $podName -n $namespace\n\n清理terminating的pod通过清理finalizers实现\nlocal namespace=&quot;&quot;local podName=&quot;&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;kubectl patch pod/$podName -n $namespace -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;&#x27;\n\n强制删除\nlocal namespace=&quot;&quot;local podName=&quot;&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;k delete pod/$podName -n $namespace --force --grace-period=0\n\n复制文件到pod容器local namespace=&quot;&quot;local podName=&quot;&quot;local sourceDir=&quot;&quot;local sourceFile=&quot;&quot;local targetDir=&quot;&quot;local targetFile=&quot;&quot;#alias kubectl=&#x27;kubectl&#x27;alias kubectl=&#x27;kubectl --kubeconfig=/Users/king/.kube/sa128.config&#x27;kubectl cp -n linkw $sourceDir/$sourceFile $podName:/targetDir/targetFile","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"Kind搭建测试集群","url":"/2025/07/kubernetes/kind-da-jian-ce-shi-ji-qun/","content":"Kind工具介绍","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"Ldap通过helm部署","url":"/2025/07/devops/ldap/ldap-tong-guo-helm-bu-shu/","content":"简介\n⚡️: OpenLDAP是轻量级目录访问协议（LDAP）的开源实现，它提供了一种存储和访问关于用户、组、计算机和其他资源的信息的中心化目录服务。\n\n属性介绍\ncn（common name）：通用名称，表示一个对象的名称。在用户条目中，通常与用户的姓名相对应；在组条目中，则与组名相对应。\nou（organizational unit）：组织单位，表示一个组织或部门。在LDAP目录服务中，可以使用ou来创建多级组织结构，并将用户和其他对象分配到相应的组织单元中，以便更好地管理它们。\ndc（domain component）：域组件，表示域名的一部分。在LDAP中，域名通常是按照层次结构组织的，例如：example.com可以被拆分为dc&#x3D;example,dc&#x3D;com。这样做有利于有效地组织和管理大规模的目录服务。\nsn（surname）：姓氏，表示一个人的姓氏。与cn属性不同，sn只表示姓氏，而且通常不唯一。\n\nkubernetes集群部署参考快速搭建kind测试集群快速搭建一本地测使用的kind集群。\nHelm部署极简部署helm repo add stable https://charts.helm.sh/stablehelm install openldap stable/openldap\n\n常规部署部署十分简单，但是我们可能需要调整Helm chart配置，所以建议把包拉取到本地调整之后再进行部署，下面是详细流程\n增加helm repo增加helm repo：\nhelm repo add stable https://charts.helm.sh/stable\n\n➜  ~ helm repo add stable https://charts.helm.sh/stable&quot;stable&quot; has been added to your repositories\n\n检索验证repo：\nhelm search repo openldap\n➜  ~ helm search repo openldapNAME                   CHART VERSION        APP VERSION        DESCRIPTION                                   stable/openldap        1.2.7                2.4.48             DEPRECATED - Community developed LDAP software\n拉取chart为了能够进行配置修改操作，将chart拉到本地：\nhelm pull stable/openldap\n\n解压chart：\ntar -zxvf openldap-1.2.7.tgz \n➜  github tar -zxvf openldap-1.2.7.tgz x openldap/Chart.yamlx openldap/values.yamlx openldap/templates/NOTES.txtx openldap/templates/_helpers.tplx openldap/templates/configmap-customldif.yamlx openldap/templates/configmap-env.yamlx openldap/templates/deployment.yamlx openldap/templates/pvc.yamlx openldap/templates/secret.yamlx openldap/templates/service.yamlx openldap/templates/tests/openldap-test-runner.yamlx openldap/templates/tests/openldap-tests.yamlx openldap/.helmignorex openldap/README.md\n\n配置调整拉取的Chart包，整体比较小，来看下大概的结构。\n.├── Chart.yaml├── README.md├── templates│   ├── NOTES.txt│   ├── _helpers.tpl│   ├── configmap-customldif.yaml│   ├── configmap-env.yaml│   ├── deployment.yaml│   ├── pvc.yaml│   ├── secret.yaml│   ├── service.yaml│   └── tests│       ├── openldap-test-runner.yaml│       └── openldap-tests.yaml└── values.yaml\n\n我们如果有参数方面的变更，可以在values.yaml文件内进行填充配置，或者修改配置。\ntemplates目录下就是整个chart的模板，包含了部署需要的configmap、deployment、pvc、secret、svc等资源yaml模板。\n刚刚我们拉取的Chart包 部署openldap，values.yaml文件内需要调整的地方不是很多:\nenv:  LDAP_ORGANISATION: &quot;King Inc.&quot;  LDAP_DOMAIN: &quot;king-ldap.net&quot;  LDAP_BACKEND: &quot;hdb&quot;  LDAP_TLS: &quot;false&quot;  LDAP_TLS_ENFORCE: &quot;false&quot;  LDAP_REMOVE_CONFIG_AFTER_SETUP: &quot;false&quot;\n\n部署在chart的根目录执行安装：\nhelm install --kubeconfig=$HOME/.kube/king_test_config ldap ./\n\nPassword:WARNING: This chart is deprecatedNAME: ldapLAST DEPLOYED: Wed Jan 15 21:56:54 2025NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:OpenLDAP has been installed. You can access the server from within the k8s cluster using:  ldap-openldap.default.svc.cluster.local:389You can access the LDAP adminPassword and configPassword using:  kubectl get secret --namespace default ldap-openldap -o jsonpath=&quot;&#123;.data.LDAP_ADMIN_PASSWORD&#125;&quot; | base64 --decode; echo  kubectl get secret --namespace default ldap-openldap -o jsonpath=&quot;&#123;.data.LDAP_CONFIG_PASSWORD&#125;&quot; | base64 --decode; echoYou can access the LDAP service, from within the cluster (or with kubectl port-forward) with a command like (replace password and domain):  ldapsearch -x -H ldap://ldap-openldap.default.svc.cluster.local:389 -b dc=example,dc=org -D &quot;cn=admin,dc=example,dc=org&quot; -w $LDAP_ADMIN_PASSWORDTest server health using Helm test:  helm test ldapYou can also consider installing the helm chart for phpldapadmin to manage this instance of OpenLDAP, or install Apache Directory Studio, and connect using kubectl port-forward.\n\nHelm 测试helm list ldap\n\n查看idap的pod：\nkubectl --kubeconfig=$HOME/.kube/king_test_config get pod -A|grep ldap\n\nNAMESPACE            NAME                                               READY   STATUS    RESTARTS   AGEdefault              ldap-openldap-6d5cc55fc-vzq5v                      1/1     Running   0          17m\n\n登录idap的pod：\nkubectl --kubeconfig=$HOME/.kube/king_test_config exec -it pod/ldap-openldap-6d5cc55fc-vzq5v -- /bin/bash\n\n可以使用ldapsearch命令进行查询：\n$ ldapsearch -x -b &lt;search_base&gt; -H &lt;ldap_host&gt; -D &lt;bind_dn&gt; -W\n\n-x：表示简单的身份认证。\n-b：指定搜索的 DC。\n-H：指定搜索的主机 URL，如果你是在 LDAP 服务器上，则不需要带这个参数。比如我这里为 ldap:&#x2F;&#x2F;192.168.31.76:389\n-D：绑定的 DN。\n-W：绑定的 DN 的密码。\n\n例如一个实际的查询命令：\nldapsearch -x -b &quot;dc=king-ldap,dc=net&quot; -D &quot;cn=admin,dc=king-ldap,dc=net&quot;  -W\n\n会要求输入密码，可以如此获取：\nenv|grep -i PASSWORD\nLDAP_CONFIG_PASSWORD=M2ulnYD2lZPvM1hKsYP5sCy8meDN8h61LDAP_ADMIN_PASSWORD=a4QL47j3QvQWaTkDolvgLHpbpX8YDTdn\n\n查询结果：\nroot@ldap-openldap-6d5cc55fc-vzq5v:~# ldapsearch -x -b &quot;dc=king-ldap,dc=net&quot; -D &quot;cn=admin,dc=king-ldap,dc=net&quot;  -WEnter LDAP Password: # extended LDIF## LDAPv3# base &lt;dc=king-ldap,dc=net&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## king-ldap.netdn: dc=king-ldap,dc=netobjectClass: topobjectClass: dcObjectobjectClass: organizationo: King Inc.dc: king-ldap# admin, king-ldap.netdn: cn=admin,dc=king-ldap,dc=netobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9eEl0d3JvcFN1cTgxelBLSFozVHYzQzJSdmtLcXNXenY=# search resultsearch: 2result: 0 Success# numResponses: 3# numEntries: 2\n\nHelm 数据持久化？看下pvc的yaml模板：\n&#123;&#123;- if and .Values.persistence.enabled (not .Values.persistence.existingClaim) &#125;&#125;kind: PersistentVolumeClaimapiVersion: v1metadata:  name: &#123;&#123; template &quot;openldap.fullname&quot; . &#125;&#125;  labels:    app: &#123;&#123; template &quot;openldap.name&quot; . &#125;&#125;    chart: &#123;&#123; template &quot;openldap.chart&quot; . &#125;&#125;    release: &#123;&#123; .Release.Name &#125;&#125;    heritage: &#123;&#123; .Release.Service &#125;&#125;&#123;&#123;- if .Values.extraLabels &#125;&#125;&#123;&#123; toYaml .Values.extraLabels | indent 4 &#125;&#125;&#123;&#123;- end &#125;&#125;spec:  accessModes:    - &#123;&#123; .Values.persistence.accessMode | quote &#125;&#125;  resources:    requests:      storage: &#123;&#123; .Values.persistence.size | quote &#125;&#125;&#123;&#123;- if .Values.persistence.storageClass &#125;&#125;&#123;&#123;- if (eq &quot;-&quot; .Values.persistence.storageClass) &#125;&#125;  storageClassName: &quot;&quot;&#123;&#123;- else &#125;&#125;  storageClassName: &quot;&#123;&#123; .Values.persistence.storageClass &#125;&#125;&quot;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;\n\n根据配置，只要在values.yaml文件的这部分进行pvc配置的填入再进行安装即可：\n## Persist data to a persistent volumepersistence:  enabled: false  ## database data Persistent Volume Storage Class  ## If defined, storageClassName: &lt;storageClass&gt;  ## If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning  ## If undefined (the default) or set to null, no storageClassName spec is  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on  ##   GKE, AWS &amp; OpenStack)  ##  # storageClass: &quot;-&quot;  accessMode: ReadWriteOnce  size: 8Gi  # existingClaim: &quot;&quot;\n","categories":["devops/ldap"],"tags":["devops","ldap"]},{"title":"仓库fork后同步","url":"/2025/07/devops/environment/cang-ku-fork-hou-tong-bu/","content":"fork仓库\n此时会将该仓库在复制一份，并存放在你的路径下：\n设置上游代码库进入本地代码库：\n# 从自己仓库进行clone(fork)git clone https://github.com/qiqiuyang/bk-ci.git# 进入目录cd bk-ci/# 查看目录结构$ lsCHANGELOG              CONTRIBUTING.md  README_EN.md  support-filesCODE_OF_CONDUCT.en.md  docker-images    README.md     THIRD-PARTY-NOTICES.txtCODE_OF_CONDUCT.md     docs             scriptsCODEOWNERS             helm-charts      SECURITY.mdCONTRIBUTING.en.md     LICENSE.txt      src\n\n查看远程仓库的路径\n$ git remote -vorigin        https://github.com/qiqiuyang/bk-ci.git (fetch)origin        https://github.com/qiqiuyang/bk-ci.git (push)\n\n这里可以发现从自己仓库clone下来后，fetch和push的路径都是自己的。\n设置上游代码库\n$ git remote add upstream git@github.com:TencentBlueKing/bk-ci.git\n\n再次查看远程仓库地址：\n$ git remote -vorigin        https://github.com/qiqiuyang/bk-ci.git (fetch)origin        https://github.com/qiqiuyang/bk-ci.git (push)upstream        git@github.com:TencentBlueKing/bk-ci.git (fetch)upstream        git@github.com:TencentBlueKing/bk-ci.git (push)\n\n同步源仓库的更新使用下面的命令拉取源仓库的更新：\n$ git fetch upstreamremote: Enumerating objects: 17643, done.remote: Counting objects: 100% (13492/13492), done.remote: Compressing objects: 100% (3960/3960), done.remote: Total 10070 (delta 4278), reused 8600 (delta 3184), pack-reused 0 (from 0)接收对象中: 100% (10070/10070), 2.44 MiB | 2.05 MiB/s, 完成.处理 delta 中: 100% (4278/4278), 完成 657 个本地对象.来自 github.com:TencentBlueKing/bk-ci * [新分支]                master          -&gt; upstream/master * [新分支]                release-1.11    -&gt; upstream/release-1.11 * [新分支]                release-1.14    -&gt; upstream/release-1.14 * [新分支]                release-1.2     -&gt; upstream/release-1.2 * [新分支]                release-1.3     -&gt; upstream/release-1.3。。。。。。 * [新标签]                v1.8.4          -&gt; v1.8.4 * [新标签]                v1.8.5          -&gt; v1.8.5\n\n到这里就比较清晰了，在更新时就是把源仓库的更新的分支放在upstream下，例如：upstream&#x2F;master\n所以同步远端分支时，就是git fetch upstream，然后将自己的分支merge目标分支内容，例如：git merge upstream&#x2F;master\n如此便可以实现自己的fork仓库同步源仓库的新变更了。\n向原仓库发起PR首先在自己的仓库点击Pull Request-&gt;New Pull Request，进入以下截图页面\nbase repository为原仓库的某个分支，head repository为fork仓库发某个分支, head的某个分支代码合到base的某个分支\n进入原仓库的Pull requests可看到刚才发起的PR， 这里就不演示了\n","categories":["devops/environment"],"tags":["devops","environment"]},{"title":"git环境配置","url":"/2025/07/devops/environment/git-huan-jing-pei-zhi/","content":"环境配置设置用户签名配置用户名： git config –global user.name 你的用户名配置邮箱： git config –global user.email 注册的邮箱\n配置好之后，可以用git config –global –list命令查看配置是否OK\n$ git config --global --listuser.name=xxxuser.email=xxx@163.com\n\n设置sshKey生成新的rsa密钥：ssh-keygen -t rsa -C ‘注册GitHub的邮箱’\nssh-keygen -t rsa -C xxx@163.com\n\n一直回车就可以，即不设置密码。\n$ ssh-keygen -t rsa -C xxx@163.comGenerating public/private rsa key pair.Enter file in which to save the key (/home/king/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/king/.ssh/id_rsaYour public key has been saved in /home/king/.ssh/id_rsa.pubThe key fingerprint is:SHA256:n+yyy xxx@163.comThe key&#x27;s randomart image is:+---[RSA 3072]----+|    .*=.+.       ||    = +=o= o     ||     * ++ o .    ||  o . o..  .     || o * .  S o      ||. X o  o = +     || + + .. = *      ||..o .  o E       ||.o==    .        |+----[SHA256]-----+\n\n此时已经生成了ssh ras的密钥：\n$ ls $HOME/.sshauthorized_keys  id_rsa  id_rsa.pub  known_hosts\n\n设置github的SSH key\n将上面生成的ssh公钥复制进去。\n","categories":["devops/environment"],"tags":["devops","environment"]},{"title":"tencentcloud_ckafka_instance","url":"/2025/07/devops/terraform/tencentcloud-ckafka-instance/","content":"前言约定格式：\n├── aliyun                                             # 云厂商实例文件│   └── aliyun_ecs_mod_demo                           # 模型名│       └── aliyun_china_platform_7111                # 云账号名│           └── ecs_instance_name_20241224121212      # 实例名│               ├── backend.tf                        # 实例state文件保存说明：oss│               └── main.tf                           # 实例具体的参数├── modules                                            # 模型数据文件夹│   ├── aliyun                                        # 云厂商模型文件夹│   │   └── aliyun_ecs_mod_demo                       # 模型名│   │       ├── main.tf                               # 模型定义主文件│   │       ├── outputs.tf                            # 模型定义输出文件│   │       └── variables.tf                          # 模型定义参数文件│   └── tenmod                                         # 另一个云厂商模型└── tenent                                              # 另一个云厂商实例文件\n\n项目结构.├── modules│   └── tcloud│       └── tcloud_ckafka_mod_demo│           ├── main.tf│           ├── outputs.tf│           └── variables.tf└── tcloud    └── tcloud_ckafka_demo        └── tcloud_china_game_x6            └── ckafaka_demo_202501221738                ├── backend.tf                └── main.tf\n\nmodules&#x2F;modules&#x2F;tcloud&#x2F;tcloud_ckafka_mod_demo&#x2F;main.tfterraform &#123;  required_providers &#123;    tencentcloud = &#123;      source = &quot;tencentcloudstack/tencentcloud&quot;    &#125;  &#125;&#125;provider &quot;tencentcloud&quot; &#123;  region = var.region&#125;data &quot;tencentcloud_availability_zones_by_product&quot; &quot;zone&quot; &#123;  name    = var.availability_zone  product = &quot;ckafka&quot;&#125;resource &quot;tencentcloud_ckafka_instance&quot; &quot;this&quot; &#123;  instance_name      = var.instance_name  zone_id            = data.tencentcloud_availability_zones_by_product.zone.zones[0].id  vpc_id             = var.vpc_id  subnet_id          = var.vswitch_id  msg_retention_time = var.msg_retention_time  kafka_version      = var.kafka_version  disk_size          = var.disk_size  band_width         = var.band_width  disk_type          = var.disk_type  partition          = var.partition  charge_type        = var.charge_type  config &#123;    auto_create_topic_enable   = var.auto_create_topic_enable    default_num_partitions     = var.num_partitions    default_replication_factor = var.replication_factor  &#125;  dynamic_retention_config &#123;    enable = var.dynamic_retention_config_enable  &#125;&#125;\n\n&#x2F;modules&#x2F;tcloud&#x2F;tcloud_ckafka_mod_demo&#x2F;outputs.tfoutput &quot;ckafka_instance_id&quot; &#123;  value = tencentcloud_ckafka_instance.this.id&#125;\n\n&#x2F;modules&#x2F;tcloud&#x2F;tcloud_ckafka_mod_demo&#x2F;variables.tfvariable &quot;region&quot; &#123;  description = &quot;地域&quot;  type        = string  default     = &quot;ap-shanghai&quot;&#125;variable &quot;instance_name&quot; &#123;  description = &quot;kafka实例名&quot;  type        = string&#125;variable &quot;availability_zone&quot; &#123;  # 如ap-shanghai-2  description = &quot;可用区&quot;  type = string&#125;variable &quot;charge_type&quot; &#123;  # PREPAID(预付费), POSTPAID_BY_HOUR(按量付费)  description = &quot;kafka实例计费方式&quot;  type = string  validation &#123;    condition     = contains([&quot;PREPAID&quot;, &quot;POSTPAID_BY_HOUR&quot;], var.charge_type)    error_message = &quot;The charge_type must be one of PREPAID, POSTPAID_BY_HOUR&quot;  &#125;&#125;variable &quot;kafka_version&quot; &#123;  # 0.10.2/1.1.1/2.4.1/2.8.1  description = &quot;kafka实例版本&quot;  type = string  validation &#123;    condition     = contains([&quot;0.10.2&quot;, &quot;1.1.1&quot;, &quot;2.4.1&quot;, &quot;2.4.2&quot;, &quot;2.8.1&quot;], var.kafka_version)    error_message = &quot;The kafka_version must be one of 0.10.2, 1.1.1, 2.4.1, 2.4.2, 2.8.1&quot;  &#125;&#125;variable &quot;vswitch_id&quot; &#123;  // 阿里云vswitch_id -&gt; 腾讯云subnet_id  description = &quot;绑定子网id&quot;  type = string&#125;variable &quot;vpc_id&quot; &#123;  description = &quot;绑定vpc_id&quot;  type = string&#125;variable &quot;disk_size&quot; &#123;  # 需满足当前实例的计费规格，此处预设200和400，可以根据需要修改  description = &quot;kafka实例磁盘规格&quot;  type = number  validation &#123;    condition     = contains([200, 400], var.disk_size)    error_message = &quot;The disk_size must be one of 200, 400&quot;  &#125;&#125;variable &quot;disk_type&quot; &#123;  # 专业版实例磁盘类型，标准版实例不需要填写，CLOUD_SSD(SSD云硬盘), CLOUD_BASIC(高性能云硬盘)  description = &quot;kafka专业版实例磁盘类型&quot;  type = string  default = &quot;&quot;  validation &#123;    condition     = contains([&quot;&quot;, &quot;CLOUD_SSD&quot;, &quot;CLOUD_BASIC&quot;], var.disk_type)    error_message = &quot;The disk_type must be empty, or one of CLOUD_SSD, CLOUD_BASIC&quot;  &#125;&#125;variable &quot;band_width&quot; &#123;  # 单位为MBps.  description = &quot;kafka实例带宽&quot;  type = number&#125;variable &quot;auto_create_topic_enable&quot; &#123;  description = &quot;是否自动创建topic&quot;  type = bool  default = true  validation &#123;    condition     = contains([true, false], var.auto_create_topic_enable)    error_message = &quot;The auto_create_topic_enable must be one of true, false&quot;  &#125;&#125;variable &quot;num_partitions&quot; &#123;  description = &quot;kafka实例默认分区数&quot;  type = number  default = 3&#125;variable &quot;replication_factor&quot; &#123;  description = &quot;kafka实例默认副本数&quot;  type = number  default = 2&#125;variable &quot;dynamic_retention_config_enable&quot; &#123;  description = &quot;是否启用动态消息保留时间配置&quot;  type = number  default = 0  validation &#123;    condition     = contains([0, 1], var.dynamic_retention_config_enable)    error_message = &quot;The dynamic_retention_config_enable must be one of 0, 1&quot;  &#125;&#125;variable &quot;msg_retention_time&quot; &#123;  # 以分钟为单位  description = &quot;kafka实例日志的最大保留时间&quot;  type = number  default = 10080&#125;variable &quot;partition&quot; &#123;  description = &quot;kafka实例分区大小&quot;  type = number  default = 3&#125;\n\nDemo&#x2F;tcloud&#x2F;tcloud_ckafka_demo&#x2F;tcloud_china_game_x6&#x2F;ckafaka_demo_202501221738&#x2F;backend.tfterraform &#123;  backend &quot;oss&quot; &#123;    endpoint = &quot;oss-cn-hangzhou.aliyuncs.com&quot;    bucket   = &quot;dz-devops&quot; # 替换为你的 OSS Bucket 名称    prefix = &quot;terraform_state/tcloud/tcloud_ckafka_demo/tcloud_china_game_x6/ckafka_demo_202501221738&quot;    key      = &quot;terraform.tfstate&quot; # 存储状态文件的路径和名称    region   = &quot;cn-hangzhou&quot;       # OSS 的地域（根据你的实际情况调整）  &#125;&#125;\n\n&#x2F;tcloud&#x2F;tcloud_ckafka_demo&#x2F;tcloud_china_game_x6&#x2F;ckafaka_demo_202501221738&#x2F;main.tfmodule &quot;ckafka_instance&quot; &#123;  source              = &quot;../../../../modules/tcloud/tcloud_ckafka_mod_demo&quot;  region              = &quot;ap-shanghai&quot;  instance_name       = &quot;ckafka-test&quot;  availability_zone   = &quot;ap-shanghai-2&quot;  charge_type         = &quot;POSTPAID_BY_HOUR&quot;  kafka_version       = &quot;2.4.2&quot;  vpc_id              = &quot;vpc-4tkroxts&quot;  vswitch_id          = &quot;subnet-oy1pqvzv&quot;  disk_size           = 200  # disk_type           = &quot;CLOUD_BASIC&quot;  band_width          = 20  auto_create_topic_enable = true  num_partitions     = 3  replication_factor = 3  dynamic_retention_config_enable = 1  msg_retention_time = 1300  partition = 400&#125;\n\n运行测试获取 AK&#x2F;SK在首次使用 Terraform 之前，需要前往腾讯云的云 API 密钥页面申请安全凭证SecretId和SecretKey2。若已有可使用的安全凭证，则跳过该步骤2。具体步骤如下2：\n\n登录腾讯云访问管理控制台，在左侧导航栏，选择访问密钥&gt;API 密钥管理。\n在API 密钥管理页面，单击新建密钥，即可以创建一对SecretId&#x2F;SecretKey。\n\n设置环境变量将获取到的SecretId和SecretKey设置为环境变量：\nexport TENCENTCLOUD_SECRET_ID=your_secret_idexport TENCENTCLOUD_SECRET_KEY=your_secret_key\n\n运行项目进入项目根目录，这里是ckafaka_demo_202501221738目录：\ncd ./tcloud/tcloud_ckafka_demo/tcloud_china_game_x6/ckafaka_demo_202501221738/\n\n初始化 Terraform 项目:\n# 将xxx替换为实际backend的ak，将yyy替换为实际backend的skterraform init -backend-config=&quot;access_key=xxx&quot; -backend-config=&quot;secret_key=yyy&quot;\n\n该命令会下载所需的插件和依赖，并初始化后端配置。类似的输出（首次使用某一个provier时，会先下载）：\nInitializing the backend...Successfully configured the backend &quot;oss&quot;! Terraform will automaticallyuse this backend unless the backend configuration changes.Initializing modules...- ckafka_instance in ../../../../modules/tcloud/tcloud_ckafka_mod_demoInitializing provider plugins...- Finding latest version of tencentcloudstack/tencentcloud...- Installing tencentcloudstack/tencentcloud v1.81.162...- Installed tencentcloudstack/tencentcloud v1.81.162 (signed by a HashiCorp partner, key ID 84F69E1C1BECF459)Partner and community providers are signed by their developers.If you&#x27;d like to know more about provider signing, you can read about it here:https://www.terraform.io/docs/cli/plugins/signing.htmlTerraform has created a lock file .terraform.lock.hcl to record the providerselections it made above. Include this file in your version control repositoryso that Terraform can guarantee to make the same selections by default whenyou run &quot;terraform init&quot; in the future.Terraform has been successfully initialized!You may now begin working with Terraform. Try running &quot;terraform plan&quot; to seeany changes that are required for your infrastructure. All Terraform commandsshould now work.If you ever set or change modules or backend configuration for Terraform,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary.\n\n预览计划变更：terraform plan\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Reading...module.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Read complete after 0s [id=2066006299]Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:  + createTerraform will perform the following actions:  # module.ckafka_instance.tencentcloud_ckafka_instance.this will be created  + resource &quot;tencentcloud_ckafka_instance&quot; &quot;this&quot; &#123;      + band_width          = 20      + charge_type         = &quot;POSTPAID_BY_HOUR&quot;      + disk_size           = 200      + disk_type           = &quot;CLOUD_BASIC&quot;      + id                  = (known after apply)      + instance_name       = &quot;ckafka-test&quot;      + instance_type       = (known after apply)      + kafka_version       = &quot;kafka_version&quot;      + max_message_byte    = (known after apply)      + msg_retention_time  = 1300      + partition           = 400      + public_network      = (known after apply)      + renew_flag          = (known after apply)      + specifications_type = &quot;profession&quot;      + subnet_id           = &quot;subnet-oy1pqvzv&quot;      + tag_set             = (known after apply)      + upgrade_strategy    = 1      + vip                 = (known after apply)      + vpc_id              = &quot;vpc-4tkroxts&quot;      + vport               = (known after apply)      + zone_id             = 200002      + config &#123;          + auto_create_topic_enable   = true          + default_num_partitions     = 3          + default_replication_factor = 3        &#125;      + dynamic_retention_config &#123;          + bottom_retention        = (known after apply)          + disk_quota_percentage   = (known after apply)          + enable                  = 1          + step_forward_percentage = (known after apply)        &#125;      + tags (known after apply)    &#125;Plan: 1 to add, 0 to change, 0 to destroy.───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────Note: You didn&#x27;t use the -out option to save this plan, so Terraform can&#x27;t guarantee to take exactly these actions if you run &quot;terraform apply&quot; now.\n\n执行变更：\nterraform apply\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Reading...module.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Read complete after 1s [id=2066006299]Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:  + createTerraform will perform the following actions:  # module.ckafka_instance.tencentcloud_ckafka_instance.this will be created  + resource &quot;tencentcloud_ckafka_instance&quot; &quot;this&quot; &#123;      + band_width          = 20      + charge_type         = &quot;POSTPAID_BY_HOUR&quot;      + disk_size           = 200      + disk_type           = &quot;CLOUD_BASIC&quot;      + id                  = (known after apply)      + instance_name       = &quot;ckafka-test&quot;      + instance_type       = (known after apply)      + kafka_version       = &quot;kafka_version&quot;      + max_message_byte    = (known after apply)      + msg_retention_time  = 1300      + partition           = 400      + public_network      = (known after apply)      + renew_flag          = (known after apply)      + specifications_type = &quot;profession&quot;      + subnet_id           = &quot;subnet-oy1pqvzv&quot;      + tag_set             = (known after apply)      + upgrade_strategy    = 1      + vip                 = (known after apply)      + vpc_id              = &quot;vpc-4tkroxts&quot;      + vport               = (known after apply)      + zone_id             = 200002      + config &#123;          + auto_create_topic_enable   = true          + default_num_partitions     = 3          + default_replication_factor = 3        &#125;      + dynamic_retention_config &#123;          + bottom_retention        = (known after apply)          + disk_quota_percentage   = (known after apply)          + enable                  = 1          + step_forward_percentage = (known after apply)        &#125;      + tags (known after apply)    &#125;Plan: 1 to add, 0 to change, 0 to destroy.Do you want to perform these actions?  Terraform will perform the actions described above.  Only &#x27;yes&#x27; will be accepted to approve.  Enter a value: yesmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Creating...module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [10s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [20s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [30s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [40s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [50s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m0s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m10s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m20s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m30s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m40s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [1m50s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m0s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m10s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m20s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m30s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m40s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [2m50s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Still creating... [3m0s elapsed]module.ckafka_instance.tencentcloud_ckafka_instance.this: Creation complete after 3m7s [id=ckafka-9jnda3jn]Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\n验证创建在前端查看是否成功创建实例：\n销毁资源执行下面的命令进行销毁：\nterraform destroy\nmodule.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Reading...module.ckafka_instance.data.tencentcloud_availability_zones_by_product.zone: Read complete after 1s [id=2066006299]module.ckafka_instance.tencentcloud_ckafka_instance.this: Refreshing state... [id=ckafka-9jnda3jn]Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:  - destroyTerraform will perform the following actions:  # module.ckafka_instance.tencentcloud_ckafka_instance.this will be destroyed  - resource &quot;tencentcloud_ckafka_instance&quot; &quot;this&quot; &#123;      - band_width          = 20 -&gt; null      - charge_type         = &quot;POSTPAID_BY_HOUR&quot; -&gt; null      - disk_size           = 200 -&gt; null      - disk_type           = &quot;CLOUD_BASIC&quot; -&gt; null      - id                  = &quot;ckafka-9jnda3jn&quot; -&gt; null      - instance_name       = &quot;ckafka-test&quot; -&gt; null      - instance_type       = 1 -&gt; null      - kafka_version       = &quot;0.10.2.1&quot; -&gt; null      - msg_retention_time  = 1300 -&gt; null      - partition           = 400 -&gt; null      - public_network      = 3 -&gt; null      - renew_flag          = 0 -&gt; null      - specifications_type = &quot;profession&quot; -&gt; null      - subnet_id           = &quot;subnet-oy1pqvzv&quot; -&gt; null      - tag_set             = &#123;&#125; -&gt; null      - upgrade_strategy    = 1 -&gt; null      - vip                 = &quot;172.17.0.3&quot; -&gt; null      - vpc_id              = &quot;vpc-4tkroxts&quot; -&gt; null      - vport               = &quot;9092&quot; -&gt; null      - zone_id             = 200002 -&gt; null      - config &#123;          - auto_create_topic_enable   = true -&gt; null          - default_num_partitions     = 3 -&gt; null          - default_replication_factor = 3 -&gt; null        &#125;      - dynamic_retention_config &#123;          - bottom_retention        = 0 -&gt; null          - disk_quota_percentage   = 0 -&gt; null          - enable                  = 1 -&gt; null          - step_forward_percentage = 0 -&gt; null        &#125;    &#125;Plan: 0 to add, 0 to change, 1 to destroy.Do you really want to destroy all resources?  Terraform will destroy all your managed infrastructure, as shown above.  There is no undo. Only &#x27;yes&#x27; will be accepted to confirm.  Enter a value: yesmodule.ckafka_instance.tencentcloud_ckafka_instance.this: Destroying... [id=ckafka-9jnda3jn]module.ckafka_instance.tencentcloud_ckafka_instance.this: Destruction complete after 5sDestroy complete! Resources: 1 destroyed.\n\n验证销毁\n","categories":["devops/terraform"],"tags":["devops","terraform"]},{"title":"alicloud_alikafka_instance","url":"/2025/07/devops/terraform/alicloud-alikafka-instance/","content":"前言约定格式：\n├── aliyun                                             # 云厂商实例文件│   └── aliyun_ecs_mod_demo                           # 模型名│       └── aliyun_china_platform_7111                # 云账号名│           └── ecs_instance_name_20241224121212      # 实例名│               ├── backend.tf                        # 实例state文件保存说明：oss│               └── main.tf                           # 实例具体的参数├── modules                                            # 模型数据文件夹│   ├── aliyun                                        # 云厂商模型文件夹│   │   └── aliyun_ecs_mod_demo                       # 模型名│   │       ├── main.tf                               # 模型定义主文件│   │       ├── outputs.tf                            # 模型定义输出文件│   │       └── variables.tf                          # 模型定义参数文件│   └── tenmod                                         # 另一个云厂商模型└── tenent                                              # 另一个云厂商实例文件\n\n项目结构.├── aliyun│   └── aliyun_alikafka_demo│       └── aliyun_china_pt_7111│           └── alikafka_demo_202502061910│               ├── backend.tf│               └── main.tf└── modules    └── aliyun        └── aliyun_alikafka_mod_demo            ├── main.tf            └── variables.tf\n\nmodules&#x2F;modules&#x2F;aliyun&#x2F;aliyun_alikafka_mod_demo&#x2F;main.tfterraform &#123;  required_providers &#123;    alicloud = &#123;      source  = &quot;aliyun/alicloud&quot;      version = &quot;1.225.0&quot;    &#125;  &#125;&#125;provider &quot;alicloud&quot; &#123;  region = var.region&#125;resource &quot;alicloud_alikafka_instance&quot; &quot;this&quot; &#123;  name          = var.instance_name  deploy_type   = var.deploy_type  disk_size     = var.disk_size  disk_type     = var.disk_type  vswitch_id    = var.vswitch_id  partition_num = var.partition_num  io_max        = var.io_max&#125;\n\n&#x2F;modules&#x2F;aliyun&#x2F;aliyun_alikafka_mod_demo&#x2F;variables.tfvariable &quot;region&quot; &#123;  description = &quot;地域&quot;  type        = string&#125;variable &quot;instance_name&quot; &#123;  description = &quot;kafka实例名&quot;  type        = string&#125;variable &quot;deploy_type&quot; &#123;  // - 4: eip/vpc instance；- 5: vpc instance.  description = &quot;部署类型&quot;  type        = number  default = 5  validation &#123;    condition     = contains([4, 5], var.deploy_type)    error_message = &quot;The deploy_type must be one of 4, 5&quot;  &#125;&#125;variable &quot;disk_size&quot; &#123;  description = &quot;kafka实例磁盘规格&quot;  type = number  validation &#123;    condition     = contains([500, 1000], var.disk_size)    error_message = &quot;The disk_size must be one of 200, 400&quot;  &#125;&#125;variable &quot;disk_type&quot; &#123;  # 0: efficient cloud disk , 1: SSD.  description = &quot;磁盘类型&quot;  type = number  default = 1  validation &#123;    condition     = contains([0, 1], var.disk_type)    error_message = &quot;The disk_type must be empty, or one of 0, 1&quot;  &#125;&#125;variable &quot;vswitch_id&quot; &#123;  description = &quot;绑定虚拟交换机id&quot;  type = string&#125;variable &quot;partition_num&quot; &#123;  description = &quot;分区数量&quot;  type = number&#125;variable &quot;io_max&quot; &#123;  description = &quot;io的最大值&quot;  type = number&#125;\n\nDemo&#x2F;aliyun&#x2F;aliyun_alikafka_demo&#x2F;aliyun_china_pt_7111&#x2F;alikafka_demo_202502061910&#x2F;backend.tfterraform &#123;  backend &quot;oss&quot; &#123;    endpoint = &quot;oss-cn-hangzhou.aliyuncs.com&quot;    bucket   = &quot;dz-devops&quot; # 替换为你的 OSS Bucket 名称    prefix = &quot;terraform_state//aliyun/aliyun_alikafka_demo/aliyun_china_pt_7111/alikafka_demo_202502061910&quot;    key      = &quot;terraform.tfstate&quot; # 存储状态文件的路径和名称    region   = &quot;cn-hangzhou&quot;       # OSS 的地域（根据你的实际情况调整）  &#125;&#125;\n\n&#x2F;aliyun&#x2F;aliyun_alikafka_demo&#x2F;aliyun_china_pt_7111&#x2F;alikafka_demo_202502061910&#x2F;main.tfmodule &quot;alikafka_instance&quot; &#123;  source        = &quot;../../../../modules/aliyun/aliyun_alikafka_mod_demo&quot;  region        = &quot;cn-hangzhou&quot;  disk_size     = 500  instance_name = &quot;alikafka-testV2&quot;  deploy_type   = 5  vswitch_id    = &quot;vsw-bp1co65f3q2s0bis9yfkg&quot;  partition_num = 50  io_max        = 20&#125;\n\n运行测试获取 AK&#x2F;SK在首次使用 Terraform 之前，需要前往腾讯云的云 API 密钥页面申请安全凭证SecretId和SecretKey2。若已有可使用的安全凭证，则跳过该步骤2。具体步骤如下2：\n\n登录腾讯云访问管理控制台，在左侧导航栏，选择访问密钥&gt;API 密钥管理。\n在API 密钥管理页面，单击新建密钥，即可以创建一对SecretId&#x2F;SecretKey。\n\n设置环境变量将获取到的SecretId和SecretKey设置为环境变量：\nexport TENCENTCLOUD_SECRET_ID=your_secret_idexport TENCENTCLOUD_SECRET_KEY=your_secret_key\n\n运行项目进入项目根目录，alikafka_demo_202502061910目录：\ncd ./aliyun/aliyun_alikafka_demo/aliyun_china_pt_7111/alikafka_demo_202502061910\n\n初始化 Terraform 项目:\n# 将xxx替换为实际backend的ak，将yyy替换为实际backend的skterraform init -backend-config=&quot;access_key=xxx&quot; -backend-config=&quot;secret_key=yyy&quot;\n\n该命令会下载所需的插件和依赖，并初始化后端配置。类似的输出（首次使用某一个provier时，会先下载）：\nInitializing the backend...Successfully configured the backend &quot;oss&quot;! Terraform will automaticallyuse this backend unless the backend configuration changes.Initializing modules...Initializing provider plugins...- Reusing previous version of aliyun/alicloud from the dependency lock file- Using previously-installed aliyun/alicloud v1.225.0Terraform has been successfully initialized!You may now begin working with Terraform. Try running &quot;terraform plan&quot; to seeany changes that are required for your infrastructure. All Terraform commandsshould now work.If you ever set or change modules or backend configuration for Terraform,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary.\n\n预览计划变更：\nterraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:  + createTerraform will perform the following actions:  # module.alikafka_instance.alicloud_alikafka_instance.this will be created  + resource &quot;alicloud_alikafka_instance&quot; &quot;this&quot; &#123;      + config            = (known after apply)      + deploy_type       = 5      + disk_size         = 500      + disk_type         = 1      + eip_max           = (known after apply)      + end_point         = (known after apply)      + group_left        = (known after apply)      + group_used        = (known after apply)      + id                = (known after apply)      + io_max            = 20      + io_max_spec       = (known after apply)      + is_partition_buy  = (known after apply)      + name              = &quot;alikafka-testV2&quot;      + paid_type         = &quot;PostPaid&quot;      + partition_left    = (known after apply)      + partition_num     = 50      + partition_used    = (known after apply)      + resource_group_id = (known after apply)      + security_group    = (known after apply)      + service_version   = (known after apply)      + spec_type         = &quot;normal&quot;      + status            = (known after apply)      + topic_left        = (known after apply)      + topic_num_of_buy  = (known after apply)      + topic_quota       = (known after apply)      + topic_used        = (known after apply)      + vpc_id            = (known after apply)      + vswitch_id        = &quot;vsw-bp1co65f3q2s0bis9yfkg&quot;      + zone_id           = (known after apply)    &#125;Plan: 1 to add, 0 to change, 0 to destroy.──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────Note: You didn&#x27;t use the -out option to save this plan, so Terraform can&#x27;t guarantee to take exactly these actions if you run &quot;terraform apply&quot; now.\n\n执行变更：\nterraform apply\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:  + createTerraform will perform the following actions:  # module.alikafka_instance.alicloud_alikafka_instance.this will be created  + resource &quot;alicloud_alikafka_instance&quot; &quot;this&quot; &#123;      + config            = (known after apply)      + deploy_type       = 5      + disk_size         = 500      + disk_type         = 1      + eip_max           = (known after apply)      + end_point         = (known after apply)      + group_left        = (known after apply)      + group_used        = (known after apply)      + id                = (known after apply)      + io_max            = 20      + io_max_spec       = (known after apply)      + is_partition_buy  = (known after apply)      + name              = &quot;alikafka-testV2&quot;      + paid_type         = &quot;PostPaid&quot;      + partition_left    = (known after apply)      + partition_num     = 50      + partition_used    = (known after apply)      + resource_group_id = (known after apply)      + security_group    = (known after apply)      + service_version   = (known after apply)      + spec_type         = &quot;normal&quot;      + status            = (known after apply)      + topic_left        = (known after apply)      + topic_num_of_buy  = (known after apply)      + topic_quota       = (known after apply)      + topic_used        = (known after apply)      + vpc_id            = (known after apply)      + vswitch_id        = &quot;vsw-bp1co65f3q2s0bis9yfkg&quot;      + zone_id           = (known after apply)    &#125;Plan: 1 to add, 0 to change, 0 to destroy.Do you want to perform these actions?  Terraform will perform the actions described above.  Only &#x27;yes&#x27; will be accepted to approve.  Enter a value: yesmodule.alikafka_instance.alicloud_alikafka_instance.this: Creating...module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [10s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [20s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [30s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [40s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [50s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m0s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m10s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m20s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m30s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m40s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [1m50s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m0s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m10s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m20s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m30s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m40s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [2m50s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m0s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m10s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m20s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m30s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m40s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [3m50s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m0s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m10s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m20s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m30s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still creating... [4m40s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Creation complete after 4m45s [id=alikafka_post-cn-0gx44g6dm006]Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\n验证创建在前端查看是否成功创建实例：\n销毁资源执行下面的命令进行销毁：\nterraform destroy\n\nmodule.alikafka_instance.alicloud_alikafka_instance.this: Refreshing state... [id=alikafka_post-cn-0gx44g6dm006]Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:  - destroyTerraform will perform the following actions:  # module.alikafka_instance.alicloud_alikafka_instance.this will be destroyed  - resource &quot;alicloud_alikafka_instance&quot; &quot;this&quot; &#123;      - config            = jsonencode(            &#123;              - &quot;cloud.maxTieredStoreSpace&quot;           = &quot;0&quot;              - &quot;enable.acl&quot;                          = &quot;false&quot;              - &quot;enable.compact&quot;                      = &quot;true&quot;              - &quot;enable.tiered&quot;                       = &quot;false&quot;              - &quot;enable.vpc_sasl_ssl&quot;                 = &quot;false&quot;              - &quot;kafka.log.retention.hours&quot;           = &quot;72&quot;              - &quot;kafka.message.max.bytes&quot;             = &quot;1048576&quot;              - &quot;kafka.offsets.retention.minutes&quot;     = &quot;10080&quot;              - &quot;kafka.ssl.bit&quot;                       = &quot;1024&quot;              - &quot;message.timestamp.difference.max.ms&quot; = &quot;9223372036854775807&quot;              - &quot;message.timestamp.type&quot;              = &quot;CreateTime&quot;            &#125;        ) -&gt; null      - deploy_type       = 5 -&gt; null      - disk_size         = 500 -&gt; null      - disk_type         = 1 -&gt; null      - eip_max           = 0 -&gt; null      - end_point         = &quot;172.31.1.64:9092,172.31.1.66:9092,172.31.1.65:9092&quot; -&gt; null      - group_left        = 2100 -&gt; null      - group_used        = 0 -&gt; null      - id                = &quot;alikafka_post-cn-0gx44g6dm006&quot; -&gt; null      - io_max            = 20 -&gt; null      - io_max_spec       = &quot;alikafka.hw.2xlarge&quot; -&gt; null      - is_partition_buy  = 1 -&gt; null      - name              = &quot;alikafka-testV2&quot; -&gt; null      - paid_type         = &quot;PostPaid&quot; -&gt; null      - partition_left    = 1050 -&gt; null      - partition_num     = 50 -&gt; null      - partition_used    = 0 -&gt; null      - resource_group_id = &quot;rg-acfmzpn54i5ejry&quot; -&gt; null      - security_group    = &quot;sg-bp184o2lwjnssf12wf3w&quot; -&gt; null      - service_version   = &quot;2.2.0&quot; -&gt; null      - spec_type         = &quot;normal&quot; -&gt; null      - status            = 5 -&gt; null      - tags              = &#123;&#125; -&gt; null      - topic_left        = 1050 -&gt; null      - topic_num_of_buy  = 1050 -&gt; null      - topic_quota       = 1050 -&gt; null      - topic_used        = 0 -&gt; null      - vpc_id            = &quot;vpc-bp1sro6pb0sec14x7s05l&quot; -&gt; null      - vswitch_id        = &quot;vsw-bp1co65f3q2s0bis9yfkg&quot; -&gt; null      - zone_id           = &quot;zonei&quot; -&gt; null        # (1 unchanged attribute hidden)    &#125;Plan: 0 to add, 0 to change, 1 to destroy.Do you really want to destroy all resources?  Terraform will destroy all your managed infrastructure, as shown above.  There is no undo. Only &#x27;yes&#x27; will be accepted to confirm.  Enter a value: yesmodule.alikafka_instance.alicloud_alikafka_instance.this: Destroying... [id=alikafka_post-cn-0gx44g6dm006]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 10s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 20s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 30s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 40s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 50s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m1s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m11s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m21s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m31s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m41s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 1m51s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m1s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m11s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m21s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Still destroying... [id=alikafka_post-cn-0gx44g6dm006, 2m31s elapsed]module.alikafka_instance.alicloud_alikafka_instance.this: Destruction complete after 2m37sDestroy complete! Resources: 1 destroyed.\n\n\n","categories":["devops/terraform"],"tags":["devops","terraform"]},{"title":"Npd代码结构了解","url":"/2025/07/devops/npd/npd-dai-ma-jie-gou-liao-jie/","content":"1 pkg目录结构这里是各个monitor的实现。\n.├── custompluginmonitor             -&gt; 用于实现自定义监视插件。node-problem-detector 支持通过插件监视特定状态或事件，此目录中包含与用户自定义插件相关的代码。├── exporters                       -&gt; 用于包含导出器（exporters）的实现，导出器负责将监视到的数据发送到外部系统，比如监控系统（例如 Prometheus）。导出器在数据收集和监控集成中起到关键作用。├── healthchecker                   -&gt; 实现健康检查逻辑，确保 node-problem-detector 本身和其他依赖的组件处于健康状态。这是监控和维护软件稳定性的重要部分。├── logcounter                      -&gt; 实现了 logcounter 的功能，通常用于计数和分析系统日志，以检测异常行为或问题。它可以与系统日志监控结合使用。├── problemdaemon                   -&gt; 实现问题守护程序。这个守护程序负责监测和识别节点中的问题，通过问题发现机制收集指标并生成告警。├── problemdetector                 -&gt; 包含问题检测的核心逻辑。这是 node-problem-detector 的主要功能部分，负责从不同的来源收集信息，并根据这些信息识别和报告节点问题。├── problemmetrics                  -&gt; 包含问题指标的实现，用于收集、处理和导出有关节点状态和问题的数据。这些指标可以展示在监控界面上，帮助用户理解系统的健康状况。├── systemlogmonitor                -&gt; 实现了系统日志监控的功能。它处理系统日志，分析日志内容，以检测可能导致问题的事件和错误。├── systemstatsmonitor              -&gt; 用于监控系统的各种统计数据，例如 CPU 使用率、内存使用情况等。它负责收集和报告这些统计信息，以帮助验证节点的健康状态。├── types                           -&gt; 用于定义项目中使用的各种类型结构体和常量。这些类型通常用于数据传输和处理，为核心逻辑提供数据模型。├── util                            -&gt; 包含一些公用工具函数和库，这些函数可以在其他模块中复用，提高代码的可维护性和可读性。└── version                         -&gt; 保存与版本相关的信息和结构。它用于管理版本号并提供版本信息的功能\n\n2 problemdaemon2.1 Registerpkg&#x2F;problemdaemon&#x2F;problem_daemon.go:32\nvar (        handlers = make(map[types.ProblemDaemonType]types.ProblemDaemonHandler))// Register registers a problem daemon factory method, which will be used to create the problem daemon.func Register(problemDaemonType types.ProblemDaemonType, handler types.ProblemDaemonHandler) &#123;        handlers[problemDaemonType] = handler&#125;\n\n注册一个问题守护进程工厂方法。参数：\n\nproblemDaemonType: 需要注册的守护进程类型。\nhandler: 对应的处理器，其中包含创建守护进程的方法。作用：将指定类型的处理器添加到 handlers 映射中，以便后续根据类型获取对应的处理器。\n\n2.2 GetProblemDaemonNamespkg&#x2F;problemdaemon&#x2F;problem_daemon.go:37\n// GetProblemDaemonNames retrieves all available problem daemon types.func GetProblemDaemonNames() []types.ProblemDaemonType &#123;        problemDaemonTypes := []types.ProblemDaemonType&#123;&#125;        for problemDaemonType := range handlers &#123;                problemDaemonTypes = append(problemDaemonTypes, problemDaemonType)        &#125;        return problemDaemonTypes&#125;\n\n检索所有可用的问题守护进程类型。\n返回值：返回一个包含所有注册的守护进程类型的切片。\n实现细节：遍历 handlers 映射，将每种问题守护进程类型添加到返回的切片中。\n2.3 GetProblemDaemonHandlerOrDiepkg&#x2F;problemdaemon&#x2F;problem_daemon.go:46\n// GetProblemDaemonHandlerOrDie retrieves the ProblemDaemonHandler for a specific type of problem daemon, panic if error occurs..func GetProblemDaemonHandlerOrDie(problemDaemonType types.ProblemDaemonType) types.ProblemDaemonHandler &#123;        handler, ok := handlers[problemDaemonType]        if !ok &#123;                panic(fmt.Sprintf(&quot;Problem daemon handler for %v does not exist&quot;, problemDaemonType))        &#125;        return handler&#125;\n\n获取特定类型问题守护进程的处理器。\n参数：问题守护进程类型。\n返回值：返回对应的处理器。\n实现细节：如果指定类型没有找到对应的处理器，则触发恐慌（panic），这通常用于在初始化时确保配置的有效性。\n2.4 NewProblemDaemonspkg&#x2F;problemdaemon&#x2F;problem_daemon.go:55\n// NewProblemDaemons creates all problem daemons based on the configurations provided.func NewProblemDaemons(monitorConfigPaths types.ProblemDaemonConfigPathMap) []types.Monitor &#123;        problemDaemonMap := make(map[string]types.Monitor)        for problemDaemonType, configs := range monitorConfigPaths &#123;                for _, config := range *configs &#123;                        if _, ok := problemDaemonMap[config]; ok &#123;                                // Skip the config if it&#x27;s duplicated.                                klog.Warningf(&quot;Duplicated problem daemon configuration %q&quot;, config)                                continue                        &#125;                        problemDaemonMap[config] = handlers[problemDaemonType].CreateProblemDaemonOrDie(config)                &#125;        &#125;        problemDaemons := []types.Monitor&#123;&#125;        for _, problemDaemon := range problemDaemonMap &#123;                problemDaemons = append(problemDaemons, problemDaemon)        &#125;        return problemDaemons&#125;\n\n根据提供的配置创建所有问题守护进程。\n参数：\nmonitorConfigPaths: 包含每种问题守护进程类型及其对应配置路径的映射。返回值：返回创建的所有 Monitor 实例的切片。实现细节\n\n创建空映射：使用 problemDaemonMap 存储已创建的守护进程，以避免重复。\n遍历配置：\n对于配置中的每种问题守护进程类型及其相应的配置路径，检查是否已经创建。\n如果配置已存在，则记录警告并跳过。\n使用注册的处理器创建新的问题守护进程并将其添加到映射中。\n\n\n返回值构建：将地图中的所有守护进程转换为切片并返回。\n\n3 problem_detector3.1 problemDetector定义pkg&#x2F;problemdetector&#x2F;problem_detector.go:33\n// ProblemDetector collects statuses from all problem daemons and update the node condition and send node event.type ProblemDetector interface &#123;        Run(context.Context) error&#125;type problemDetector struct &#123;        monitors  []types.Monitor        exporters []types.Exporter&#125;\n\n字段：\nmonitors: 存储监控器的切片，用于收集问题状态。\nexporters: 存储导出器的切片，用于将收集到的状态发送到外部系统或用户。\n方法说明： Run(context.Context) error 启动问题检测器，并在上下文被取消或出现错误时返回错误。\n3.2 NewProblemDetectorpkg&#x2F;problemdetector&#x2F;problem_detector.go:40\n// NewProblemDetector creates the problem detector. Currently we just directly passed in the problem daemons, but// in the future we may want to let the problem daemons register themselves.func NewProblemDetector(monitors []types.Monitor, exporters []types.Exporter) ProblemDetector &#123;        return &amp;problemDetector&#123;                monitors:  monitors,                exporters: exporters,        &#125;&#125;\n创建一个新的问题检测器实例。\n参数：\nmonitors: 监控器数组，用于检测不同问题。\nexporters: 导出器数组，用于将状态发送给外部系统。\n返回值：返回实现 ProblemDetector 接口的新的 problemDetector 实例。\n3.3 Runpkg&#x2F;problemdetector&#x2F;problem_detector.go:48\n// Run starts the problem detector.func (p *problemDetector) Run(ctx context.Context) error &#123;        // Start the log monitors one by one.        var chans []&lt;-chan *types.Status        failureCount := 0        for _, m := range p.monitors &#123;                ch, err := m.Start()                if err != nil &#123;                        // Do not return error and keep on trying the following config files.                        klog.Errorf(&quot;Failed to start problem daemon %v: %v&quot;, m, err)                        failureCount++                        continue                &#125;                if ch != nil &#123;                        chans = append(chans, ch)                &#125;        &#125;        allMonitors := p.monitors        if len(allMonitors) == failureCount &#123;                return fmt.Errorf(&quot;no problem daemon is successfully setup&quot;)        &#125;        defer func() &#123;                for _, m := range allMonitors &#123;                        m.Stop()                &#125;        &#125;()        ch := groupChannel(chans)        klog.Info(&quot;Problem detector started&quot;)        for &#123;                select &#123;                case &lt;-ctx.Done():                        return nil                case status := &lt;-ch:                        for _, exporter := range p.exporters &#123;                                exporter.ExportProblems(status)                        &#125;                &#125;        &#125;&#125;\n\n执行问题检测器的主要运行逻辑。\n实现细节：\n\n开始监控器：遍历并启动每个监控器。\n如果启动失败，记录错误并增加失败计数。\n如果成功，则将返回的通道添加到 chans 切片。\n检查失败情况：如果所有监控器都失败，则返回错误。\n延迟关闭：在函数结束时停止所有监控器。\n通道组合：调用 groupChannel 函数，用于将所有监控器的状态通道组合成单一通道。\n监听状态：使用 select 不断监听上下文和监控器的状态。\n如果收到状态，从所有导出器导出问题。\n\n3.4 groupChannelpkg&#x2F;problemdetector&#x2F;problem_detector.go:91\nfunc groupChannel(chans []&lt;-chan *types.Status) &lt;-chan *types.Status &#123;        statuses := make(chan *types.Status)        for _, ch := range chans &#123;                go func(c &lt;-chan *types.Status) &#123;                        for status := range c &#123;                                statuses &lt;- status                        &#125;                &#125;(ch)        &#125;        return statuses&#125;\n\n将多个状态通道组合为一个通道。\n参数：切片 chans，包含多个状态通道。\n返回值：返回一个新的通道 statuses，用于接收来自所有监控器的状态。\n实现细节\n启动多个 goroutine，每个 goroutine 从一个监控器的通道读取状态，并将其转发到新的 statuses 通道中。\n4 problemmetrics4.1 ProblemMetricsManager定义pkg&#x2F;problemmetrics&#x2F;problem_metrics.go:40\n// ProblemMetricsManager manages problem-converted metrics.// ProblemMetricsManager is thread-safe.type ProblemMetricsManager struct &#123;        problemCounter           metrics.Int64MetricInterface        problemGauge             metrics.Int64MetricInterface        problemTypeToReason      map[string]string        problemTypeToReasonMutex sync.Mutex&#125;\n\n字段：\n\nproblemCounter: 一个整型指标接口，用于计数特定问题发生的次数。\nproblemGauge: 一个整型指标接口，用于表示特定问题是否对节点产生影响。\nproblemTypeToReason: 一个映射，保存问题类型与原因的关系。\nproblemTypeToReasonMutex: 一个互斥锁，用于保护 problemTypeToReason 的并发访问。\n\n4.2 initpkg&#x2F;problemmetrics&#x2F;problem_metrics.go:31\n// GlobalProblemMetricsManager is a singleton of ProblemMetricsManager,// which should be used to manage all problem-converted metrics across all// problem daemons.var GlobalProblemMetricsManager *ProblemMetricsManagerfunc init() &#123;        GlobalProblemMetricsManager = NewProblemMetricsManagerOrDie()&#125;\n\nGlobalProblemMetricsManager: 全局唯一的 ProblemMetricsManager 实例，用于管理所有问题相关的指标。\ninit 函数: 在程序启动时初始化 GlobalProblemMetricsManager，确保其可用。\n4.3 NewProblemMetricsManagerOrDiepkg&#x2F;problemmetrics&#x2F;problem_metrics.go:47\nfunc NewProblemMetricsManagerOrDie() *ProblemMetricsManager &#123;        pmm := ProblemMetricsManager&#123;&#125;        var err error        pmm.problemCounter, err = metrics.NewInt64Metric(                metrics.ProblemCounterID,                string(metrics.ProblemCounterID),                &quot;Number of times a specific type of problem have occurred.&quot;,                &quot;1&quot;,                metrics.Sum,                []string&#123;&quot;reason&quot;&#125;)        if err != nil &#123;                klog.Fatalf(&quot;Failed to create problem_counter metric: %v&quot;, err)        &#125;        pmm.problemGauge, err = metrics.NewInt64Metric(                metrics.ProblemGaugeID,                string(metrics.ProblemGaugeID),                &quot;Whether a specific type of problem is affecting the node or not.&quot;,                &quot;1&quot;,                metrics.LastValue,                []string&#123;&quot;type&quot;, &quot;reason&quot;&#125;)        if err != nil &#123;                klog.Fatalf(&quot;Failed to create problem_gauge metric: %v&quot;, err)        &#125;        pmm.problemTypeToReason = make(map[string]string)        return &amp;pmm&#125;\n\n创建并初始化 ProblemMetricsManager 实例。\n实现细节：\n创建 problemCounter 指标，用于记录特定问题的发生次数，接收标签 reason。\n创建 problemGauge 指标，用于指示特定问题是否正在影响节点，接收标签 type 和 reason。\n如果创建指标失败，则程序会崩溃（Fatalf）。\n初始化问题类型到原因的映射。\n4.4 IncrementProblemCounterpkg&#x2F;problemmetrics&#x2F;problem_metrics.go:79\n// IncrementProblemCounter increments the value of a problem counter.func (pmm *ProblemMetricsManager) IncrementProblemCounter(reason string, count int64) error &#123;        if pmm.problemCounter == nil &#123;                return errors.New(&quot;problem counter is being incremented before initialized.&quot;)        &#125;        return pmm.problemCounter.Record(map[string]string&#123;&quot;reason&quot;: reason&#125;, count)&#125;\n\n增加问题计数器的值，根据 reason 标签记录统计数据。\n参数：\nreason: 问题发生的原因。\ncount: 要增加的数量。\n返回值：返回可能的错误。\n4.5 SetProblemGaugepkg&#x2F;problemmetrics&#x2F;problem_metrics.go:88\n// SetProblemGauge sets the value of a problem gauge.func (pmm *ProblemMetricsManager) SetProblemGauge(problemType string, reason string, value bool) error &#123;        if pmm.problemGauge == nil &#123;                return errors.New(&quot;problem gauge is being set before initialized.&quot;)        &#125;        pmm.problemTypeToReasonMutex.Lock()        defer pmm.problemTypeToReasonMutex.Unlock()        // We clear the last reason, because the expected behavior is that at any point of time,        // for each type of permanent problem, there should be at most one reason got set to 1.        // This behavior is consistent with the behavior of node condition in Kubernetes.        // However, problemGauges with different &quot;type&quot; and &quot;reason&quot; are considered as different        // metrics in Prometheus. So we need to clear the previous metrics explicitly.        if lastReason, ok := pmm.problemTypeToReason[problemType]; ok &#123;                err := pmm.problemGauge.Record(map[string]string&#123;&quot;type&quot;: problemType, &quot;reason&quot;: lastReason&#125;, 0)                if err != nil &#123;                        return fmt.Errorf(&quot;failed to clear previous reason %q for type %q: %v&quot;,                                problemType, lastReason, err)                &#125;        &#125;        pmm.problemTypeToReason[problemType] = reason        var valueInt int64        if value &#123;                valueInt = 1        &#125;        return pmm.problemGauge.Record(map[string]string&#123;&quot;type&quot;: problemType, &quot;reason&quot;: reason&#125;, valueInt)&#125;\n\n设置问题量规的值，指示特定问题对节点的影响。\n参数：\n\nproblemType: 问题类型（例如，节点不可用）。\nreason: 具体原因。\nvalue: 表示问题的状态，true表示问题存在 (1)，false表示问题消失 (0)。\n\n实现细节：\n\n使用互斥锁保护对 problemTypeToReason 的并发访问。\n清除之前的原因（如果存在），保证每个问题类型在任何时刻都最多只有一个原因被设置为 1。\n记录新的量规值。\n\n5 exporters目录结构：\n.├── k8sexporter│   ├── condition│   ├── k8s_exporter.go│   └── problemclient├── prometheusexporter│   └── prometheus_exporter.go├── register.go├── register_test.go└── stackdriver    ├── config    ├── gce    ├── stackdriver_exporter.go    └── stackdriver_exporter_test.go\n这里就涉及到了三个exporter，分别是k8sexporter、prometheusexporter、stackdriver_exporter\n这三个是比较独立的，因为能力是完全不一样的，\n\nexporter：专注于将types.Status的event和conditation上报，并且会有一些http服务接口开放(healthz、conditions、&#x2F;debug&#x2F;pprof)，\nprometheusexporter：直接用的prometheuse的库，简单理解就是为了让prometheuse可以采集到数据\nstackdriver_exporter：处理与 Google Stackdriver 的集成，Stackdriver 是 Google Cloud 提供的监控和管理平台，该exporter负责将指标数据发送到Stackdriver，以便在 Google Cloud 环境中监控应用程序和集群状态。\n\n这里来说说我的理解吧，prometheusexporter和stackdriver_exporter这两个exporter是为了将metric数据发送到对应的监控平台，而k8sexporter则是为了将types.Status发送到k8s，目前已经开始向register继承方向延展，如stackdriver_exporter已经使用该模式并且已经统一了struct的实现方法，但是prometheusexporter和k8sexporter还没完成演变，目前还是通过package直接调用的方式使用的。\n这个模块就不进行代码级别解析了，整体是非常简单的。\n6 healthchecker目录结构：\n├── health_checker.go├── health_checker_darwin.go├── health_checker_linux.go├── health_checker_test.go├── health_checker_windows.go└── types├── types.go├── types_test.go├── types_unix.go└── types_windows.go\n\n6.1 healthChecker定义pkg&#x2F;healthchecker&#x2F;health_checker.go:31\ntype healthChecker struct &#123;  component       string  service         string  enableRepair    bool  healthCheckFunc func () (bool, error)  // The repair is &quot;best-effort&quot; and ignores the error from the underlying actions.  // The bash commands to kill the process will fail if the service is down and hence ignore.  repairFunc         func ()  uptimeFunc         func () (time.Duration, error)  crictlPath         string  healthCheckTimeout time.Duration  coolDownTime       time.Duration  loopBackTime       time.Duration  logPatternsToCheck map[string]int&#125;\n\n字段含义：\n\ncomponent：被检查的 Kubernetes 组件名称（如 Kubelet、Kube Proxy 等）。\nservice：服务名称或标识符。\nenableRepair：布尔值，指示是否在发现服务不健康时尝试修复它。\nhealthCheckFunc：一个函数，用于执行健康检查，返回健康状态和可能的错误。\nrepairFunc：一个函数，用于尝试修复不健康的服务。\nuptimeFunc：一个函数，用于获取服务的运行时间。\ncrictlPath：指向 CRI 工具的路径。\nhealthCheckTimeout：健康检查的超时设置。\ncoolDownTime：在尝试修复之前要求服务必须正常运行的时间。\nloopBackTime：用于回溯检查日志的时间段。\nlogPatternsToCheck：一个映射，包含要检查的日志模式及其出现次数阈值。\n\n6.2 NewHealthCheckerpkg&#x2F;healthchecker&#x2F;health_checker.go:48\n// NewHealthChecker returns a new health checker configured with the given options.func NewHealthChecker(hco *options.HealthCheckerOptions) (types.HealthChecker, error) &#123;  hc := &amp;healthChecker&#123;    component:          hco.Component,    enableRepair:       hco.EnableRepair,    crictlPath:         hco.CriCtlPath,    healthCheckTimeout: hco.HealthCheckTimeout,    coolDownTime:       hco.CoolDownTime,    service:            hco.Service,    loopBackTime:       hco.LoopBackTime,    logPatternsToCheck: hco.LogPatterns.GetLogPatternCountMap(),  &#125;  hc.healthCheckFunc = getHealthCheckFunc(hco)  hc.repairFunc = getRepairFunc(hco)  hc.uptimeFunc = getUptimeFunc(hco.Service)  return hc, nil&#125;\nNewHealthChecker会根据输入的options.HealthCheckerOptions来进行types.HealthChecker的构造。\n6.3 CheckHealthpkg&#x2F;healthchecker&#x2F;health_checker.go:67\n// CheckHealth checks for the health of the component and tries to repair if enabled.// Returns true if healthy, false otherwise.func (hc *healthChecker) CheckHealth() (bool, error) &#123;        healthy, err := hc.healthCheckFunc()        if err != nil &#123;                return healthy, err        &#125;        logPatternHealthy, err := logPatternHealthCheck(hc.service, hc.loopBackTime, hc.logPatternsToCheck)        if err != nil &#123;                return logPatternHealthy, err        &#125;        if healthy &amp;&amp; logPatternHealthy &#123;                return true, nil        &#125;        // The service is unhealthy.        // Attempt repair based on flag.        if hc.enableRepair &#123;                // repair if the service has been up for the cool down period.                uptime, err := hc.uptimeFunc()                if err != nil &#123;                        klog.Infof(&quot;error in getting uptime for %v: %v\\n&quot;, hc.component, err)                        return false, nil                &#125;                klog.Infof(&quot;%v is unhealthy, component uptime: %v\\n&quot;, hc.component, uptime)                if uptime &gt; hc.coolDownTime &#123;                        klog.Infof(&quot;%v cooldown period of %v exceeded, repairing&quot;, hc.component, hc.coolDownTime)                        hc.repairFunc()                &#125;        &#125;        return false, nil&#125;\n\n检查组件的健康状态，返回是否健康，并在不健康的情况下尝试修复。\n逻辑：\n\n调用 healthCheckFunc 执行健康检查。\n使用 logPatternHealthCheck 检查日志模式。\n如果组件不健康且启用了修复，检查组件的运行时间并调用修复函数。\n\n6.4 logPatternHealthCheckpkg&#x2F;healthchecker&#x2F;health_checker.go:100\n// logPatternHealthCheck checks for the provided logPattern occurrences in the service logs.// Returns true if the pattern is empty or does not exist logThresholdCount times since start of service, false otherwise.func logPatternHealthCheck(service string, loopBackTime time.Duration, logPatternsToCheck map[string]int) (bool, error) &#123;        if len(logPatternsToCheck) == 0 &#123;                return true, nil        &#125;        uptimeFunc := getUptimeFunc(service)        klog.Infof(&quot;Getting uptime for service: %v\\n&quot;, service)        uptime, err := uptimeFunc()        if err != nil &#123;                klog.Warningf(&quot;Failed to get the uptime: %+v&quot;, err)                return true, err        &#125;        logStartTime := time.Now().Add(-uptime).Format(types.LogParsingTimeLayout)        if loopBackTime &gt; 0 &amp;&amp; uptime &gt; loopBackTime &#123;                logStartTime = time.Now().Add(-loopBackTime).Format(types.LogParsingTimeLayout)        &#125;        for pattern, count := range logPatternsToCheck &#123;                healthy, err := checkForPattern(service, logStartTime, pattern, count)                if err != nil || !healthy &#123;                        return healthy, err                &#125;        &#125;        return true, nil&#125;\n检查服务日志中指定模式的出现次数。\n参数：\n\nservice：被监控的服务名称。\nloopBackTime：用于确定要检查的日志时间范围。\nlogPatternsToCheck：要检查的日志模式及其出现的阈值。返回值：布尔值表示健康状态和可能的错误。\n\n6.5 healthCheckEndpointOKFuncpkg&#x2F;healthchecker&#x2F;health_checker.go:126\n// healthCheckEndpointOKFunc returns a function to check the status of an http endpointfunc healthCheckEndpointOKFunc(endpoint string, timeout time.Duration) func() (bool, error) &#123;        return func() (bool, error) &#123;                httpClient := http.Client&#123;Timeout: timeout&#125;                response, err := httpClient.Get(endpoint)                if err != nil || response.StatusCode != http.StatusOK &#123;                        return false, nil                &#125;                return true, nil        &#125;&#125;\n\n返回一个函数，该函数检查给定 HTTP 端点的健康状态。\n参数：端点 URI 和超时时间。\n返回值：健康检查函数。\n6.6 getHealthCheckFuncpkg&#x2F;healthchecker&#x2F;health_checker.go:138\n// getHealthCheckFunc returns the health check function based on the component.func getHealthCheckFunc(hco *options.HealthCheckerOptions) func() (bool, error) &#123;        switch hco.Component &#123;        case types.KubeletComponent:                return healthCheckEndpointOKFunc(types.KubeletHealthCheckEndpoint(), hco.HealthCheckTimeout)        case types.KubeProxyComponent:                return healthCheckEndpointOKFunc(types.KubeProxyHealthCheckEndpoint(), hco.HealthCheckTimeout)        case types.DockerComponent:                return func() (bool, error) &#123;                        if _, err := execCommand(hco.HealthCheckTimeout, getDockerPath(), &quot;ps&quot;); err != nil &#123;                                return false, nil                        &#125;                        return true, nil                &#125;        case types.CRIComponent:                return func() (bool, error) &#123;                        _, err := execCommand(                                hco.HealthCheckTimeout,                                hco.CriCtlPath,                                &quot;--timeout=&quot;+hco.CriTimeout.String(),                                &quot;--runtime-endpoint=&quot;+hco.CriSocketPath,                                &quot;pods&quot;,                                &quot;--latest&quot;,                        )                        if err != nil &#123;                                return false, nil                        &#125;                        return true, nil                &#125;        default:                klog.Warningf(&quot;Unsupported component: %v&quot;, hco.Component)        &#125;        return nil&#125;\n\n根据组件类型返回相应的健康检查方法。\n参数：健康检查选项。\n返回值：执行健康检查的函数。\n6.7 execCommandpkg&#x2F;healthchecker&#x2F;health_checker.go:174\n// execCommand executes the bash command and returns the (output, error) from command, error if timeout occurs.func execCommand(timeout time.Duration, command string, args ...string) (string, error) &#123;        ctx, cancel := context.WithTimeout(context.Background(), timeout)        defer cancel()        cmd := exec.CommandContext(ctx, command, args...)        out, err := cmd.CombinedOutput()        if err != nil &#123;                klog.Infof(&quot;command %v failed: %v, %s\\n&quot;, cmd, err, string(out))                return &quot;&quot;, err        &#125;        return strings.TrimSuffix(string(out), &quot;\\n&quot;), nil&#125;\n执行给定的命令，并在指定的超时时间内返回输出和错误。\n参数：超时时间、命令和参数。\n返回值：命令的输出和错误信息。\n6.8 getUptimeFuncpkg&#x2F;healthchecker&#x2F;health_checker_linux.go:32\n// getUptimeFunc returns the time for which the given service has been running.func getUptimeFunc(service string) func() (time.Duration, error) &#123;        return func() (time.Duration, error) &#123;                // Using InactiveExitTimestamp to capture the exact time when systemd tried starting the service. The service will                // transition from inactive -&gt; activating and the timestamp is captured.                // Source : https://www.freedesktop.org/wiki/Software/systemd/dbus/                // Using ActiveEnterTimestamp resulted in race condition where the service was repeatedly killed by plugin when                // RestartSec of systemd and invoke interval of plugin got in sync. The service was repeatedly killed in                // activating state and hence ActiveEnterTimestamp was never updated.                out, err := execCommand(types.CmdTimeout, &quot;systemctl&quot;, &quot;show&quot;, service, &quot;--property=InactiveExitTimestamp&quot;)                if err != nil &#123;                        return time.Duration(0), err                &#125;                val := strings.Split(out, &quot;=&quot;)                if len(val) &lt; 2 &#123;                        return time.Duration(0), errors.New(&quot;could not parse the service uptime time correctly&quot;)                &#125;                t, err := time.Parse(types.UptimeTimeLayout, val[1])                if err != nil &#123;                        return time.Duration(0), err                &#125;                return time.Since(t), nil        &#125;&#125;\n\n返回一个函数，计算指定服务的运行时间。\n参数：\n\nservice string - 被监控服务的名称。\n\n返回值：返回一个函数，该函数返回服务运行的持续时间或错误。\n\n实现细节\n\n使用 systemctl show 命令获取 InactiveExitTimestamp，这是服务从 inactive 状态转换时的时间戳。\n\n解析命令的输出，将时间戳转换为 time.Time 类型，并计算自该时间以来的持续时间。\n\n此方法可用于获取服务的当前运行时间，以便评估是否在修复之前达到冷却时间。\n\n\n6.9 getRepairFuncpkg&#x2F;healthchecker&#x2F;health_checker_linux.go:58\n// getRepairFunc returns the repair function based on the component.func getRepairFunc(hco *options.HealthCheckerOptions) func() &#123;        // Use `systemctl kill` instead of `systemctl restart` for the repair function.        // We start to rely on the kernel message difference for the two commands to        // indicate if the component restart is due to an administrative plan (restart)        // or a system issue that needs repair (kill).        // See https://github.com/kubernetes/node-problem-detector/issues/847.        switch hco.Component &#123;        case types.DockerComponent:                // Use &quot;docker ps&quot; for docker health check. Not using crictl for docker to remove                // dependency on the kubelet.                return func() &#123;                        execCommand(types.CmdTimeout, &quot;pkill&quot;, &quot;-SIGUSR1&quot;, &quot;dockerd&quot;)                        execCommand(types.CmdTimeout, &quot;systemctl&quot;, &quot;kill&quot;, &quot;--kill-who=main&quot;, hco.Service)                &#125;        default:                // Just kill the service for all other components                return func() &#123;                        execCommand(types.CmdTimeout, &quot;systemctl&quot;, &quot;kill&quot;, &quot;--kill-who=main&quot;, hco.Service)                &#125;        &#125;&#125;\n\n根据服务的组件类型返回一个演示修复功能的方法。\n参数：\n\nhco *options.HealthCheckerOptions - 健康检查的选项。\n返回值：返回一个函数，该函数执行相应修复操作。\n\n实现细节\n\n对于 Docker 组件，通过 pkill 发送 SIGUSR1 信号以更优雅地停止 Docker 守护进程。\n对于其他组件，调用 systemctl kill 命令终止主要进程。\n这种设计使得修复功能能够根据不同的组件采取合适的处理方式。\n\n6.10 checkForPatternpkg&#x2F;healthchecker&#x2F;health_checker_linux.go:82\n// checkForPattern returns (true, nil) if logPattern occurs less than logCountThreshold number of times since last// service restart. (false, nil) otherwise.func checkForPattern(service, logStartTime, logPattern string, logCountThreshold int) (bool, error) &#123;        out, err := execCommand(types.CmdTimeout, &quot;/bin/sh&quot;, &quot;-c&quot;,                // Query service logs since the logStartTime                `journalctl --unit &quot;`+service+`&quot; --since &quot;`+logStartTime+                        // Grep the pattern                        `&quot; | grep -i &quot;`+logPattern+                        // Get the count of occurrences                        `&quot; | wc -l`)        if err != nil &#123;                return true, err        &#125;        occurrences, err := strconv.Atoi(out)        if err != nil &#123;                return true, err        &#125;        if occurrences &gt;= logCountThreshold &#123;                klog.Infof(&quot;%s failed log pattern check, %s occurrences: %v&quot;, service, logPattern, occurrences)                return false, nil        &#125;        return true, nil&#125;\n检查指定服务的日志中是否包含特定模式，并返回其出现次数是否达到阈值。\n参数：\n\nservice：被监控服务的名称。\nlogStartTime：起始时间，用于过滤日志。\nlogPattern：需要检查的日志模式。\nlogCountThreshold：模式出现的最大次数阈值。\n返回值：返回一个布尔值，指示是否健康，以及可能的错误。\n\n实现细节\n\n使用 journalctl 命令获取从指定时间开始的服务日志，并通过 grep 过滤符合条件的日志模式。\n使用 wc -l 计算匹配的行数。\n如果日志模式出现的次数大于阈值，记录相关信息并返回健康检查失败的结果。\n\n7 logcounter7.1 logCounter定义pkg&#x2F;logcounter&#x2F;log_counter.go:42\ntype logCounter struct &#123;        logCh         &lt;-chan *systemtypes.Log        buffer        systemlogmonitor.LogBuffer        pattern       string        revertPattern string        clock         clock.Clock&#125;\n字段解释：\n\nlogCh：只读通道，用于接收系统日志。\nbuffer：存储日志的缓冲区，以便分析和计数。\npattern：处理日志时使用的匹配模式。\nrevertPattern：可用来反向计数的模式。\nclock：用于获取当前时间的时钟，支持模拟时钟功能。\n\n7.2 NewJournaldLogCounterpkg&#x2F;logcounter&#x2F;log_counter.go:50\nfunc NewJournaldLogCounter(options *options.LogCounterOptions) (types.LogCounter, error) &#123;        watcher := journald.NewJournaldWatcher(watchertypes.WatcherConfig&#123;                Plugin:       &quot;journald&quot;,                PluginConfig: map[string]string&#123;journaldSourceKey: options.JournaldSource&#125;,                LogPath:      options.LogPath,                Lookback:     options.Lookback,                Delay:        options.Delay,        &#125;)        logCh, err := watcher.Watch()        if err != nil &#123;                return nil, fmt.Errorf(&quot;error watching journald: %v&quot;, err)        &#125;        return &amp;logCounter&#123;                logCh:         logCh,                buffer:        systemlogmonitor.NewLogBuffer(bufferSize),                pattern:       options.Pattern,                revertPattern: options.RevertPattern,                clock:         clock.RealClock&#123;&#125;,        &#125;, nil&#125;\n\n初始化并返回一个新的 logCounter 实例。\n参数：\n\noptions *options.LogCounterOptions - 包含配置信息（例如日志源、路径、模式等）。\n返回值：返回一个实现了 types.LogCounter 接口的 logCounter 实例。\n\n实现细节\n\n创建一个 journald 日志观察者实例来监控指定的日志路径。\n如果监视操作成功，建立日志通道并创建 logCounter 的实例，设置缓冲区、匹配模式等。\n\n7.3 Countpkg&#x2F;logcounter&#x2F;log_counter.go:71\nfunc (e *logCounter) Count() (count int, err error) &#123;        start := e.clock.Now()        for &#123;                select &#123;                case log, ok := &lt;-e.logCh:                        if !ok &#123;                                err = fmt.Errorf(&quot;log channel closed unexpectedly&quot;)                                return                        &#125;                        if start.Before(log.Timestamp) &#123;                                return                        &#125;                        e.buffer.Push(log)                        if len(e.buffer.Match(e.pattern)) != 0 &#123;                                count++                        &#125;                        if e.revertPattern != &quot;&quot; &amp;&amp; len(e.buffer.Match(e.revertPattern)) != 0 &#123;                                count--                        &#125;                case &lt;-e.clock.After(timeout):                        return                &#125;        &#125;&#125;\n统计在运行期间匹配的日志条目数量。\n返回值：返回符合条件的日志计数及任何潜在的错误。\n实现细节\n\n使用当前时间 (start) 记录开始时刻。\n在无限循环中，使用 select 语句监听日志通道和超时事件。\n如果从 logCh 读取到日志：\n检查通道是否已经关闭。\n如果读取到的日志时间戳晚于开始时间，则停止计数。\n将日志条目压入缓冲区。\n检查当前日志是否与模式匹配，如果匹配则计数加一；如果匹配反向模式，则计数减一。\n如果在超时时间内没有收到新日志，结束循环并返回计数。\n\n\n\n","categories":["devops/npd"],"tags":["devops","npd"]},{"title":"NPD介绍","url":"/2025/07/devops/npd/npd-jie-shao/","content":"1 简介官方文档库地址：https://github.com/kubernetes/node-problem-detector\nnode-problem-detector 旨在使集群管理堆栈中的上游层能够看到各种节点问题。\n它是一个在每个节点上运行的守护进程，检测节点问题并将其报告给apiserver。\nnode-problem-detector 可以作为 DaemonSet 运行，也可以独立运行。\n现在它作为 GKE集群中默认启用的Kubernetes Addon运行。它也作为AKS Linux Extension的一部分在 AKS 中默认启用。\n1.2 背景大量节点问题可能会影响节点上运行的 pod，例如：\n\n基础设施守护进程问题：ntp 服务关闭；\n硬件问题：CPU、内存或磁盘损坏；\n内核问题：内核死锁、文件系统损坏；\n容器运行时问题：运行时守护进程无响应；\n…\n\n目前，这些问题对于集群管理堆栈中的上游层是不可见的，因此 Kubernetes 将继续将 pod 调度到坏节点。为了解决这个问题，我们引入了这个新的守护进程node-problem-detector来从各种守护进程收集节点问题，并将它们提供给上游层。一旦上游层能够看到这些问题，我们就可以讨论 remedy systems了。\n1.3 Problem APInode-problem-detector 使用Event并向NodeConditionapiserver 报告问题。\n\nNodeCondition：导致节点无法用于 pod 的永久性问题应报告为NodeCondition。\nEvent：对 pod 影响有限但具有参考意义的临时问题应报告为Event。\n\n1.4 Problem Daemon问题守护进程是 node-problem-detector 的一个子守护进程。它监视特定类型的节点问题并将其报告给 node-problem-detector。守护进程可能是：\n\n专为 Kubernetes 专用用例设计的微型守护进程。\n与节点问题检测器集成的现有节点健康监控守护进程。\n\n目前，问题守护进程在 node-problem-detector 二进制文件中以 goroutine 的形式运行。\n未来，我们会将 node-problem-detector和问题守护进程分离到不同的容器中，并使用 pod 规范将它们组合在一起。\n每种类型的问题守护进程都可以通过设置相应的构建标签在编译时禁用。\n如果在编译时禁用它们，则它们的所有构建依赖项、全局变量和后台goroutine 都将从编译的可执行文件中剔除。\n支持的守护进程列表：\n\n\n\n问题守护进程类型\n节点状态\n解释\n配置\n禁用构建标签\n\n\n\nSystemLogMonitor\nKernelDeadlock ReadonlyFilesystem FrequentKubeletRestart FrequentDockerRestart FrequentContainerdRestart\n系统日志监视器监视系统日志并根据预定义的规则报告问题和指标。\nfilelog, kmsg, kernel, abrt, systemd\ndisable_system_log_monitor\n\n\nSystemStatsMonitor\nNone(Could be added in the future)\n节点问题检测器的系统统计监视器，用于收集各种与健康相关的系统统计信息作为指标。请参阅此处的提案。\nsystem-stats-monitor\ndisable_system_stats_monitor\n\n\nCustomPluginMonitor\nOn-demand(According to users configuration), existing example: NTPProblem\n一个自定义的 node-problem-detector 插件监控器，用于调用和检查各种节点问题，并使用用户定义的检查脚本。请参阅此处的提案。\nexample\ndisable_custom_plugin_monitor\n\n\nHealthChecker\nKubeletUnhealthy ContainerRuntimeUnhealthy\n节点问题检测器的健康检查器用于检查 kubelet 和容器运行时的健康状况。\n\n\n\n\n1.5 Exporterexporter 是 node-problem-detector 的一个组件。\n它向某些后端报告节点问题 和&#x2F;或 指标。\n其中一些可以在编译时使用构建标记禁用。支持的导出器列表：\n\n\n\nExporter\n解释\n禁用构建标签\n\n\n\nKubernetes exporter\nKubernetes 导出器向 Kubernetes API 服务器报告节点问题：临时问题报告为事件，永久问题报告为节点状况。\n\n\n\nPrometheus exporter\nPrometheus 导出器将节点问题和指标本地报告为 Prometheus 指标\n\n\n\nStackdriver exporter\nStackdriver 导出器向 Stackdriver Monitoring API 报告节点问题和指标。\ndisable_stackdriver_exporter\n\n\n1.6 用法1.6.1 标志\n–version：打印节点问题检测器的当前版本。\n–hostname-override：node-problem-detector 用于更新状态和发出事件的自定义节点名称。node-problem-detector 首先从hostname-override获取节点名称，然后从NODE_NAME环境变量 获取，最后返回到os.Hostname。\n\n1.6.2 对于系统日志监控\n–config.system-log-monitor：系统日志监视器配置文件路径列表，以逗号分隔，例如 config&#x2F;kernel-monitor.json。节点问题检测器将为每个配置启动单独的日志监视器。您可以使用不同的日志监视器来监视不同的系统日志。\n\n1.6.3 对于系统统计监控\n–config.system-stats-monitor：系统统计监控配置文件的路径列表，以逗号分隔，例如 config&#x2F;system-stats-monitor.json。节点问题检测器将为每个配置启动一个单独的系统统计监控器。您可以使用不同的系统统计监控器来监控与问题相关的不同系统统计信息。\n\n1.6.4 对于自定义插件监视器\n–config.custom-plugin-monitor：自定义插件监控配置文件路径列表，以逗号分隔，例如 config&#x2F;custom-plugin-monitor.json。节点问题检测器将为每个配置启动一个单独的自定义插件监控。您可以使用不同的自定义插件监控来监控不同的节点问题。\n\n1.6.5 对于健康检查者\n–enable-k8s-exporter：启用向 Kubernetes API 服务器报告，默认为true。\n–apiserver-override：用于自定义 node-problem-detector 如何连接 apiserver 的 URI 参数。如果–enable-k8s-exporter是，则忽略此参数。格式与Heapster 的标志false相同。例如，要无需身份验证即可运行，请使用以下配置： source\n\nhttp://APISERVER_IP:APISERVER_PORT?inClusterConfig=false\n\n请参阅heapster 文档以获取可用选项的完整列表。\n\n–address：绑定节点问题检测服务器的地址。\n–port：绑定节点问题检测服务器的端口。使用 0 表示禁用。\n\n1.6.6 对于 Prometheus exporter\n–prometheus-address：绑定Prometheus抓取端点的地址，默认为127.0.0.1。\n–prometheus-port：绑定 Prometheus 抓取端点的端口，默认为 20257。使用 0 表示禁用。\n\n1.6.7 对于 Stackdriver exporter\n–exporter.stackdriver：Stackdriver 导出器配置文件的路径，例如config&#x2F;exporter&#x2F;stackdriver-exporter.json，默认为空字符串。设置为空字符串即可禁用。\n\n1.6.8 已弃用的标志\n–system-log-monitors：系统日志监控配置文件的路径列表，以逗号分隔。此选项已弃用，由–config.system-log-monitor替代，并将被删除。如果同时设置–system-log-monitors和 –config.system-log-monitor ， NPD 将崩溃。\n–custom-plugin-monitors：自定义插件监控配置文件的路径列表，以逗号分隔。此选项已弃用，由–config.custom-plugin-monitor替代，并将被删除。如果同时设置–custom-plugin-monitors和 –config.custom-plugin-monitor，NPD 将崩溃。\n\n1.7 构建镜像\n安装libsystemdARM GCC 工具链的开发依赖项\nDebian &#x2F; Ubuntu：apt install libsystemd-dev gcc-aarch64-linux-gnu\n\n\ngit clone &#103;&#105;&#116;&#64;&#103;&#105;&#x74;&#x68;&#x75;&#x62;&#x2e;&#x63;&#x6f;&#x6d;:kubernetes&#x2F;node-problem-detector.git\n在顶层目录中运行make。它将：\n构建二进制文件。\n构建docker镜像。二进制文件和config&#x2F;被复制到docker镜像中。\n\n\n\n如果您不需要某些类别的问题守护进程，您可以选择在编译时禁用它们。这是保持 node-problem-detector运行时紧凑且没有不必要代码（例如全局变量、goroutines 等）的最佳方法。\n您可以通过BUILD_TAGS在运行之前设置环境变量来实现这一点make。例如：\nBUILD_TAGS=&quot;disable_custom_plugin_monitor disable_system_stats_monitor&quot; make\n\n上述命令将在不使用自定义插件监视器和系统统计监视器的情况下编译node-problem-detector 。查看问题守护进程部分，了解如何在编译时禁用每个问题守护进程。\n1.8 推送镜像make push将 docker 镜像上传到注册表。默认情况下，镜像将上传到 staging-k8s.gcr.io。可以轻松修改Makefile以将镜像推送到另一个注册表。\n1.9 安装将 node-problem-detector安装到集群中的最简单方法是使用Helm 图表：\nhelm repo add deliveryhero https://charts.deliveryhero.io/helm install --generate-name deliveryhero/node-problem-detector\n\n或者，手动安装 node-problem-detector：\n编辑node-problem-detector.yaml以适应您的环境。将log卷设置为您的系统日志目录（由 SystemLogMonitor 使用）。您可以使用 ConfigMap 覆盖config pod 内的目录。\n编辑node-problem-detector-config.yaml来配置 node-problem-detector。\n\n编辑rbac.yaml以适合您的环境。\n使用 创建 ServiceAccount 和 ClusterRoleBinding kubectl create -f rbac.yaml。\n使用 创建 ConfigMap kubectl create -f node-problem-detector-config.yaml。\n使用 创建 DaemonSet kubectl create -f node-problem-detector.yaml。\n\n1.10 开始独立运行要独立运行 node-problem-detector，您应该设置inClusterConfig为false，并且教 node-problem-detector 如何 访问 apiserver，也就是apiserver-override。\n要使用不安全的 apiserver 连接独立运行 node-problem-detector：\nnode-problem-detector --apiserver-override=http://APISERVER_IP:APISERVER_INSECURE_PORT?inClusterConfig=false\n\n更多场景请见此处\n1.11 试用您可以在正在运行的集群中尝试使用 node-problem-detector，方法是将消息注入到 node-problem-detector 正在监视的日志中。\n例如，假设 node-problem-detector 正在使用KernelMonitor。\n在您的机器上运行kubectl get events -w。在节点上运行sudo sh -c &quot;echo &#39;kernel: BUG: unable to handle kernel NULL pointer dereference at TESTING&#39; &gt;&gt; /dev/kmsg&quot;。然后您应该会看到该KernelOopss事件。\n添加新规则或开发节点问题检测器时，在独立模式下在本地工作站上进行测试可能更容易。\n对于 API 服务器，一种简单的方法是使用kubectl proxy正在运行的集群的 API 服务器在本地可用。\n您会收到一些错误，因为 API 服务器无法识别您的本地工作站。但无论如何，您仍然应该能够测试您的新规则。\n例如，测试KernelMonitor规则：\n\nmake（本地构建node-problem-detector）\nkubectl proxy --port=8080（使正在运行的集群的 API 服务器在本地可用）\n将KernelMonitor更新到您的本地内核日志目录logPath。例如，在某些 Linux 系统上，它是/run/log/journal, 而不是/var/log/journal。\n./bin/node-problem-detector --logtostderr --apiserver-override=http://127.0.0.1:8080?inClusterConfig=false --config.system-log-monitor=config/kernel-monitor.json --config.system-stats-monitor=config/system-stats-monitor.json --port=20256 --prometheus-port=2025（或指向任何 API 服务器地址：端口和 Prometheus 端口）\nsudo sh -c &quot;echo &#39;kernel: BUG: unable to handle kernel NULL pointer dereference at TESTING&#39; &gt;&gt; /dev/kmsg&quot;\n您可以KernelOops在节点问题检测器日志中看到事件。\nsudo sh -c &quot;echo &#39;kernel: INFO: task docker:20744 blocked for more than 120 seconds.&#39; &gt;&gt; /dev/kmsg&quot;\n您可以在node-problem-detector log看DockerHung event和状态\n您可以在http://127.0.0.1:20256/conditions查看DockerHung状态\n您可以在http://127.0.0.1:20257/metrics上查看 Prometheus 格式的磁盘相关系统指标。\n\n注： 您可以在test&#x2F;kernel_log_generator&#x2F;problems下查看更多规则示例。\n\n对于KernelMonitor消息注入，所有消息都应该有kernel: 前缀（另请注意，后面有一个空格:）；或者使用generator.sh。\n要将其他日志（如 systemd 日志）注入 journald，请使用echo &#39;Some systemd message&#39; | systemd-cat -t systemd。\n\n1.12 Remedy SystemsRemedy Systems是一个或多个旨在尝试解决node-problem-detector检测到的问题的过程。\nRemedy Systems会观察node-problem-detector发出的事件 和&#x2F;或 节点状况，并采取措施使 Kubernetes 集群恢复健康状态。\n补救系统有以下几种：\n\nDraino 根据标签和节点条件自动排空 Kubernetes节点。与所有提供的标签和任何提供的节点条件匹配的节点将被阻止立即接受新 pod（也称为“封锁”），并在可配置的时间后排空。Draino可以与 Cluster Autoscaler结合使用，以自动终止排空的节点。请参阅 此问题 ，了解 Draino的示例生产用例。\nDescheduler 取消调度策略RemovePodsViolatingNodeTaints会驱逐节点上违反NoSchedule污染的Pod。必须启用k8s调度程序的TaintNodesByCondition功能。\nCluster Autoscaler 可用于自动终止耗尽的节点。\nmediK8S 是一个基于Node Health Check Operator (NHC)构建的自动修复系统的总体项目，该系统监控节点状况并使用修复 API将修复委托给外部修复程序。\nPoison-Pill是一个修复程序，它将重新启动节点并确保所有有状态的工作负载都得到重新安排。如果集群具有足够的健康容量，NHC支持有条件地进行修复，或者手动暂停任何操作以最大限度地减少集群中断。\nCluster API 的MachineHealthCheck负责修复不健康的机器。\n\n","categories":["devops/npd"],"tags":["devops","npd"]},{"title":"BK-CI插件开发指引","url":"/2025/07/devops/blueking/bk-ci-cha-jian-kai-fa-zhi-yin/","content":"写在前面\n开发插件前，先进入插件工作台初始化一个插件，确定插件在平台中的唯一标识\n\n工作台可以在这里进行新增&#x2F;发布&#x2F;下架等管理插件的操作\n功能区介绍\n\n切换资源类型\n新增插件\n单个插件的管理入口\n升级、下架、删除插件快捷入口\n指引文档和插件 UI 调试工具入口\n\n新增插件\n\n标识\n\n\n插件在平台中的唯一标识，建议取和插件功能相关的可读性好的英文标识\n\n\n调试项目\n\n\n插件发布过程中，可以在调试项目下将插件添加到流水线执行，对插件进行测试，保证插件功能满足预期。\n建议新增专用的插件调试项目，避免测试过程中影响到业务。\n\n\n开发语言\n\n\n支持四种语言开发插件：\nJava（推荐）\nPython\nGolang\nNodejs\n\n\n\n开发插件\n初始化好插件之后，可以开始开发插件\n\n\n根据开发语言参考对应的开发指引\nJava 插件开发指引\nPython 插件开发指引\nGolang 插件开发指引\nNodejs 插件开发指引\n\n\n\n插件私有配置\n插件级别的敏感信息，如 token、用户名密码、IP、域名等，不建议直接提交到代码库，通过工作台私有配置界面管理\n\n\nGolang 插件开发插件开发框架说明\n插件最终打包成一个命令行可执行的命令即可，对开发框架无硬性要求 下边以 demo 插件为例示范\n\n示例插件代码工程的整体结构如下|- &lt;你的插件标识&gt;    |- cmd        |- application            |- main.go    |- hello        |- hello.go\n\n如何开发插件：\n参考 plugin-demo-golang\n\n\n建插件代码工程\n插件代码建议企业下统一管理。 通用的开源插件可以联系蓝鲸官方放到 TencentBlueKing 下，供更多用户使用\n\n\n实现插件功能\n规范：\n插件开发规范\n插件配置规范\n\n\n插件前端不仅可以通过 task.json 进行标准化配置，也可以自定义开发：\n自定义插件 UI 交互指引\n插件输出规范\n插件错误码规范\n插件发布包规范\n\n\n\nDemo示例研读将 plugin-demo-golang clone到本地。来看下项目结构：\n.├── CONTRIBUTING.md├── LICENSE├── Makefile├── README.md├── go.mod├── go.sum├── i18n│   ├── message_en_US.properties│   └── message_zh_CN.properties├── main.go├── task.json└── translation    └── translation.go3 directories, 11 files\n\n咦！和上面说的插件代码工程的整体架构不一样：\n|- &lt;你的插件标识&gt;    |- cmd        |- application            |- main.go    |- hello        |- hello.go\n\n不过，这不重要，只要插件最终能够打包成一个命令行可执行的命令即可。这里i18n是实现中英文国际化使用的，\n├── i18n│   ├── message_en_US.properties│   └── message_zh_CN.properties\n\n看下内容对比：message_en_US.properties：\ninput.desc.label=desc100001=input param [&#123;0&#125;] invitated\n\nmessage_zh_CN.properties\ninput.desc.label=描述100001=输入参数[&#123;0&#125;]非法\n\n那我们就可以猜测，这里是实现输入参数[{0}]非法这句话的中英文，其中[{0}]会使用具体的参数填充。然后i18ngenerator则是根据properties文件的配置，生成translation代码，如translation.go：\n// Code generated by &quot;i18ngenerator&quot;; DO NOT EDIT.package translation// Translationsvar Translations map[string][][]string = make(map[string][][]string)func init() &#123;    Translations[&quot;en-US&quot;] = [][]string&#123;       &#123;          &quot;100001&quot;,          &quot;input param [&#123;0&#125;] invitated&quot;,       &#125;,       &#123;          &quot;input.desc.label&quot;,          &quot;desc&quot;,       &#125;,    &#125;    Translations[&quot;zh-CN&quot;] = [][]string&#123;       &#123;          &quot;100001&quot;,          &quot;输入参数[&#123;0&#125;]非法&quot;,       &#125;,       &#123;          &quot;input.desc.label&quot;,          &quot;描述&quot;,       &#125;,    &#125;&#125;\n\n该文件是由i18ngenerator自动生成的，不要自己改。main函数内，实现了一个小的输出功能，简单看下源码，然后去进行测试：\npackage mainimport (    &quot;fmt&quot;    &quot;io/ioutil&quot;    &quot;os&quot;    &quot;runtime&quot;    &quot;time&quot;    &quot;github.com/ci-plugins/golang-plugin-sdk/api&quot;    &quot;github.com/ci-plugins/golang-plugin-sdk/log&quot;    &quot;github.com/ci-plugins/plugin-demo-golang/translation&quot;)//go:generate i18ngenerator i18n ./translation/translation.gotype greetingParam struct &#123;    UserName string `json:&quot;userName&quot;`    Greeting string `json:&quot;greeting&quot;`&#125;func (a *greetingParam) String() string &#123;    return fmt.Sprintf(&quot;userName: %v, greeting: %v&quot;, a.UserName, a.Greeting)&#125;func main() &#123;    runtime.GOMAXPROCS(4)    log.Info(&quot;atom-demo-glang starts&quot;)    defer func() &#123;       if err := recover(); err != nil &#123;          log.Error(&quot;panic: &quot;, err)          api.FinishBuild(api.StatusError, &quot;panic occurs&quot;)       &#125;    &#125;()    api.InitI18n(translation.Translations, api.GetRuntimeLanguage())    msg, err := api.Localize(&quot;input.desc.label&quot;)    if err != nil &#123;       log.Error(err)    &#125;    log.Info(msg)    helloBuild()&#125;func helloBuild() &#123;    // 获取单个输入参数    userName := api.GetInputParam(&quot;userName&quot;)    log.Info(&quot;userName: &quot;, userName)    // 打屏    log.Info(&quot;\\nBuildInfo:&quot;)    log.Info(&quot;Project Name:     &quot;, api.GetProjectDisplayName())    log.Info(&quot;Pipeline Id:      &quot;, api.GetPipelineId())    log.Info(&quot;Pipeline Name:    &quot;, api.GetPipelineName())    log.Info(&quot;Pipeline Version: &quot;, api.GetPipelineVersion())    log.Info(&quot;Build Id:         &quot;, api.GetPipelineBuildId())    log.Info(&quot;Build Num:        &quot;, api.GetPipelineBuildNumber())    log.Info(&quot;Start Type:       &quot;, api.GetPipelineStartType())    log.Info(&quot;Start UserId:     &quot;, api.GetPipelineStartUserId())    log.Info(&quot;Start UserName:   &quot;, api.GetPipelineStartUserName())    log.Info(&quot;Start Time:       &quot;, api.GetPipelineStartTimeMills())    log.Info(&quot;Workspace:        &quot;, api.GetWorkspace())    // 输入参数解析到对象    paramData := new(greetingParam)    api.LoadInputParam(paramData)    log.Info(fmt.Sprintf(&quot;\\n%v，%v\\n&quot;, paramData.Greeting, paramData.UserName))    // 业务逻辑    log.Info(&quot;start build&quot;)    build()    time.Sleep(2 * time.Second)    // 输出    // 字符串输出    strData := api.NewStringData(&quot;test&quot;)    api.AddOutputData(&quot;strData_01&quot;, strData)    // 文件归档输出    artifactData := api.NewArtifactData()    artifactData.AddArtifact(&quot;result.dat&quot;)    api.AddOutputData(&quot;artifactData_02&quot;, artifactData)    // 报告输出    reportData := api.NewReportData(&quot;label_01&quot;, api.GetWorkspace()+&quot;/report&quot;, &quot;report.htm&quot;)    api.AddOutputData(&quot;report_01&quot;, reportData)    api.WriteOutput()    log.Info(&quot;build done&quot;)&#125;func build() &#123;    log.Info(&quot;write result.dat&quot;)    ioutil.WriteFile(api.GetWorkspace()+&quot;/result.dat&quot;, []byte(&quot;content&quot;), 0644)    log.Info(&quot;write report.htm&quot;)    os.Mkdir(api.GetWorkspace()+&quot;/report&quot;, os.ModePerm)    ioutil.WriteFile(api.GetWorkspace()+&quot;/report/report.htm&quot;, []byte(&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;Report&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;H1&gt;This is a Report&lt;/H1&gt;&lt;/body&gt;&lt;/html&gt;&quot;), 0644)&#125;\n\n在根目录下已经给我们预设了一个task.json文件，后面可以简单修改下这个文件来实现测试：\n&#123;  &quot;atomCode&quot;: &quot;goDemo&quot;,  &quot;execution&quot;: &#123;    &quot;language&quot;: &quot;golang&quot;,    &quot;packagePath&quot;: &quot;goDemo&quot;,    &quot;demands&quot;: [      &quot;chmod +x goDemo&quot;    ],    &quot;target&quot;: &quot;./goDemo&quot;  &#125;,  &quot;input&quot;: &#123;    &quot;greeting&quot;: &#123;      &quot;label&quot;: &quot;欢迎词&quot;,      &quot;default&quot;: &quot;Glad to see you&quot;,      &quot;placeholder&quot;: &quot;欢迎词&quot;,      &quot;type&quot;: &quot;vuex-input&quot;,      &quot;desc&quot;: &quot;欢迎词&quot;,      &quot;required&quot;: true,      &quot;disabled&quot;: false,      &quot;hidden&quot;: false,      &quot;isSensitive&quot;: false    &#125;,    &quot;userName&quot;: &#123;      &quot;label&quot;: &quot;姓名&quot;,      &quot;default&quot;: &quot;Mr. Huang&quot;,      &quot;placeholder&quot;: &quot;姓名&quot;,      &quot;type&quot;: &quot;vuex-input&quot;,      &quot;desc&quot;: &quot;姓名&quot;,      &quot;required&quot;: true,      &quot;disabled&quot;: false,      &quot;hidden&quot;: false,      &quot;isSensitive&quot;: false    &#125;  &#125;,  &quot;output&quot;: &#123;    &quot;strData_01&quot;: &#123;      &quot;description&quot;: &quot;测试&quot;,      &quot;type&quot;: &quot;string&quot;,      &quot;isSensitive&quot;: false    &#125;  &#125;&#125;\n\n如何打包发布\n进入插件代码工程目录下\n打包\n\n\n如果按照正常的demo的目录结构是需要进入cmd&#x2F;application内执行build命令，因为main在此\n\ncd cmd/applicationGO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/$&#123;executable&#125;\n\n这次用的demo则直接在根目录下执行build命令，因为main在根目录下\n\nGO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/$&#123;executable&#125;\n\n这里go build -o bin&#x2F;${executable}会在bin目录下，生成可执行文件，文件名是${executable}，即项目名。也可以自定义一个名字，如GO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/kingtest\n\n在任意位置新建文件夹，命名示例：release_pkg &#x3D; &lt;你的插件标识&gt;_release\n将步骤 2 生产的执行包拷贝到 下\n添加 task.json 文件到 下 task.json 见示例，按照插件功能配置。\n\nmkdir kingtest_releasecp bin/kingtest kingtest_release/kingtesttouch kingtest_release/task.json\n\n\n插件配置规范\ntask.json 示例：\n\n&#123;  &quot;atomCode&quot;: &quot;king-test&quot;,                  # atomCode 要与工作台录入的一致  &quot;execution&quot;: &#123;    &quot;language&quot;: &quot;golang&quot;,    &quot;packagePath&quot;: &quot;kingtest&quot;,              # 发布包中插件安装包的相对路径    &quot;demands&quot;: [      &quot;echo start run chmod +x kingtest&quot;,   # 插件启动前需要执行的安装命令，顺序执行      &quot;chmod +x kingtest&quot;,                  # 插件启动前需要执行的安装命令，顺序执行      &quot;echo stop run chmod +x kingtest&quot;,    # 插件启动前需要执行的安装命令，顺序执行    ],    &quot;target&quot;: &quot;./kingtest&quot;  &#125;,  &quot;input&quot;: &#123;    &quot;greeting&quot;: &#123;      &quot;label&quot;: &quot;欢迎词&quot;,      &quot;default&quot;: &quot;Glad to see you&quot;,      &quot;placeholder&quot;: &quot;欢迎词&quot;,      &quot;type&quot;: &quot;vuex-input&quot;,      &quot;desc&quot;: &quot;欢迎词&quot;,      &quot;required&quot;: true,      &quot;disabled&quot;: false,      &quot;hidden&quot;: false,      &quot;isSensitive&quot;: false    &#125;,    &quot;userName&quot;: &#123;      &quot;label&quot;: &quot;姓名&quot;,      &quot;default&quot;: &quot;Mr. Huang&quot;,      &quot;placeholder&quot;: &quot;姓名&quot;,      &quot;type&quot;: &quot;vuex-input&quot;,      &quot;desc&quot;: &quot;姓名&quot;,      &quot;required&quot;: true,      &quot;disabled&quot;: false,      &quot;hidden&quot;: false,      &quot;isSensitive&quot;: false    &#125;  &#125;,  &quot;output&quot;: &#123;    &quot;strData_01&quot;: &#123;      &quot;description&quot;: &quot;测试&quot;,      &quot;type&quot;: &quot;string&quot;,      &quot;isSensitive&quot;: false    &#125;  &#125;&#125;\n\n\n在 目录下，把所有文件打成 zip 包即可\n\ncd kingtest_release &amp;&amp; zip kingtest_release.zip kingtest task.json\n\nzip包结构示例：\n|- kingtest_release.zip         # 发布包   |- kingtest                  # 插件执行包   |- task.json                 # 插件配置文件\n\n打包完成后，在插件工作台提单发布，即可测试或发布插件\n上传一个流水线插件\n开发好插件之后，通过研发商店工作台，将插件发布到研发商店，提供给用户添加到流水线中使用。\n\n入口在工作台列表，点击如下入口发起发布流程：\n首次发布时，入口名为上架后续更新版本时，入口名为升级或者在插件发布管理-&gt;版本管理界面发起发布流程：\n填写插件相关信息&#x2F;上传插件发布包上架&#x2F;升级插件时，可以修改插件的基本信息，如下所示：\n\n适用 Job 类型：\n\n\n和流水线 Job 类型对应，请按照插件实际适用情况选择\n若选错，需新增版本修改\n\n\n发布包：\n\n\ntask.json 中的 atomCode 需和 新增插件时填写的标识一致，否则上传会失败\n\n测试&#x2F;发布插件\n填写好信息，提交后，进入发布流程，可以测试-&gt;重新传包-&gt;测试，直至插件满足预期后，手动继续流程将插件发布到研发商店\n\n\n\n测试：点击后跳转到插件调试项目的流水线服务下，可以将当前插件添加到流水线，验证 UI、功能是否满足预期\n重新传包：当测试发现问题，修复后，重新上传发布包，再次进行测试\n继续：测试 OK，满足预期后，确认提交发布\n取消发布：发布过程中，随时可以终止发布\n\n遇见的几个错误无权限执行在测试中遇见一个问题：无权限执行\n在execution-&gt;demands增加一个命令chmod +x kingtest即可解决\n发布进度里重新传包持续报错task.json格式错误还有个问题，在发布进度里重新传包时，一直报错task.json格式错误，但实际格式是对的！触发的原因暂不知道，但是确实是一个隐藏的bug。\n直接点击继续，然后走升级插件的方式可以正常使用。\ncannot execute binary file: Exec format error这里的问题是我造成的，最初我在mac环境下编译的可执行文件，命令是：\n# mac下执行go build -o bin/kingtest\n\n但是插件里选择的编译环境是linux。\n解决方式：让插件选择的编译环境和可执行文件的平台统一。我这里选择重新编译下可执行文件，采用在mac平台交叉编译linux平台可执行文件的方式。\nGO111MODULE=on GOOS=linux GOARCH=amd64 go build -o bin/kingtest\n\n也可以新建一个插件，编译环境选择mac。\n运行结果流水线结果：\n输出结果：\n[Plugin info]=====================================================================Task           : king-testDescription    : bk插件测试Version        : 1.0.3Author         : huariHelp           : More Information=====================================================================-----[Input]input(normal): (欢迎词)greeting=Glad to see youinput(normal): (姓名)userName=Mr. Huang-----[Install plugin]-----start run chmod +x kingteststop run chmod +x kingtestatom-demo-glang starts描述userName:  Mr. HuangBuildInfo:Project Name:      GOPSPipeline Id:       p-8967ed52b08847c8a5b0140937db0975Pipeline Name:     king-testPipeline Version:  11Build Id:          b-78947b5c39f34b32bbafb803042d1e22Build Num:         15Start Type:        MANUALStart UserId:      huariStart UserName:    huariStart Time:        1733298107286Workspace:         /data/devops/workspaceGlad to see you，Mr. Huangstart buildwrite result.datwrite report.htmbuild done[Output]1 file match:   /data/devops/workspace/result.datprepare to upload 7 B1/1 file(s) finishedoutput(except): artifactData_02=result.dat入口文件检测完成上传自定义产出物成功，共产生了1个文件output(except): report_01=report.htmoutput(normal): strData_01=test-----\n\n\n\n\n\n\n","categories":["devops/blueking"],"tags":["devops","blueking"]},{"title":"BKCI 简介","url":"/2025/07/devops/blueking/bkci-jian-jie/","content":"文档BK-CI官方文档：https://bk.tencent.com/docs/markdown/ZH/Devops/3.0/UserGuide/intro/README.mdgithub仓库地址：https://github.com/TencentBlueKing/bk-ci\n导航\n\n\n🐤 了解基本概念\n👉 使用 BKCI\n🚀 部署 BKCI\n\n\n\nBKCI 是什么？\n创建你的第一条流水线\nBKCI 硬件规格指南\n\n\nBKCI 组件\n关联你的第一个代码库\nBKCI 系统要求\n\n\n快速熟悉流水线\n为你的Git工程开启CI\n\n\n\n术语解释\n示例\n\n\n\n\nAPI接口\n\n\n\n\n\n\n📔 产品功能\n🏪 研发商店\n\n\n\n流水线\n浏览研发商店\n\n\n控制台\n开发一个流水线插件\n\n\n凭证管理\n在 BKCI 里使用商店插件\n\n\n构建资源\n\n\n\n相关文档蓝鲸学习社区：https://bk.tencent.com/s-mart/communities蓝鲸官方文档：https://bk.tencent.com/docs/\n蓝鲸体系蓝鲸简介：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/intro.md核心优势：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/advantages.md体系架构：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/solution.mdCI领域：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/ci_intro.mdCD领域：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/cd_intro.mdCO领域：https://bk.tencent.com/docs/markdown/ZH/BlueKingFamily/7.2/UserGuide/Solution/co_intro.md\n","categories":["devops/blueking"],"tags":["devops","blueking"]}]