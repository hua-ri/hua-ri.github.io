<!DOCTYPE html>
<html lang="zh-Hans">
  <head>
    

    
<script>!function(){var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,t=localStorage.getItem("use-color-scheme")||"auto";("dark"===t||e&&"light"!==t)&&document.documentElement.classList.toggle("dark",!0)}()</script>
    

<meta charset="utf-8" >

<title>LangGraph 案例：生成提示词</title>
<meta name="keywords" content="LangGraph 案例：生成提示词, 花日の博客">
<meta name="description" content="环境准备ollama部署qwen2.5:7b参考：https://hua-ri.cn/2025/08/llm/ollama/ollama-bu-shu-qwen25-7b/
UV安装mac环境安装命令：
1curl -LsSf https:">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="LangGraph 案例：生成提示词">
<meta property="og:description" content="环境准备ollama部署qwen2.5:7b参考：https://hua-ri.cn/2025/08/llm/ollama/ollama-bu-shu-qwen25-7b/
UV安装mac环境安装命令：
1curl -LsSf https:">

<link rel="shortcut icon" href="/images/huari.ico">
<link rel="stylesheet" href="/style/main.css">

  <link rel="stylesheet" href="/style/simple-lightbox.min.css"><meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div id="app" class="main">

<div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="http://hua-ri.cn">
        <img class="avatar" src="/images/huari-image.png" alt="logo" width="32px" height="32px">
      </a>
      <a href="http://hua-ri.cn">
        <h1 class="site-title">花日の博客</h1>
      </a>
    </div>
    <div class="right">
        <i class="icon menu-switch icon-menu-outline" ></i>
    </div>
  </div>
</div>

<div class="menu-container" style="height: 0;opacity: 0;">
<nav class="menu-list">
  
    
      <a href="/" class="menu purple-link">
        首页
      </a>
    
  
    
      <a href="/tags" class="menu purple-link">
        标签
      </a>
    
  
    
      <a href="/categories" class="menu purple-link">
        分类
      </a>
    
  
    
      <a href="/archives" class="menu purple-link">
        归档
      </a>
    
  
    
      <a href="/about" class="menu purple-link">
        关于
      </a>
    
  
</nav>
</div>



  <div class="content-container">
    <div class="post-detail">
      
      <h2 class="post-title">LangGraph 案例：生成提示词</h2>
      <div class="post-info post-detail-info">
        <span><i class="icon icon-calendar-outline"></i> 2025-08-13</span>
        
          <span>
          <i class="icon icon-pricetags-outline"></i>
            
              <a href="/tags/llm/">
              llm
                
                  ，
                
              </a>
            
              <a href="/tags/langgraph/">
              langgraph
                
              </a>
            
          </span>
        
      </div>
      <div class="post-content-wrapper">
        <div class="post-content">
          <h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>ollama部署qwen2.5:7b参考：<a href="https://hua-ri.cn/2025/08/llm/ollama/ollama-bu-shu-qwen25-7b/">https://hua-ri.cn/2025/08/llm/ollama/ollama-bu-shu-qwen25-7b/</a></p>
<h2 id="UV安装"><a href="#UV安装" class="headerlink" title="UV安装"></a>UV安装</h2><p>mac环境安装命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -LsSf https://astral.sh/uv/install.sh | sh</span><br></pre></td></tr></table></figure>

<h2 id="安装python-3-13"><a href="#安装python-3-13" class="headerlink" title="安装python 3.13"></a>安装python 3.13</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看已安装的python版本</span></span><br><span class="line">uv python list</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装python版本3.13</span></span><br><span class="line">uv python install 3.13</span><br></pre></td></tr></table></figure>

<h2 id="进入工作空间"><a href="#进入工作空间" class="headerlink" title="进入工作空间"></a>进入工作空间</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p llm &amp;&amp; cd llm</span><br></pre></td></tr></table></figure>

<h2 id="创建工作目录：prompt"><a href="#创建工作目录：prompt" class="headerlink" title="创建工作目录：prompt"></a>创建工作目录：prompt</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用指定python版本初始化工作目录</span></span><br><span class="line">uv init prompt -p 3.13</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入工作目录</span></span><br><span class="line">cd prompt</span><br></pre></td></tr></table></figure>

<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加依赖</span></span><br><span class="line">uv add langchain langgraph langchain_openai langchain_core dotenv IPython</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">对于使用pip作为依赖关系的项目</span></span><br><span class="line">pip install -U langchain langgraph</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="案例流程"><a href="#案例流程" class="headerlink" title="案例流程"></a>案例流程</h1><p>在这个例子中，我们将创建一个帮助用户生成prompt的聊天机器人。它将首先收集用户的需求，然后生成prompt，并根据用户输入进行逐步优化。这两个状态被分为两个独立的状态，LLM 决定何时在它们之间转换。<br>该系统的图形表示如下所示。</p>
<p><a class="simple-lightbox" target="_blank" rel="noopener" href="https://hua-ri-1308422075.cos.ap-guangzhou.myqcloud.com/huari-blog/imageslangchain_img_2.png"><img   src="/images/loading.svg" data-src="https://hua-ri-1308422075.cos.ap-guangzhou.myqcloud.com/huari-blog/imageslangchain_img_2.png"  alt="img_2.png" lazyload></a></p>
<p>这个例子里的机器人有两个主要“工作模式”或者说“状态”（state，也可以想象成节点，或者 agent）：</p>
<ol>
<li>收集信息状态 (Gather Information)：先跟你聊天，问清楚你想要什么样的指令模板。比如，这个模板的目标是啥？里面要包含哪些变量？输出结果有啥限制或要求？</li>
<li>生成提示词状态 (Generate Prompt)：信息收集够了，它就切换到这个状态，根据你给的信息，帮你写出那个指令模板。</li>
</ol>
<blockquote>
<p>LangGraph 这个库，就是用来帮你管理这种多状态、多步骤的复杂AI应用的“流程控制器”。</p>
</blockquote>
<h1 id="LLM配置"><a href="#LLM配置" class="headerlink" title="LLM配置"></a>LLM配置</h1><p>通过<code>.env</code>和<code>dotenv</code>实现llm配置的传入。</p>
<h2 id="env"><a href="#env" class="headerlink" title=".env"></a>.env</h2><p>我是通过<code>ollama</code>本地部署<code>qwen2.5:7b</code>，这里可以根据实际情况自行变更。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LLM_API_KEY=sk-ollama</span><br><span class="line">LLM_MODEL_NAME=qwen2.5:7b</span><br><span class="line">LLM_BASE_URL=http://localhost:11434/v1</span><br><span class="line">LLM_TEMPERATURE = 0</span><br><span class="line">LLM_MAX_TOKENS = 512</span><br></pre></td></tr></table></figure>

<p>可以如此家在配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from dotenv import load_dotenv</span><br><span class="line">load_dotenv()</span><br></pre></td></tr></table></figure>

<p>对应的通过ChatOpenAI初始化llm实例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=os.getenv(&quot;LLM_MODEL_NAME&quot;),</span><br><span class="line">    api_key=os.getenv(&quot;LLM_API_KEY&quot;),</span><br><span class="line">    base_url=os.getenv(&quot;LLM_BASE_URL&quot;),</span><br><span class="line">    temperature=os.getenv(&quot;LLM_TEMPERATURE&quot;),</span><br><span class="line">    max_tokens=os.getenv(&quot;LLM_MAX_TOKENS&quot;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h1 id="收集信息"><a href="#收集信息" class="headerlink" title="收集信息"></a>收集信息</h1><p>首先，让我们定义图谱中用于收集用户需求的部分。这将是一个带有特定系统消息的 LLM 调用。它将能够访问一个工具，当它准备好生成提示时可以调用该工具。</p>
<blockquote>
<p>本笔记本使用 Pydantic v2 BaseModel，需要langchain-core &gt;&#x3D; 0.3。<br>langchain-core &lt; 0.3由于混合使用 Pydantic v1 和 v2 ，使用 会导致错误BaseModels。</p>
</blockquote>
<p>API 参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html">SystemMessage</a> | <a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html">ChatOpenAI</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">from typing import List</span><br><span class="line"></span><br><span class="line">from langchain_core.messages import SystemMessage</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line"></span><br><span class="line">from pydantic import BaseModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">template = &quot;&quot;&quot;</span><br><span class="line">你的任务是从用户那里获取他们想要创建哪种类型的提示词模板信息。</span><br><span class="line"></span><br><span class="line">你应该从他们那里获取以下信息：</span><br><span class="line"></span><br><span class="line">- 提示词的目标是什么</span><br><span class="line">- 哪些变量会传递到提示词模板中</span><br><span class="line">- 输出不应该做的任何限制条件</span><br><span class="line">- 输出必须遵守的任何要求</span><br><span class="line"></span><br><span class="line">如果你无法识别这些信息，请要求他们澄清！不要试图胡乱猜测。</span><br><span class="line"></span><br><span class="line">在你能够识别所有信息后，调用相关的工具。&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_messages_info(messages):</span><br><span class="line">    return [SystemMessage(content=template)] + messages</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PromptInstructions(BaseModel):</span><br><span class="line">    &quot;&quot;&quot;Instructions on how to prompt the LLM.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    objective: str</span><br><span class="line">    variables: List[str]</span><br><span class="line">    constraints: List[str]</span><br><span class="line">    requirements: List[str]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm_with_tool = llm.bind_tools([PromptInstructions])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def info_chain(state):</span><br><span class="line">    messages = get_messages_info(state[&quot;messages&quot;])</span><br><span class="line">    response = llm_with_tool.invoke(messages)</span><br><span class="line">    return &#123;&quot;messages&quot;: [response]&#125;</span><br></pre></td></tr></table></figure>

<h1 id="生成提示"><a href="#生成提示" class="headerlink" title="生成提示"></a>生成提示</h1><p>我们现在设置生成提示的状态。这将需要一个单独的系统消息，以及一个函数来过滤工具调用之前的所有消息（因为那是前一个状态决定生成提示的时间）。</p>
<p>(API参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html">AIMessage</a> |<a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html">人类留言</a>|<a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html">工具消息</a>)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import AIMessage, HumanMessage, ToolMessage</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">New system prompt</span></span><br><span class="line">prompt_system = &quot;&quot;&quot;Based on the following requirements, write a good prompt template:</span><br><span class="line"></span><br><span class="line">&#123;reqs&#125;&quot;&quot;&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Function to get the messages <span class="keyword">for</span> the prompt</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Will only get messages AFTER the tool call</span></span><br><span class="line">def get_prompt_messages(messages: list):</span><br><span class="line">    tool_call = None</span><br><span class="line">    other_msgs = []</span><br><span class="line">    for m in messages:</span><br><span class="line">        if isinstance(m, AIMessage) and m.tool_calls:</span><br><span class="line">            tool_call = m.tool_calls[0][&quot;args&quot;]</span><br><span class="line">        elif isinstance(m, ToolMessage):</span><br><span class="line">            continue</span><br><span class="line">        elif tool_call is not None:</span><br><span class="line">            other_msgs.append(m)</span><br><span class="line">    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def prompt_gen_chain(state):</span><br><span class="line">    messages = get_prompt_messages(state[&quot;messages&quot;])</span><br><span class="line">    response = llm.invoke(messages)</span><br><span class="line">    return &#123;&quot;messages&quot;: [response]&#125;</span><br></pre></td></tr></table></figure>

<h1 id="定义状态逻辑"><a href="#定义状态逻辑" class="headerlink" title="定义状态逻辑"></a>定义状态逻辑</h1><p>这是聊天机器人所处状态的逻辑。如果最后一条消息是工具调用，那么我们处于“提示创建者”（<code>prompt</code>）应该响应的状态。否则，如果最后一条消息不是 HumanMessage，那么我们知道接下来应该由人工响应，因此我们处于此END状态。如果最后一条消息是 HumanMessage，并且之前有过工具调用，那么我们处于此<code>prompt</code>状态。否则，我们处于“信息收集”（<code>info</code>）状态。<br>(API 参考：<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END">END</a>)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from typing import Literal</span><br><span class="line"></span><br><span class="line">from langgraph.graph import END</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_state(state):</span><br><span class="line">    messages = state[&quot;messages&quot;]</span><br><span class="line">    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:</span><br><span class="line">        return &quot;add_tool_message&quot;</span><br><span class="line">    elif not isinstance(messages[-1], HumanMessage):</span><br><span class="line">        return END</span><br><span class="line">    return &quot;info&quot;</span><br></pre></td></tr></table></figure>

<h1 id="创建图表"><a href="#创建图表" class="headerlink" title="创建图表"></a>创建图表</h1><p>现在我们可以创建图表了。我们将使用 SqliteSaver 来保存对话历史记录。<br>(API 参考：<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver">InMemorySaver</a> | <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph">StateGraph</a> | <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START">START</a> | <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages">add_messages</a>)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.checkpoint.memory import InMemorySaver</span><br><span class="line">from langgraph.graph import StateGraph, START</span><br><span class="line">from langgraph.graph.message import add_messages</span><br><span class="line">from typing import Annotated</span><br><span class="line">from typing_extensions import TypedDict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class State(TypedDict):</span><br><span class="line">    messages: Annotated[list, add_messages]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">memory = InMemorySaver()</span><br><span class="line">workflow = StateGraph(State)</span><br><span class="line">workflow.add_node(&quot;info&quot;, info_chain)</span><br><span class="line">workflow.add_node(&quot;prompt&quot;, prompt_gen_chain)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@workflow.add_node</span><br><span class="line">def add_tool_message(state: State):</span><br><span class="line">    return &#123;</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            ToolMessage(</span><br><span class="line">                content=&quot;Prompt generated!&quot;,</span><br><span class="line">                tool_call_id=state[&quot;messages&quot;][-1].tool_calls[0][&quot;id&quot;],</span><br><span class="line">            )</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">workflow.add_conditional_edges(&quot;info&quot;, get_state, [&quot;add_tool_message&quot;, &quot;info&quot;, END])</span><br><span class="line">workflow.add_edge(&quot;add_tool_message&quot;, &quot;prompt&quot;)</span><br><span class="line">workflow.add_edge(&quot;prompt&quot;, END)</span><br><span class="line">workflow.add_edge(START, &quot;info&quot;)</span><br><span class="line">graph = workflow.compile(checkpointer=memory)</span><br></pre></td></tr></table></figure>

<h2 id="查看图"><a href="#查看图" class="headerlink" title="查看图"></a>查看图</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">画图</span></span><br><span class="line">png_bytes = graph.get_graph().draw_mermaid_png()</span><br><span class="line"></span><br><span class="line">with open(&quot;graph.png&quot;, &quot;wb&quot;) as f:</span><br><span class="line">    f.write(png_bytes)</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">os.system(&quot;open graph.png&quot;)</span><br></pre></td></tr></table></figure>

<p><a class="simple-lightbox" target="_blank" rel="noopener" href="https://hua-ri-1308422075.cos.ap-guangzhou.myqcloud.com/huari-blog/imageslangchain_img_3.png"><img   src="/images/loading.svg" data-src="https://hua-ri-1308422075.cos.ap-guangzhou.myqcloud.com/huari-blog/imageslangchain_img_3.png"  alt="img_3.png" lazyload></a></p>
<h1 id="使用图表"><a href="#使用图表" class="headerlink" title="使用图表"></a>使用图表</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import uuid</span><br><span class="line">cached_human_responses = [&quot;哈喽!&quot;, &quot;rag prompt&quot;, &quot;1 rag, 2 none, 3 no, 4 no&quot;, &quot;q&quot;]</span><br><span class="line">cached_response_index = 0</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: str(uuid.uuid4())&#125;&#125;</span><br><span class="line">while True:</span><br><span class="line">    user = cached_human_responses[cached_response_index]</span><br><span class="line">    cached_response_index += 1</span><br><span class="line">    print(f&quot;User (q/Q to quit): &#123;user&#125;&quot;)</span><br><span class="line">    if user in &#123;&quot;q&quot;, &quot;Q&quot;&#125;:</span><br><span class="line">        print(&quot;AI: Byebye&quot;)</span><br><span class="line">        break</span><br><span class="line">    output = None</span><br><span class="line">    for output in graph.stream(</span><br><span class="line">        &#123;&quot;messages&quot;: [HumanMessage(content=user)]&#125;, config=config, stream_mode=&quot;updates&quot;</span><br><span class="line">    ):</span><br><span class="line">        last_message = next(iter(output.values()))[&quot;messages&quot;][-1]</span><br><span class="line">        last_message.pretty_print()</span><br><span class="line"></span><br><span class="line">    if output and &quot;prompt&quot; in output:</span><br><span class="line">        print(&quot;Done!&quot;)</span><br></pre></td></tr></table></figure>

<h1 id="最后的代码"><a href="#最后的代码" class="headerlink" title="最后的代码"></a>最后的代码</h1><p>完整代码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line">from typing import List</span><br><span class="line">import os</span><br><span class="line">import uuid</span><br><span class="line">from langchain_core.messages import SystemMessage</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line"></span><br><span class="line">from pydantic import BaseModel</span><br><span class="line">from dotenv import load_dotenv</span><br><span class="line">from langchain_core.messages import AIMessage, HumanMessage, ToolMessage</span><br><span class="line">from langgraph.checkpoint.memory import InMemorySaver</span><br><span class="line">from langgraph.graph import StateGraph, START</span><br><span class="line">from langgraph.graph.message import add_messages</span><br><span class="line">from typing import Annotated</span><br><span class="line">from typing_extensions import TypedDict</span><br><span class="line"></span><br><span class="line">from langgraph.graph import END</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=os.getenv(&quot;LLM_MODEL_NAME&quot;),</span><br><span class="line">    api_key=os.getenv(&quot;LLM_API_KEY&quot;),</span><br><span class="line">    base_url=os.getenv(&quot;LLM_BASE_URL&quot;),</span><br><span class="line">    temperature=os.getenv(&quot;LLM_TEMPERATURE&quot;),</span><br><span class="line">    max_tokens=os.getenv(&quot;LLM_MAX_TOKENS&quot;)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">template = &quot;&quot;&quot;</span><br><span class="line">你的任务是从用户那里获取他们想要创建哪种类型的提示词模板信息。</span><br><span class="line"></span><br><span class="line">你应该从他们那里获取以下信息：</span><br><span class="line"></span><br><span class="line">- 提示词的目标是什么</span><br><span class="line">- 哪些变量会传递到提示词模板中</span><br><span class="line">- 输出不应该做的任何限制条件</span><br><span class="line">- 输出必须遵守的任何要求</span><br><span class="line"></span><br><span class="line">如果你无法识别这些信息，请要求他们澄清！不要试图胡乱猜测。</span><br><span class="line"></span><br><span class="line">在你能够识别所有信息后，调用相关的工具。&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_messages_info(messages):</span><br><span class="line">    return [SystemMessage(content=template)] + messages</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PromptInstructions(BaseModel):</span><br><span class="line">    &quot;&quot;&quot;Instructions on how to prompt the LLM.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    objective: str</span><br><span class="line">    variables: List[str]</span><br><span class="line">    constraints: List[str]</span><br><span class="line">    requirements: List[str]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm_with_tool = llm.bind_tools([PromptInstructions])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def info_chain(state):</span><br><span class="line">    messages = get_messages_info(state[&quot;messages&quot;])</span><br><span class="line">    response = llm_with_tool.invoke(messages)</span><br><span class="line">    return &#123;&quot;messages&quot;: [response]&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">New system prompt</span></span><br><span class="line">prompt_system = &quot;&quot;&quot;Based on the following requirements, write a good prompt template:</span><br><span class="line"></span><br><span class="line">&#123;reqs&#125;&quot;&quot;&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Function to get the messages <span class="keyword">for</span> the prompt</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Will only get messages AFTER the tool call</span></span><br><span class="line">def get_prompt_messages(messages: list):</span><br><span class="line">    tool_call = None</span><br><span class="line">    other_msgs = []</span><br><span class="line">    for m in messages:</span><br><span class="line">        if isinstance(m, AIMessage) and m.tool_calls:</span><br><span class="line">            tool_call = m.tool_calls[0][&quot;args&quot;]</span><br><span class="line">        elif isinstance(m, ToolMessage):</span><br><span class="line">            continue</span><br><span class="line">        elif tool_call is not None:</span><br><span class="line">            other_msgs.append(m)</span><br><span class="line">    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def prompt_gen_chain(state):</span><br><span class="line">    messages = get_prompt_messages(state[&quot;messages&quot;])</span><br><span class="line">    response = llm.invoke(messages)</span><br><span class="line">    return &#123;&quot;messages&quot;: [response]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_state(state):</span><br><span class="line">    messages = state[&quot;messages&quot;]</span><br><span class="line">    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:</span><br><span class="line">        return &quot;add_tool_message&quot;</span><br><span class="line">    elif not isinstance(messages[-1], HumanMessage):</span><br><span class="line">        return END</span><br><span class="line">    return &quot;info&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class State(TypedDict):</span><br><span class="line">    messages: Annotated[list, add_messages]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">memory = InMemorySaver()</span><br><span class="line">workflow = StateGraph(State)</span><br><span class="line">workflow.add_node(&quot;info&quot;, info_chain)</span><br><span class="line">workflow.add_node(&quot;prompt&quot;, prompt_gen_chain)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@workflow.add_node</span><br><span class="line">def add_tool_message(state: State):</span><br><span class="line">    return &#123;</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            ToolMessage(</span><br><span class="line">                content=&quot;Prompt generated!&quot;,</span><br><span class="line">                tool_call_id=state[&quot;messages&quot;][-1].tool_calls[0][&quot;id&quot;],</span><br><span class="line">            )</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">workflow.add_conditional_edges(&quot;info&quot;, get_state, [&quot;add_tool_message&quot;, &quot;info&quot;, END])</span><br><span class="line">workflow.add_edge(&quot;add_tool_message&quot;, &quot;prompt&quot;)</span><br><span class="line">workflow.add_edge(&quot;prompt&quot;, END)</span><br><span class="line">workflow.add_edge(START, &quot;info&quot;)</span><br><span class="line">graph = workflow.compile(checkpointer=memory)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">画图</span></span><br><span class="line">png_bytes = graph.get_graph().draw_mermaid_png()</span><br><span class="line"></span><br><span class="line">with open(&quot;graph.png&quot;, &quot;wb&quot;) as f:</span><br><span class="line">    f.write(png_bytes)</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">os.system(&quot;open graph.png&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cached_human_responses = [&quot;哈喽!&quot;, &quot;rag prompt&quot;, &quot;1 rag, 2 none, 3 no, 4 no&quot;, &quot;q&quot;]</span><br><span class="line">cached_response_index = 0</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: str(uuid.uuid4())&#125;&#125;</span><br><span class="line">while True:</span><br><span class="line">    user = cached_human_responses[cached_response_index]</span><br><span class="line">    cached_response_index += 1</span><br><span class="line">    print(f&quot;User (q/Q to quit): &#123;user&#125;&quot;)</span><br><span class="line">    if user in &#123;&quot;q&quot;, &quot;Q&quot;&#125;:</span><br><span class="line">        print(&quot;AI: Byebye&quot;)</span><br><span class="line">        break</span><br><span class="line">    output = None</span><br><span class="line">    for output in graph.stream(</span><br><span class="line">        &#123;&quot;messages&quot;: [HumanMessage(content=user)]&#125;, config=config, stream_mode=&quot;updates&quot;</span><br><span class="line">    ):</span><br><span class="line">        last_message = next(iter(output.values()))[&quot;messages&quot;][-1]</span><br><span class="line">        last_message.pretty_print()</span><br><span class="line"></span><br><span class="line">    if output and &quot;prompt&quot; in output:</span><br><span class="line">        print(&quot;Done!&quot;)</span><br></pre></td></tr></table></figure>

<p>对话记录：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">User (q/Q <span class="keyword">to</span> quit): 哈喽!</span><br><span class="line">================================== Ai Message ==================================</span><br><span class="line"></span><br><span class="line">你好！很高兴帮助你创建提示词模板。为了更好地理解你的需求，请告诉我：</span><br><span class="line"></span><br><span class="line">1. 这个提示词的目标是什么？</span><br><span class="line">2. 有哪些变量会传递到提示词模板中？</span><br><span class="line">3. 输出不应该做的任何限制条件是什么？</span><br><span class="line">4. 输出必须遵守的任何要求又是什么呢？</span><br><span class="line"></span><br><span class="line">请尽量详细地提供这些信息，这样我可以更准确地帮助你。<span class="built_in"></span></span><br><span class="line"><span class="built_in">User </span>(q/Q <span class="keyword">to</span> quit): rag prompt</span><br><span class="line">================================== Ai Message ==================================</span><br><span class="line"></span><br><span class="line">好的，让我们来明确一下“RAG”（Retrieval-Augmented Generation）提示词的目标和相关信息。</span><br><span class="line"></span><br><span class="line">1. **目标**：RAG 提示词的主要目的是结合检索到的信息和生成的文本来增强最终输出的质量。</span><br><span class="line">2. **变量**：</span><br><span class="line">   - `query`：用户的问题或查询。</span><br><span class="line">   - `context`：从数据库或其他来源检索的相关信息片段。</span><br><span class="line">3. **限制条件**：输出不应包含任何未经过验证的事实，确保所有引用的信息都是可靠的。</span><br><span class="line">4. **要求**：输出应清晰、简洁，并且在可能的情况下提供具体的例子来支持结论。</span><br><span class="line"></span><br><span class="line">请确认这些信息是否准确，或者你是否有其他特定的要求或变量需要添加。<span class="built_in"></span></span><br><span class="line"><span class="built_in">User </span>(q/Q <span class="keyword">to</span> quit): 1 rag, 2 none, 3 <span class="literal">no</span>, 4 <span class="literal">no</span></span><br><span class="line">================================== Ai Message ==================================</span><br><span class="line"></span><br><span class="line">明白了！根据你的反馈，我们将创建一个简单的 RAG 提示词模板，不包含额外的限制条件和要求。</span><br><span class="line"></span><br><span class="line">以下是提示词的目标、变量以及相关信息：</span><br><span class="line"></span><br><span class="line">- **目标**：结合检索到的信息和生成的内容来增强最终输出的质量。</span><br><span class="line">- **变量**：</span><br><span class="line">  - `query`：用户的问题或查询。</span><br><span class="line">  - `context`：从数据库或其他来源检索的相关信息片段。</span><br><span class="line"></span><br><span class="line">现在，我将调用相关的工具来创建这个提示词模板。请稍等片刻。<span class="built_in"></span></span><br><span class="line"><span class="built_in">Tool </span>Calls:</span><br><span class="line">  PromptInstructions (call_jasy7crr)</span><br><span class="line"> Call ID: call_jasy7crr</span><br><span class="line">  Args:</span><br><span class="line">    constraints: []</span><br><span class="line">    objective: 结合检索到的信息和生成的内容来增强最终输出的质量</span><br><span class="line">    requirements: []</span><br><span class="line">    variables: [<span class="string">&#x27;query&#x27;</span>, <span class="string">&#x27;context&#x27;</span>]</span><br><span class="line">=================================<span class="built_in"> Tool </span>Message =================================</span><br><span class="line"></span><br><span class="line">Prompt generated!</span><br><span class="line">================================== Ai Message ==================================</span><br><span class="line"></span><br><span class="line">```json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;promptTemplate&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;intro&quot;</span>: <span class="string">&quot;根据以下提供的查询和上下文信息，您需要综合运用检索到的相关信息以及您的创造力来生成高质量的输出。请确保最终内容既包含相关背景知识又具有创新性。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;variables&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;query&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;用户的具体需求或问题描述，例如：关于如何在家制作美味蛋糕的方法和技巧。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;example&quot;</span>: <span class="string">&quot;如何在家制作美味蛋糕&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;context&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;与查询相关的背景信息或限制条件，例如：需要考虑食材的可获得性、适合儿童操作等。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;example&quot;</span>: <span class="string">&quot;使用家庭厨房常见的材料，确保步骤简单易懂且安全无毒&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;instructions&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">&quot;step&quot;</span>: 1,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;首先，根据提供的查询和上下文信息进行初步分析，理解用户的具体需求。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;example&quot;</span>: <span class="string">&quot;理解用户希望了解如何在家制作美味蛋糕，并考虑到使用家庭厨房常见的材料且步骤简单易懂且安全无毒。&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">&quot;step&quot;</span>: 2,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;利用检索到的相关信息来丰富和深化您的回答，确保内容的准确性和完整性。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;example&quot;</span>: <span class="string">&quot;查找并整合关于制作蛋糕的基本知识、常见食材及其替代品、简单的食谱以及注意事项等。&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">&quot;step&quot;</span>: 3,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;结合检索到的信息和生成的内容来增强最终输出的质量，确保信息的连贯性和创新性。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;example&quot;</span>: <span class="string">&quot;编写一份详细且易于理解的蛋糕制作指南，包括材料清单、步骤说明以及一些创意装饰建议。&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;outro&quot;</span>: <span class="string">&quot;完成上述步骤后，请检查您的回答是否满足所有要求，并确保内容既全面又具有吸引力。&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">此模板旨在指导用户如何结合检索到的信息和生成的内容来增强最终输出的质量，同时提供了具体的变量说明、操作步骤以及示例以帮助理解。</span><br><span class="line">Done!<span class="built_in"></span></span><br><span class="line"><span class="built_in">User </span>(q/Q <span class="keyword">to</span> quit): q</span><br><span class="line">AI: Byebye</span><br><span class="line"></span><br></pre></td></tr></table></figure>
        </div>
          
        <div class="top-div">
          <ol class="top-box"><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="top-box-text">环境准备</span></a><ol class="top-box-child"><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#UV%E5%AE%89%E8%A3%85"><span class="top-box-text">UV安装</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%AE%89%E8%A3%85python-3-13"><span class="top-box-text">安装python 3.13</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E8%BF%9B%E5%85%A5%E5%B7%A5%E4%BD%9C%E7%A9%BA%E9%97%B4"><span class="top-box-text">进入工作空间</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%88%9B%E5%BB%BA%E5%B7%A5%E4%BD%9C%E7%9B%AE%E5%BD%95%EF%BC%9Aprompt"><span class="top-box-text">创建工作目录：prompt</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"><span class="top-box-text">安装依赖</span></a></li></ol></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E6%A1%88%E4%BE%8B%E6%B5%81%E7%A8%8B"><span class="top-box-text">案例流程</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#LLM%E9%85%8D%E7%BD%AE"><span class="top-box-text">LLM配置</span></a><ol class="top-box-child"><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#env"><span class="top-box-text">.env</span></a></li></ol></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E6%94%B6%E9%9B%86%E4%BF%A1%E6%81%AF"><span class="top-box-text">收集信息</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E7%94%9F%E6%88%90%E6%8F%90%E7%A4%BA"><span class="top-box-text">生成提示</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E5%AE%9A%E4%B9%89%E7%8A%B6%E6%80%81%E9%80%BB%E8%BE%91"><span class="top-box-text">定义状态逻辑</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E5%88%9B%E5%BB%BA%E5%9B%BE%E8%A1%A8"><span class="top-box-text">创建图表</span></a><ol class="top-box-child"><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E6%9F%A5%E7%9C%8B%E5%9B%BE"><span class="top-box-text">查看图</span></a></li></ol></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E4%BD%BF%E7%94%A8%E5%9B%BE%E8%A1%A8"><span class="top-box-text">使用图表</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E6%9C%80%E5%90%8E%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="top-box-text">最后的代码</span></a></li></ol>
        </div>
          
      </div>
    </div>

    
      <div class="next-post">
        <a class="purple-link" href="/2025/08/llm/mcp/mcp-golang-sdk/">
          <h3 class="post-title">
            
            下一篇：Mcp-Golang-SDK
          </h3>
        </a>
      </div>
    
  </div>










<footer>
<div class="site-footer">
  <div class="social-container">
    
      
        <a aria-label="跳转至github" href="https://github.com/f-dong" target="_blank">
          <i class="icon icon-github"></i>
        </a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  </div>
  
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <a href="https://github.com/f-dong/hexo-theme-minimalism" target="_blank">Theme</a>
  
  
  
  
  
  
</div>
</footer>


      </div>
    </div>
    
<script id="hexo-configurations"> window.theme_config = {"image":{"lazyload_enable":true,"lazyload_placeholder":null,"photo_zoom":"simple-lightbox"},"search":{"enable":true,"type":"json","placeholder":"输入关键词搜索...","algolia":{"appID":"","apiKey":"","indexName":""}},"language":"zh-Hans"}; window.is_post = true; </script>

<script src="/js/main.js"></script>






  <script src="/js/simple-lightbox.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {new SimpleLightbox('.post-detail .simple-lightbox', {fileExt: false,captionsData:'alt'});});</script></body>
</html>

